#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œä»»åŠ¡è¯„æµ‹å™¨
æ”¯æŒå¤šä¸ªä»»åŠ¡çš„å¹¶è¡Œè¯„æµ‹ï¼Œæ¯ä¸ªä»»åŠ¡ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„æ¨¡æ‹Ÿå™¨å®ä¾‹
"""

import os
import sys
import time
import copy
import json
import logging
import threading
import multiprocessing
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple

from config import ConfigManager
from utils.task_evaluator import TaskEvaluator
from utils.run_naming import RunNamingManager
from utils.data_loader import DataLoader

logger = logging.getLogger(__name__)


class ParallelTaskEvaluator:
    """
    å¹¶è¡Œä»»åŠ¡è¯„æµ‹å™¨ - æ”¯æŒå¤šä¸ªä»»åŠ¡çš„å¹¶è¡Œè¯„æµ‹
    æ¯ä¸ªä»»åŠ¡ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„æ¨¡æ‹Ÿå™¨å®ä¾‹ï¼Œç¡®ä¿å®Œå…¨éš”ç¦»
    """

    def __init__(self, config_file: str, agent_type: str, task_type: str,
                 scenario_id: str = None, custom_suffix: str = None):
        """
        åˆå§‹åŒ–å¹¶è¡Œè¯„æµ‹å™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶å
            agent_type: æ™ºèƒ½ä½“ç±»å‹ ('single' æˆ– 'multi')
            task_type: ä»»åŠ¡ç±»å‹ ('sequential', 'combined', 'independent')
            scenario_id: åœºæ™¯ID
            custom_suffix: è‡ªå®šä¹‰åç¼€
        """
        self.config_manager = ConfigManager()
        self.config = self.config_manager.get_config(config_file)
        self.config_file = config_file
        self.agent_type = agent_type
        self.task_type = task_type
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–é»˜è®¤å€¼
        eval_config = self.config.get('evaluation', {})
        run_settings = eval_config.get('run_settings', {})
        
        self.scenario_id = scenario_id or eval_config.get('default_scenario', '00001')
        self.custom_suffix = custom_suffix or run_settings.get('default_suffix', 'parallel')
        
        # å¹¶è¡Œè¯„æµ‹é…ç½®
        self.parallel_config = self.config.get('parallel_evaluation', {})
        if not self.parallel_config.get('enabled', False):
            raise ValueError("å¹¶è¡Œè¯„æµ‹æœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® parallel_evaluation.enabled = true")
        
        # å¹¶è¡Œæ‰§è¡Œå‚æ•°
        self.execution_mode = self.parallel_config.get('execution_mode', 'thread')
        self.max_parallel_tasks = self.parallel_config.get('max_parallel_tasks', 4)
        if self.max_parallel_tasks == 0:
            self.max_parallel_tasks = multiprocessing.cpu_count()
        
        self.startup_delay = self.parallel_config.get('startup_delay', 2.0)
        self.task_timeout = self.parallel_config.get('task_timeout', 1800)
        
        # æ•…éšœå¤„ç†é…ç½®
        failure_config = self.parallel_config.get('failure_handling', {})
        self.continue_on_failure = failure_config.get('continue_on_task_failure', True)
        self.max_retries = failure_config.get('max_retries', 1)
        self.retry_delay = failure_config.get('retry_delay', 5.0)
        
        # ç”Ÿæˆè¿è¡Œåç§°å’Œè¾“å‡ºç›®å½•
        self.run_name = RunNamingManager.generate_run_name(
            agent_type=self.agent_type,
            task_type=f"parallel_{self.task_type}",
            scenario_id=self.scenario_id,
            config_name=self.config_file,
            custom_suffix=self.custom_suffix
        )
        
        # åˆ›å»ºä¸»è¾“å‡ºç›®å½•
        base_output_dir = self.config.get('output_dir', 'output')
        self.output_dir = RunNamingManager.generate_output_directory(base_output_dir, self.run_name)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®ä¸»æ—¥å¿—æ–‡ä»¶
        self.main_log_file = os.path.join(self.output_dir, "parallel_execution.log")
        self._setup_main_logger()
        
        # æ•°æ®åŠ è½½å™¨
        self.data_loader = DataLoader()
        
        # ç»“æœæ”¶é›†
        self.results = {
            'run_info': {
                'run_name': self.run_name,
                'start_time': None,
                'end_time': None,
                'total_duration': 0,
                'parallel_config': self.parallel_config
            },
            'task_results': [],
            'summary': {
                'total_tasks': 0,
                'completed_tasks': 0,
                'failed_tasks': 0,
                'average_duration': 0,
                'parallel_efficiency': 0
            }
        }
        
        # çº¿ç¨‹å®‰å…¨é”
        self.results_lock = threading.Lock()
        
        logger.info(f"ğŸš€ å¹¶è¡Œä»»åŠ¡è¯„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"âš™ï¸ å¹¶è¡Œæ¨¡å¼: {self.execution_mode}, æœ€å¤§å¹¶è¡Œæ•°: {self.max_parallel_tasks}")

    def _setup_main_logger(self):
        """è®¾ç½®ä¸»æ—¥å¿—è®°å½•å™¨"""
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(self.main_log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ åˆ°æ ¹æ—¥å¿—è®°å½•å™¨
        logging.getLogger().addHandler(file_handler)

    def _select_tasks(self, all_tasks: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®é…ç½®é€‰æ‹©è¦è¯„æµ‹çš„ä»»åŠ¡

        Args:
            all_tasks: æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨

        Returns:
            List[Dict]: é€‰æ‹©çš„ä»»åŠ¡åˆ—è¡¨
        """
        selection_config = self.parallel_config.get('task_selection', {})
        mode = selection_config.get('mode', 'all')
        
        if mode == 'all':
            selected_tasks = all_tasks
        elif mode == 'range':
            range_config = selection_config.get('range', {})
            start = range_config.get('start_index', 0)
            end = range_config.get('end_index', -1)
            if end == -1:
                selected_tasks = all_tasks[start:]
            else:
                selected_tasks = all_tasks[start:end]
        elif mode == 'list':
            indices = selection_config.get('task_indices', [])
            selected_tasks = [all_tasks[i] for i in indices if 0 <= i < len(all_tasks)]
        elif mode == 'category':
            categories = selection_config.get('categories', [])
            selected_tasks = [task for task in all_tasks 
                            if task.get('task_category') in categories]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡é€‰æ‹©æ¨¡å¼: {mode}")
        
        logger.info(f"ğŸ“‹ ä»»åŠ¡é€‰æ‹©æ¨¡å¼: {mode}, é€‰ä¸­ {len(selected_tasks)}/{len(all_tasks)} ä¸ªä»»åŠ¡")
        return selected_tasks

    def _filter_tasks_by_agent_type(self, tasks: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è¿‡æ»¤ä»»åŠ¡

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            List[Dict]: è¿‡æ»¤åçš„ä»»åŠ¡åˆ—è¡¨
        """
        if self.agent_type == 'single':
            # å•æ™ºèƒ½ä½“æ¨¡å¼ï¼šæ’é™¤åä½œä»»åŠ¡
            collaboration_categories = [
                'explicit_collaboration', 
                'implicit_collaboration', 
                'compound_collaboration'
            ]
            filtered_tasks = [task for task in tasks 
                            if task.get('task_category') not in collaboration_categories]
            
            logger.info(f"ğŸ¤– å•æ™ºèƒ½ä½“æ¨¡å¼ï¼šè¿‡æ»¤æ‰ {len(tasks) - len(filtered_tasks)} ä¸ªåä½œä»»åŠ¡")
            return filtered_tasks
        else:
            # å¤šæ™ºèƒ½ä½“æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰ä»»åŠ¡
            logger.info(f"ğŸ¤– å¤šæ™ºèƒ½ä½“æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰ {len(tasks)} ä¸ªä»»åŠ¡")
            return tasks

    def _execute_single_task(self, task_info: Tuple[int, Dict, str]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹/è¿›ç¨‹ä¸­è¿è¡Œï¼‰

        Args:
            task_info: (ä»»åŠ¡ç´¢å¼•, ä»»åŠ¡æ•°æ®, ä»»åŠ¡è¾“å‡ºç›®å½•) çš„å…ƒç»„

        Returns:
            Dict: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        task_index, task_data, task_output_dir = task_info
        task_id = f"task_{task_index:05d}"

        # åˆ›å»ºä»»åŠ¡ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
        task_logger = logging.getLogger(f"parallel_task_{task_id}")
        task_log_file = os.path.join(task_output_dir, f"{task_id}_execution.log")

        # è®¾ç½®ä»»åŠ¡æ—¥å¿—å¤„ç†å™¨
        task_handler = logging.FileHandler(task_log_file, encoding='utf-8')
        task_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        task_handler.setFormatter(formatter)
        task_logger.addHandler(task_handler)
        task_logger.setLevel(logging.DEBUG)

        start_time = time.time()
        result = {
            'task_id': task_id,
            'task_index': task_index,
            'task_description': task_data.get('task_description', ''),
            'task_category': task_data.get('task_category', ''),
            'status': 'running',
            'start_time': datetime.now().isoformat(),
            'end_time': None,
            'duration': 0,
            'success_rate': 0.0,
            'error': None,
            'output_dir': task_output_dir,
            'retry_count': 0
        }

        try:
            task_logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}: {task_data.get('task_description', '')}")

            # æ·»åŠ å¯åŠ¨å»¶è¿Ÿï¼Œé¿å…åŒæ—¶åˆå§‹åŒ–é€ æˆèµ„æºç«äº‰
            if task_index > 0:
                delay = self.startup_delay * (task_index % 3)  # é”™å¼€å¯åŠ¨æ—¶é—´
                task_logger.info(f"â±ï¸ å¯åŠ¨å»¶è¿Ÿ {delay:.1f} ç§’")
                time.sleep(delay)

            # æ‰§è¡Œä»»åŠ¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            for retry in range(self.max_retries + 1):
                try:
                    result['retry_count'] = retry
                    if retry > 0:
                        task_logger.info(f"ğŸ”„ ç¬¬ {retry} æ¬¡é‡è¯•")
                        time.sleep(self.retry_delay)

                    # åˆ›å»ºå®Œå…¨ç‹¬ç«‹çš„ä»»åŠ¡è¯„æµ‹å™¨å®ä¾‹
                    task_result = self._run_isolated_task(task_data, task_output_dir, task_logger)

                    # æ›´æ–°ç»“æœ
                    result.update(task_result)
                    result['status'] = 'completed' if task_result.get('success', False) else 'failed'
                    break

                except Exception as e:
                    task_logger.exception(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ (å°è¯• {retry + 1}/{self.max_retries + 1}): {e}")
                    if retry == self.max_retries:
                        result['status'] = 'failed'
                        result['error'] = str(e)

        except Exception as e:
            task_logger.exception(f"âŒ ä»»åŠ¡æ‰§è¡Œå‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            result['status'] = 'failed'
            result['error'] = str(e)

        finally:
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            result['end_time'] = datetime.now().isoformat()
            result['duration'] = time.time() - start_time

            task_logger.info(f"âœ… ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆï¼ŒçŠ¶æ€: {result['status']}, è€—æ—¶: {result['duration']:.2f}ç§’")

            # æ¸…ç†ä»»åŠ¡æ—¥å¿—å¤„ç†å™¨
            task_logger.removeHandler(task_handler)
            task_handler.close()

        return result

    def _run_isolated_task(self, task_data: Dict, task_output_dir: str, task_logger) -> Dict[str, Any]:
        """
        åœ¨å®Œå…¨éš”ç¦»çš„ç¯å¢ƒä¸­è¿è¡Œå•ä¸ªä»»åŠ¡

        Args:
            task_data: ä»»åŠ¡æ•°æ®
            task_output_dir: ä»»åŠ¡è¾“å‡ºç›®å½•
            task_logger: ä»»åŠ¡ä¸“ç”¨æ—¥å¿—è®°å½•å™¨

        Returns:
            Dict: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        # åˆ›å»ºä»»åŠ¡ä¸“ç”¨çš„é…ç½®å‰¯æœ¬ï¼ˆæ·±æ‹·è´ç¡®ä¿å®Œå…¨éš”ç¦»ï¼‰
        task_config = copy.deepcopy(self.config)

        # ä¸ºä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„TaskEvaluatorå®ä¾‹
        # æ¯ä¸ªTaskEvaluatorä¼šåˆ›å»ºè‡ªå·±çš„æ¨¡æ‹Ÿå™¨å®ä¾‹ï¼Œç¡®ä¿å®Œå…¨éš”ç¦»
        task_evaluator = TaskEvaluator(
            config_file=self.config_file,
            agent_type=self.agent_type,
            task_type='independent',  # ä½¿ç”¨ç‹¬ç«‹æ¨¡å¼æ‰§è¡Œå•ä¸ªä»»åŠ¡
            scenario_id=self.scenario_id,
            custom_suffix=f"task_{hash(task_data.get('task_description', '')) % 10000}"
        )

        # è®¾ç½®ä»»åŠ¡ä¸“ç”¨çš„è¾“å‡ºç›®å½•
        task_evaluator.output_dir = task_output_dir
        task_evaluator.run_name = os.path.basename(task_output_dir)

        try:
            # åˆå§‹åŒ–åœºæ™¯ï¼ˆæ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç‹¬ç«‹çš„æ¨¡æ‹Ÿå™¨å®ä¾‹ï¼‰
            if not task_evaluator.initialize_scenario(self.scenario_id):
                raise RuntimeError("åœºæ™¯åˆå§‹åŒ–å¤±è´¥")

            # åˆå§‹åŒ–æ™ºèƒ½ä½“
            if not task_evaluator.initialize_agents():
                raise RuntimeError("æ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥")

            # è·å–å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯
            task_info = task_evaluator.bridge.get_task_info()
            if not task_info:
                raise RuntimeError("æ— æ³•è·å–ä»»åŠ¡ä¿¡æ¯")

            # åˆ›å»ºåªåŒ…å«å½“å‰ä»»åŠ¡çš„ä»»åŠ¡ä¿¡æ¯
            single_task_info = {
                'task_background': task_info.get('task_background', ''),
                'tasks': [task_data],  # åªåŒ…å«å½“å‰ä»»åŠ¡
                'scene_id': self.scenario_id,
                'agents_config': task_info.get('agents_config', [])
            }

            # æ‰‹åŠ¨è®¾ç½®ä»»åŠ¡ä¿¡æ¯åˆ°è½¨è¿¹è®°å½•å™¨
            task_evaluator.trajectory_recorder.set_task_info(single_task_info)
            task_evaluator.trajectory_recorder.set_evaluation_mode('independent')

            # æ›´æ–°ä»»åŠ¡éªŒè¯å™¨
            task_evaluator._update_task_verifier(single_task_info)

            # ç›´æ¥è¿è¡Œæ–°çš„ç‹¬ç«‹è¯„æµ‹æ¨¡å¼
            task_evaluator._run_independent_evaluation(single_task_info)

            # æå–ç»“æœ
            results = task_evaluator.results
            success = results.get('summary', {}).get('completed_tasks', 0) > 0
            success_rate = results.get('summary', {}).get('completion_rate', 0.0)

            return {
                'success': success,
                'success_rate': success_rate,
                'evaluation_result': results,
                'steps_taken': results.get('summary', {}).get('total_steps', 0)
            }

        except Exception as e:
            task_logger.exception(f"âŒ éš”ç¦»ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise

        finally:
            # ç¡®ä¿æ¸…ç†èµ„æº
            try:
                if hasattr(task_evaluator, 'bridge') and task_evaluator.bridge:
                    # å°è¯•æ¸…ç†æ¨¡æ‹Ÿå™¨è¿æ¥
                    if hasattr(task_evaluator.bridge, 'disconnect'):
                        task_evaluator.bridge.disconnect()
                    elif hasattr(task_evaluator.bridge, 'close'):
                        task_evaluator.bridge.close()
            except Exception as e:
                task_logger.warning(f"âš ï¸ æ¸…ç†ä»»åŠ¡èµ„æºæ—¶å‡ºé”™: {e}")

    def run_parallel_evaluation(self) -> Dict[str, Any]:
        """
        è¿è¡Œå¹¶è¡Œè¯„æµ‹

        Returns:
            Dict: å¹¶è¡Œè¯„æµ‹ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œä»»åŠ¡è¯„æµ‹ - æ¨¡å¼: {self.agent_type}_{self.task_type}")
        logger.info(f"ğŸƒ è¿è¡Œåç§°: {self.run_name}")

        self.results['run_info']['start_time'] = datetime.now().isoformat()
        start_time = time.time()

        try:
            # åŠ è½½ä»»åŠ¡æ•°æ®
            task_data = self.data_loader.load_task(f"{self.scenario_id}_task")
            if not task_data:
                raise RuntimeError(f"æ— æ³•åŠ è½½ä»»åŠ¡æ•°æ®: {self.scenario_id}_task")

            all_tasks = task_data.get('tasks', [])
            if not all_tasks:
                raise RuntimeError("ä»»åŠ¡æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡åˆ—è¡¨")

            # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è¿‡æ»¤ä»»åŠ¡
            filtered_tasks = self._filter_tasks_by_agent_type(all_tasks)

            # æ ¹æ®é…ç½®é€‰æ‹©ä»»åŠ¡
            selected_tasks = self._select_tasks(filtered_tasks)

            if not selected_tasks:
                raise RuntimeError("æ²¡æœ‰é€‰æ‹©åˆ°ä»»ä½•ä»»åŠ¡è¿›è¡Œè¯„æµ‹")

            self.results['summary']['total_tasks'] = len(selected_tasks)

            # å‡†å¤‡ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨
            task_infos = []
            for i, task in enumerate(selected_tasks):
                task_output_dir = os.path.join(self.output_dir, f"task_{i:05d}")
                os.makedirs(task_output_dir, exist_ok=True)
                task_infos.append((i, task, task_output_dir))

            logger.info(f"ğŸ“‹ å‡†å¤‡å¹¶è¡Œæ‰§è¡Œ {len(task_infos)} ä¸ªä»»åŠ¡")

            # æ‰§è¡Œå¹¶è¡Œè¯„æµ‹
            if self.execution_mode == 'process':
                executor_class = ProcessPoolExecutor
            else:
                executor_class = ThreadPoolExecutor

            with executor_class(max_workers=self.max_parallel_tasks) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_task = {
                    executor.submit(self._execute_single_task, task_info): task_info[0]
                    for task_info in task_infos
                }

                # æ”¶é›†ç»“æœ
                completed_count = 0
                failed_count = 0

                for future in as_completed(future_to_task, timeout=self.task_timeout):
                    task_index = future_to_task[future]

                    try:
                        task_result = future.result()

                        # çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°ç»“æœ
                        with self.results_lock:
                            self.results['task_results'].append(task_result)

                            if task_result['status'] == 'completed':
                                completed_count += 1
                                self.results['summary']['completed_tasks'] = completed_count
                            else:
                                failed_count += 1
                                self.results['summary']['failed_tasks'] = failed_count

                        logger.info(f"âœ… ä»»åŠ¡ {task_index} å®Œæˆ: {task_result['status']}")

                    except Exception as e:
                        logger.exception(f"âŒ ä»»åŠ¡ {task_index} æ‰§è¡Œå¼‚å¸¸: {e}")

                        # è®°å½•å¤±è´¥ä»»åŠ¡
                        with self.results_lock:
                            failed_count += 1
                            self.results['summary']['failed_tasks'] = failed_count

                            self.results['task_results'].append({
                                'task_id': f"task_{task_index:05d}",
                                'task_index': task_index,
                                'status': 'failed',
                                'error': str(e),
                                'duration': 0
                            })

                        if not self.continue_on_failure:
                            logger.error("âŒ ä»»åŠ¡å¤±è´¥ä¸”é…ç½®ä¸ºä¸ç»§ç»­æ‰§è¡Œï¼Œåœæ­¢å¹¶è¡Œè¯„æµ‹")
                            break

        except Exception as e:
            logger.exception(f"âŒ å¹¶è¡Œè¯„æµ‹æ‰§è¡Œå¤±è´¥: {e}")
            self.results['error'] = str(e)

        finally:
            # å®Œæˆè¯„æµ‹
            self.results['run_info']['end_time'] = datetime.now().isoformat()
            self.results['run_info']['total_duration'] = time.time() - start_time

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            self._calculate_summary_statistics()

            # ä¿å­˜ç»“æœ
            self._save_parallel_results()

            logger.info(f"ğŸ¯ å¹¶è¡Œè¯„æµ‹å®Œæˆ")
            logger.info(f"ğŸ“Š æ€»ä»»åŠ¡æ•°: {self.results['summary']['total_tasks']}")
            logger.info(f"âœ… å®Œæˆä»»åŠ¡: {self.results['summary']['completed_tasks']}")
            logger.info(f"âŒ å¤±è´¥ä»»åŠ¡: {self.results['summary']['failed_tasks']}")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {self.results['run_info']['total_duration']:.2f}ç§’")
            logger.info(f"ğŸ“ˆ å¹¶è¡Œæ•ˆç‡: {self.results['summary']['parallel_efficiency']:.2f}")

        return self.results

    def _calculate_summary_statistics(self):
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        task_results = self.results['task_results']

        if not task_results:
            return

        # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
        total_duration = sum(result.get('duration', 0) for result in task_results)
        self.results['summary']['average_duration'] = total_duration / len(task_results)

        # è®¡ç®—å¹¶è¡Œæ•ˆç‡ï¼ˆç†è®ºä¸Šä¸²è¡Œæ‰§è¡Œæ—¶é—´ vs å®é™…å¹¶è¡Œæ‰§è¡Œæ—¶é—´ï¼‰
        theoretical_serial_time = total_duration
        actual_parallel_time = self.results['run_info']['total_duration']

        if actual_parallel_time > 0:
            self.results['summary']['parallel_efficiency'] = theoretical_serial_time / actual_parallel_time
        else:
            self.results['summary']['parallel_efficiency'] = 0

        # è®¡ç®—æˆåŠŸç‡
        completed_tasks = self.results['summary']['completed_tasks']
        total_tasks = self.results['summary']['total_tasks']

        if total_tasks > 0:
            self.results['summary']['success_rate'] = completed_tasks / total_tasks
        else:
            self.results['summary']['success_rate'] = 0

    def _save_parallel_results(self):
        """ä¿å­˜å¹¶è¡Œè¯„æµ‹ç»“æœ"""
        try:
            # ä¿å­˜è¯¦ç»†ç»“æœ
            results_file = os.path.join(self.output_dir, "parallel_results.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)

            # ä¿å­˜è¿è¡Œæ‘˜è¦
            summary_file = os.path.join(self.output_dir, "run_summary.json")
            summary_data = {
                'run_info': self.results['run_info'],
                'summary': self.results['summary'],
                'task_count_by_status': self._get_task_count_by_status(),
                'task_count_by_category': self._get_task_count_by_category()
            }

            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“Š å¹¶è¡Œè¯„æµ‹ç»“æœå·²ä¿å­˜: {results_file}")
            logger.info(f"ğŸ“‹ è¿è¡Œæ‘˜è¦å·²ä¿å­˜: {summary_file}")

        except Exception as e:
            logger.exception(f"âŒ ä¿å­˜å¹¶è¡Œè¯„æµ‹ç»“æœå¤±è´¥: {e}")

    def _get_task_count_by_status(self) -> Dict[str, int]:
        """è·å–æŒ‰çŠ¶æ€åˆ†ç»„çš„ä»»åŠ¡æ•°é‡ç»Ÿè®¡"""
        status_counts = {}
        for result in self.results['task_results']:
            status = result.get('status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        return status_counts

    def _get_task_count_by_category(self) -> Dict[str, int]:
        """è·å–æŒ‰ç±»åˆ«åˆ†ç»„çš„ä»»åŠ¡æ•°é‡ç»Ÿè®¡"""
        category_counts = {}
        for result in self.results['task_results']:
            category = result.get('task_category', 'unknown')
            category_counts[category] = category_counts.get(category, 0) + 1
        return category_counts
