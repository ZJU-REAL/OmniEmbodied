#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹ä»»åŠ¡æ‰§è¡Œå™¨ - åŸºäºç‹¬ç«‹å®ä¾‹çš„Independentè¯„æµ‹å®ç°

è¿™ä¸ªæ¨¡å—å®ç°äº†æ–°çš„Independentè¯„æµ‹æ–¹å¼ï¼Œæ¯ä¸ªå­ä»»åŠ¡éƒ½åœ¨å®Œå…¨ç‹¬ç«‹çš„TaskEvaluatorå®ä¾‹ä¸­æ‰§è¡Œï¼Œ
é¿å…äº†ç¯å¢ƒé‡ç½®çš„å¤æ‚æ€§å’Œå¼€é”€ï¼Œç¡®ä¿äº†å®Œå…¨çš„ä»»åŠ¡éš”ç¦»ã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š
1. æ¯ä¸ªå­ä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„TaskEvaluatorå®ä¾‹
2. æ¯ä¸ªå­ä»»åŠ¡æœ‰ç‹¬ç«‹çš„æ¨¡æ‹Ÿå™¨å®ä¾‹
3. å®Œå…¨çš„èµ„æºéš”ç¦»ï¼Œæ— çŠ¶æ€æ±¡æŸ“
4. ä¸²è¡Œæ‰§è¡Œï¼Œç»“æœå¯é‡ç°
5. ç‹¬ç«‹çš„è¾“å‡ºç›®å½•å’Œæ—¥å¿—æ–‡ä»¶
"""

import os
import sys
import time
import copy
import json
import logging
import threading
import gc
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from utils.task_evaluator import TaskEvaluator
from config import ConfigManager

logger = logging.getLogger(__name__)


class IndependentTaskExecutor:
    """
    ç‹¬ç«‹ä»»åŠ¡æ‰§è¡Œå™¨
    
    å®ç°åŸºäºç‹¬ç«‹å®ä¾‹çš„Independentè¯„æµ‹æ–¹å¼ï¼Œæ¯ä¸ªå­ä»»åŠ¡éƒ½åœ¨å®Œå…¨ç‹¬ç«‹çš„ç¯å¢ƒä¸­æ‰§è¡Œã€‚
    è¿™ç§æ–¹å¼é¿å…äº†ç¯å¢ƒé‡ç½®çš„å¤æ‚æ€§ï¼Œç¡®ä¿äº†å®Œå…¨çš„ä»»åŠ¡éš”ç¦»ã€‚
    """
    
    def __init__(self, config_file: str, agent_type: str, scenario_id: str, 
                 custom_suffix: str, output_dir: str):
        """
        åˆå§‹åŒ–ç‹¬ç«‹ä»»åŠ¡æ‰§è¡Œå™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶å
            agent_type: æ™ºèƒ½ä½“ç±»å‹ ('single' æˆ– 'multi')
            scenario_id: åœºæ™¯ID
            custom_suffix: è‡ªå®šä¹‰åç¼€
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config_file = config_file
        self.agent_type = agent_type
        self.scenario_id = scenario_id
        self.custom_suffix = custom_suffix
        self.output_dir = output_dir
        
        # åŠ è½½é…ç½®
        self.config_manager = ConfigManager()
        self.config = self.config_manager.get_config(config_file)
        
        # è·å–independenté…ç½®
        eval_config = self.config.get('evaluation', {})
        exec_config = eval_config.get('execution', {})
        self.independent_config = exec_config.get('independent', {})
        
        # æ‰§è¡Œé…ç½®
        self.continue_on_failure = self.independent_config.get('continue_on_failure', True)
        self.delay_between_subtasks = self.independent_config.get('delay_between_subtasks', 1.0)
        self.show_subtask_progress = self.independent_config.get('show_subtask_progress', True)
        
        # èµ„æºç®¡ç†é…ç½®ï¼ˆç¡¬ç¼–ç åˆç†é»˜è®¤å€¼ï¼‰
        self.force_garbage_collection = True  # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œç¡®ä¿å†…å­˜æ¸…ç†
        self.cleanup_timeout = 10  # å®ä¾‹æ¸…ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.monitor_memory_usage = False  # ä¸ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ˆé¿å…æ€§èƒ½å¼€é”€ï¼‰
        
        # è¾“å‡ºç®¡ç†é…ç½®
        output_config = self.independent_config.get('output_management', {})
        self.subtask_dir_pattern = output_config.get('subtask_dir_pattern', 'subtask_{index:03d}_{hash}')
        self.create_subtask_directories = output_config.get('create_subtask_directories', False)
        self.save_individual_logs = output_config.get('save_individual_logs', True)
        self.generate_subtask_trajectories = output_config.get('generate_subtask_trajectories', True)
        
        # ç»“æœèšåˆ
        self.subtask_results = []
        self.aggregated_results = {
            'evaluation_info': {
                'mode': 'independent',
                'scenario_id': scenario_id,
                'agent_type': agent_type,
                'start_time': None,
                'end_time': None
            },
            'subtask_results': [],
            'aggregated_summary': {
                'total_subtasks': 0,
                'completed_subtasks': 0,
                'failed_subtasks': 0,
                'completion_rate': 0.0,
                'total_steps': 0,
                'total_execution_time': 0.0,
                'average_subtask_duration': 0.0,
                'category_performance': {}
            }
        }
        
        # çº¿ç¨‹å®‰å…¨é”
        self.results_lock = threading.Lock()
        
        logger.info(f"ğŸš€ ç‹¬ç«‹ä»»åŠ¡æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"âš™ï¸ æ‰§è¡Œç­–ç•¥: isolated_instances")
    
    def execute_independent_evaluation(self, task_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œç‹¬ç«‹è¯„æµ‹
        
        Args:
            task_info: ä»»åŠ¡ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰å­ä»»åŠ¡
            
        Returns:
            Dict: èšåˆçš„è¯„æµ‹ç»“æœ
        """
        logger.info("ğŸ“‹ å¼€å§‹ç‹¬ç«‹è¯„æµ‹æ¨¡å¼ï¼ˆåŸºäºç‹¬ç«‹å®ä¾‹ï¼‰")
        
        subtasks = task_info.get("tasks", [])
        if not subtasks:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å­ä»»åŠ¡")
            return self.aggregated_results
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        self.aggregated_results['evaluation_info']['start_time'] = datetime.now().isoformat()
        self.aggregated_results['aggregated_summary']['total_subtasks'] = len(subtasks)
        
        logger.info(f"ğŸ“Š å‡†å¤‡æ‰§è¡Œ {len(subtasks)} ä¸ªå­ä»»åŠ¡")
        
        # ä¸²è¡Œæ‰§è¡Œæ¯ä¸ªå­ä»»åŠ¡
        for subtask_index, subtask in enumerate(subtasks):
            if self.show_subtask_progress:
                logger.info(f"\nğŸ¯ å¼€å§‹æ‰§è¡Œå­ä»»åŠ¡ {subtask_index + 1}/{len(subtasks)}")
                logger.info(f"ğŸ“ ä»»åŠ¡æè¿°: {subtask.get('task_description', 'æœªçŸ¥ä»»åŠ¡')}")
            
            # æ‰§è¡Œå•ä¸ªå­ä»»åŠ¡
            subtask_result = self._execute_single_subtask(
                subtask, subtask_index, task_info
            )
            
            # è®°å½•ç»“æœ
            with self.results_lock:
                self.subtask_results.append(subtask_result)
                self.aggregated_results['subtask_results'].append(subtask_result)
                
                # æ›´æ–°ç»Ÿè®¡
                if subtask_result['result']['status'] == 'success':
                    self.aggregated_results['aggregated_summary']['completed_subtasks'] += 1
                else:
                    self.aggregated_results['aggregated_summary']['failed_subtasks'] += 1
                
                self.aggregated_results['aggregated_summary']['total_steps'] += subtask_result['result']['steps_taken']
            
            # æ˜¾ç¤ºè¿›åº¦
            if self.show_subtask_progress:
                status_icon = "âœ…" if subtask_result['result']['status'] == 'success' else "âŒ"
                logger.info(f"ğŸ“Š å­ä»»åŠ¡ {subtask_index + 1} å®Œæˆ: {status_icon}")
            
            # æ£€æŸ¥æ˜¯å¦ç»§ç»­æ‰§è¡Œ
            if subtask_result['result']['status'] != 'success' and not self.continue_on_failure:
                logger.info("â¹ï¸ å­ä»»åŠ¡å¤±è´¥ä¸”é…ç½®ä¸ºä¸ç»§ç»­ï¼Œåœæ­¢æ‰§è¡Œ")
                break
            
            # å­ä»»åŠ¡é—´å»¶è¿Ÿ
            if self.delay_between_subtasks > 0 and subtask_index < len(subtasks) - 1:
                time.sleep(self.delay_between_subtasks)
        
        # è®°å½•ç»“æŸæ—¶é—´å’Œç”Ÿæˆæœ€ç»ˆç»“æœ
        end_time = time.time()
        self.aggregated_results['evaluation_info']['end_time'] = datetime.now().isoformat()
        self.aggregated_results['aggregated_summary']['total_execution_time'] = end_time - start_time
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        self._calculate_final_statistics()
        
        logger.info("ğŸ‰ ç‹¬ç«‹è¯„æµ‹å®Œæˆï¼")
        self._log_final_summary()
        
        return self.aggregated_results

    def _execute_single_subtask(self, subtask: Dict[str, Any], subtask_index: int,
                               task_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªå­ä»»åŠ¡ï¼ˆåœ¨ç‹¬ç«‹çš„TaskEvaluatorå®ä¾‹ä¸­ï¼‰

        Args:
            subtask: å­ä»»åŠ¡æ•°æ®
            subtask_index: å­ä»»åŠ¡ç´¢å¼•
            task_info: å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯

        Returns:
            Dict: å­ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åˆ›å»ºå­ä»»åŠ¡ç‹¬ç«‹ç›®å½•
        if self.create_subtask_directories:
            # åˆ›å»ºå­ä»»åŠ¡è¾“å‡ºç›®å½•
            subtask_hash = hash(subtask.get('task_description', '')) % 10000
            subtask_dir_name = self.subtask_dir_pattern.format(
                index=subtask_index,
                hash=f"{subtask_hash:04d}"
            )
            subtask_output_dir = os.path.join(self.output_dir, subtask_dir_name)
            os.makedirs(subtask_output_dir, exist_ok=True)
        else:
            # ä½¿ç”¨ä¸»è¾“å‡ºç›®å½•ï¼Œä¸åˆ›å»ºå­ä»»åŠ¡ç‹¬ç«‹ç›®å½•
            subtask_output_dir = self.output_dir
            subtask_dir_name = f"subtask_{subtask_index:03d}"

        # åˆå§‹åŒ–ç»“æœç»“æ„
        subtask_result = {
            'subtask_index': subtask_index,
            'subtask_id': f"subtask_{subtask_index:03d}",
            'subtask_description': subtask.get('task_description', ''),
            'task_category': subtask.get('task_category', 'unknown'),
            'execution_info': {
                'start_time': datetime.now().isoformat(),
                'end_time': None,
                'duration': 0.0,
                'instance_creation_time': 0.0,
                'cleanup_time': 0.0
            },
            'result': {
                'status': 'failed',
                'steps_taken': 0,
                'completion_rate': 0.0,
                'validation_results': []
            },
            'resource_usage': {
                'peak_memory_mb': 0,
                'cpu_time_seconds': 0.0
            },
            'output_files': {
                'trajectory_file': None,
                'log_file': None
            }
        }

        start_time = time.time()
        task_evaluator = None

        try:
            logger.info(f"ğŸ”§ ä¸ºå­ä»»åŠ¡ {subtask_index + 1} åˆ›å»ºç‹¬ç«‹æ‰§è¡Œç¯å¢ƒ")

            # è®°å½•å®ä¾‹åˆ›å»ºå¼€å§‹æ—¶é—´
            instance_start_time = time.time()

            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢TaskEvaluatoråˆ›å»ºè‡ªå·±çš„è¾“å‡ºç›®å½•å’Œé‡å¤æ—¥å¿—
            os.environ['SCENARIO_OUTPUT_DIR'] = subtask_output_dir
            os.environ['DISABLE_AUTO_OUTPUT_DIR'] = 'true'
            os.environ['DISABLE_SUBTASK_LOGGING'] = 'true'  # ç¦ç”¨å­ä»»åŠ¡ç‹¬ç«‹æ—¥å¿—

            # åˆ›å»ºç‹¬ç«‹çš„TaskEvaluatorå®ä¾‹
            subtask_suffix = f"subtask_{subtask_index:03d}"
            task_evaluator = TaskEvaluator(
                config_file=self.config_file,
                agent_type=self.agent_type,
                task_type='independent',  # ä½¿ç”¨independentæ¨¡å¼ï¼Œä½†å®é™…ä¸Šä¼šè¢«å•ç‹¬å¤„ç†
                scenario_id=f"{self.scenario_id}_{subtask_suffix}",  # ä¸ºæ¯ä¸ªå­ä»»åŠ¡æä¾›å”¯ä¸€çš„scenario_id
                custom_suffix=None  # ä¸ä½¿ç”¨åç¼€ï¼Œé¿å…è‡ªåŠ¨ç”Ÿæˆç‹¬ç«‹ç›®å½•
            )

            # æ¸…ç†ç¯å¢ƒå˜é‡
            if 'SCENARIO_OUTPUT_DIR' in os.environ:
                del os.environ['SCENARIO_OUTPUT_DIR']
            if 'DISABLE_AUTO_OUTPUT_DIR' in os.environ:
                del os.environ['DISABLE_AUTO_OUTPUT_DIR']
            if 'DISABLE_SUBTASK_LOGGING' in os.environ:
                del os.environ['DISABLE_SUBTASK_LOGGING']

            # å¼ºåˆ¶è®¾ç½®å­ä»»åŠ¡ä¸“ç”¨çš„è¾“å‡ºç›®å½•
            task_evaluator.output_dir = subtask_output_dir
            task_evaluator.run_name = subtask_dir_name

            # è®°å½•å®ä¾‹åˆ›å»ºæ—¶é—´
            instance_creation_time = time.time() - instance_start_time
            subtask_result['execution_info']['instance_creation_time'] = instance_creation_time

            logger.info(f"âœ… ç‹¬ç«‹æ‰§è¡Œç¯å¢ƒåˆ›å»ºå®Œæˆ ({instance_creation_time:.2f}ç§’)")

            # åˆå§‹åŒ–ç‹¬ç«‹ç¯å¢ƒ
            if not self._initialize_isolated_environment(task_evaluator, task_info, subtask):
                raise RuntimeError("ç‹¬ç«‹ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")

            # æ‰§è¡Œå­ä»»åŠ¡
            execution_result = self._run_subtask_in_isolated_environment(
                task_evaluator, subtask, subtask_index
            )

            # æ›´æ–°ç»“æœ
            subtask_result['result'].update(execution_result)

            # è®°å½•è¾“å‡ºæ–‡ä»¶
            if hasattr(task_evaluator, 'trajectory_recorder'):
                subtask_result['output_files']['trajectory_file'] = \
                    os.path.relpath(task_evaluator.trajectory_recorder.trajectory_file, self.output_dir)
                subtask_result['output_files']['log_file'] = \
                    os.path.relpath(task_evaluator.trajectory_recorder.log_file, self.output_dir)

            logger.info(f"âœ… å­ä»»åŠ¡ {subtask_index + 1} æ‰§è¡Œå®Œæˆ")

        except Exception as e:
            logger.exception(f"âŒ å­ä»»åŠ¡ {subtask_index + 1} æ‰§è¡Œå¤±è´¥: {e}")
            subtask_result['result']['error'] = str(e)

        finally:
            # æ¸…ç†èµ„æº
            cleanup_start_time = time.time()
            self._cleanup_subtask_resources(task_evaluator)
            cleanup_time = time.time() - cleanup_start_time
            subtask_result['execution_info']['cleanup_time'] = cleanup_time

            # è®°å½•æ€»æ‰§è¡Œæ—¶é—´
            total_duration = time.time() - start_time
            subtask_result['execution_info']['duration'] = total_duration
            subtask_result['execution_info']['end_time'] = datetime.now().isoformat()

            # è®°å½•è¾“å‡ºç›®å½•ï¼ˆç”¨äºèšåˆï¼‰
            subtask_result['execution_info']['output_dir'] = subtask_output_dir

            logger.info(f"ğŸ§¹ å­ä»»åŠ¡ {subtask_index + 1} èµ„æºæ¸…ç†å®Œæˆ ({cleanup_time:.2f}ç§’)")

        return subtask_result

    def _initialize_isolated_environment(self, task_evaluator: TaskEvaluator,
                                       task_info: Dict[str, Any], subtask: Dict[str, Any]) -> bool:
        """
        åˆå§‹åŒ–ç‹¬ç«‹çš„æ‰§è¡Œç¯å¢ƒ

        Args:
            task_evaluator: TaskEvaluatorå®ä¾‹
            task_info: å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯
            subtask: å½“å‰å­ä»»åŠ¡æ•°æ®

        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
        """
        try:
            # åˆå§‹åŒ–åœºæ™¯
            if not task_evaluator.initialize_scenario(self.scenario_id):
                logger.error("âŒ åœºæ™¯åˆå§‹åŒ–å¤±è´¥")
                return False

            # åˆå§‹åŒ–æ™ºèƒ½ä½“
            if not task_evaluator.initialize_agents():
                logger.error("âŒ æ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥")
                return False

            # åˆ›å»ºåªåŒ…å«å½“å‰å­ä»»åŠ¡çš„ä»»åŠ¡ä¿¡æ¯
            single_task_info = {
                'task_background': task_info.get('task_background', ''),
                'tasks': [subtask],  # åªåŒ…å«å½“å‰å­ä»»åŠ¡
                'scene_id': self.scenario_id,
                'agents_config': task_info.get('agents_config', [])
            }

            # è®¾ç½®ä»»åŠ¡ä¿¡æ¯åˆ°è½¨è¿¹è®°å½•å™¨
            if hasattr(task_evaluator, 'trajectory_recorder'):
                task_evaluator.trajectory_recorder.set_task_info(single_task_info)
                task_evaluator.trajectory_recorder.set_evaluation_mode('independent')

            # æ›´æ–°ä»»åŠ¡éªŒè¯å™¨
            task_evaluator._update_task_verifier(single_task_info)

            logger.info("âœ… ç‹¬ç«‹ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
            return True

        except Exception as e:
            logger.exception(f"âŒ ç‹¬ç«‹ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def _run_subtask_in_isolated_environment(self, task_evaluator: TaskEvaluator,
                                           subtask: Dict[str, Any], subtask_index: int) -> Dict[str, Any]:
        """
        åœ¨ç‹¬ç«‹ç¯å¢ƒä¸­è¿è¡Œå­ä»»åŠ¡

        Args:
            task_evaluator: TaskEvaluatorå®ä¾‹
            subtask: å­ä»»åŠ¡æ•°æ®
            subtask_index: å­ä»»åŠ¡ç´¢å¼•

        Returns:
            Dict: æ‰§è¡Œç»“æœ
        """
        try:
            # å¼€å§‹ä»»åŠ¡è®°å½•
            if hasattr(task_evaluator, 'trajectory_recorder'):
                task_evaluator.trajectory_recorder.start_task(
                    subtask_index + 1,
                    subtask.get('task_description', ''),
                    'independent_subtask'
                )

            # è·å–é…ç½®å‚æ•°
            max_steps_per_task = self.config.get('task_evaluator', {}).get('max_steps_per_task', 30)
            max_total_steps = self.config.get('execution', {}).get('max_total_steps', 300)

            # æ‰§è¡Œå­ä»»åŠ¡ï¼ˆä½¿ç”¨TaskEvaluatorçš„å•ä»»åŠ¡æ‰§è¡Œæ–¹æ³•ï¼‰
            task_result = task_evaluator._execute_single_task(
                subtask, subtask_index + 1, max_steps_per_task, max_total_steps
            )

            # è®°å½•ä»»åŠ¡å®Œæˆæƒ…å†µ
            if hasattr(task_evaluator, 'trajectory_recorder'):
                task_evaluator.trajectory_recorder.record_task_completion(task_result['completed'])
                task_evaluator.trajectory_recorder.end_task()

            # è¿”å›æ ‡å‡†åŒ–çš„ç»“æœ
            return {
                'status': 'success' if task_result['completed'] else 'failed',
                'steps_taken': task_result['steps_taken'],
                'completion_rate': 1.0 if task_result['completed'] else 0.0,
                'validation_results': task_result.get('validation_results', [])
            }

        except Exception as e:
            logger.exception(f"âŒ å­ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'status': 'failed',
                'steps_taken': 0,
                'completion_rate': 0.0,
                'validation_results': [],
                'error': str(e)
            }

    def _cleanup_subtask_resources(self, task_evaluator: Optional[TaskEvaluator]):
        """
        æ¸…ç†å­ä»»åŠ¡èµ„æº

        Args:
            task_evaluator: TaskEvaluatorå®ä¾‹ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        """
        try:
            if task_evaluator is not None:
                # æ¸…ç†æ¨¡æ‹Ÿå™¨è¿æ¥
                if hasattr(task_evaluator, 'bridge') and task_evaluator.bridge:
                    try:
                        if hasattr(task_evaluator.bridge, 'disconnect'):
                            task_evaluator.bridge.disconnect()
                        elif hasattr(task_evaluator.bridge, 'close'):
                            task_evaluator.bridge.close()
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ¸…ç†æ¨¡æ‹Ÿå™¨è¿æ¥æ—¶å‡ºé”™: {e}")

                # æ¸…ç†å…¶ä»–èµ„æº
                if hasattr(task_evaluator, 'agents'):
                    task_evaluator.agents.clear()

                # åˆ é™¤å¼•ç”¨
                del task_evaluator

            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.force_garbage_collection:
                gc.collect()

        except Exception as e:
            logger.warning(f"âš ï¸ èµ„æºæ¸…ç†æ—¶å‡ºé”™: {e}")

    def _calculate_final_statistics(self):
        """è®¡ç®—æœ€ç»ˆç»Ÿè®¡æ•°æ®"""
        summary = self.aggregated_results['aggregated_summary']

        # è®¡ç®—å®Œæˆç‡
        if summary['total_subtasks'] > 0:
            summary['completion_rate'] = summary['completed_subtasks'] / summary['total_subtasks']

        # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
        if summary['total_subtasks'] > 0:
            total_duration = sum(
                result['execution_info']['duration']
                for result in self.aggregated_results['subtask_results']
            )
            summary['average_subtask_duration'] = total_duration / summary['total_subtasks']

        # æŒ‰ç±»åˆ«ç»Ÿè®¡æ€§èƒ½
        category_stats = {}
        for result in self.aggregated_results['subtask_results']:
            category = result['task_category']
            if category not in category_stats:
                category_stats[category] = {'total': 0, 'completed': 0, 'rate': 0.0}

            category_stats[category]['total'] += 1
            if result['result']['status'] == 'success':
                category_stats[category]['completed'] += 1

        # è®¡ç®—å„ç±»åˆ«å®Œæˆç‡
        for category, stats in category_stats.items():
            if stats['total'] > 0:
                stats['rate'] = stats['completed'] / stats['total']

        summary['category_performance'] = category_stats

    def _log_final_summary(self):
        """è®°å½•æœ€ç»ˆæ‘˜è¦"""
        summary = self.aggregated_results['aggregated_summary']

        logger.info("ğŸ“Š ç‹¬ç«‹è¯„æµ‹ç»“æœæ‘˜è¦:")
        logger.info(f"  - æ€»å­ä»»åŠ¡æ•°: {summary['total_subtasks']}")
        logger.info(f"  - å®Œæˆå­ä»»åŠ¡æ•°: {summary['completed_subtasks']}")
        logger.info(f"  - å¤±è´¥å­ä»»åŠ¡æ•°: {summary['failed_subtasks']}")
        logger.info(f"  - å®Œæˆç‡: {summary['completion_rate']:.1%}")
        logger.info(f"  - æ€»æ­¥æ•°: {summary['total_steps']}")
        logger.info(f"  - æ€»æ‰§è¡Œæ—¶é—´: {summary['total_execution_time']:.2f}ç§’")
        logger.info(f"  - å¹³å‡å­ä»»åŠ¡æ—¶é—´: {summary['average_subtask_duration']:.2f}ç§’")

        # æŒ‰ç±»åˆ«æ˜¾ç¤ºç»Ÿè®¡
        if summary['category_performance']:
            logger.info("ğŸ“ˆ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            for category, stats in summary['category_performance'].items():
                logger.info(f"  - {category}: {stats['completed']}/{stats['total']} ({stats['rate']:.1%})")

    def save_aggregated_results(self, output_file: str = None) -> str:
        """
        ä¿å­˜èšåˆç»“æœåˆ°æ–‡ä»¶

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = os.path.join(self.output_dir, 'independent_evaluation_results.json')

        try:
            import json
            os.makedirs(os.path.dirname(output_file), exist_ok=True)

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.aggregated_results, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“„ èšåˆç»“æœå·²ä¿å­˜: {output_file}")
            return output_file

        except Exception as e:
            logger.exception(f"âŒ ä¿å­˜èšåˆç»“æœå¤±è´¥: {e}")
            return ""

    def aggregate_compact_trajectories(self, main_trajectory_recorder) -> None:
        """
        èšåˆæ‰€æœ‰å­ä»»åŠ¡çš„compact_trajectoryåˆ°ä¸»è½¨è¿¹æ–‡ä»¶ä¸­

        Args:
            main_trajectory_recorder: ä¸»TaskEvaluatorçš„è½¨è¿¹è®°å½•å™¨
        """
        try:
            logger.info("ğŸ“‹ å¼€å§‹èšåˆå­ä»»åŠ¡çš„compact_trajectoryæ•°æ®")

            aggregated_tasks = []

            # éå†æ‰€æœ‰å­ä»»åŠ¡ç»“æœï¼Œæ”¶é›†compact_trajectoryæ•°æ®
            logger.debug(f"ğŸ” å¼€å§‹éå† {len(self.subtask_results)} ä¸ªå­ä»»åŠ¡ç»“æœ")
            for i, subtask_result in enumerate(self.subtask_results):
                subtask_output_dir = subtask_result.get('execution_info', {}).get('output_dir')
                logger.debug(f"ğŸ” å­ä»»åŠ¡ {i+1} è¾“å‡ºç›®å½•: {subtask_output_dir}")

                if not subtask_output_dir or not os.path.exists(subtask_output_dir):
                    logger.debug(f"âš ï¸ å­ä»»åŠ¡ {i+1} è¾“å‡ºç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                    continue

                # å­ä»»åŠ¡çš„compact_trajectoryæ–‡ä»¶è·¯å¾„
                # æ¯ä¸ªå­ä»»åŠ¡éƒ½æœ‰å”¯ä¸€çš„scenario_idï¼Œæ ¼å¼ä¸ºï¼šåŸscenario_id_subtask_xxx
                subtask_index = subtask_result.get('subtask_index', i+1)
                subtask_scenario_id = f"{self.scenario_id}_subtask_{subtask_index:03d}"
                compact_trajectory_file = os.path.join(subtask_output_dir, 'trajectories', f'{subtask_scenario_id}_compact_trajectory.json')
                logger.debug(f"ğŸ” æŸ¥æ‰¾è½¨è¿¹æ–‡ä»¶: {compact_trajectory_file}")

                if os.path.exists(compact_trajectory_file):
                    logger.debug(f"âœ… æ‰¾åˆ°è½¨è¿¹æ–‡ä»¶: {compact_trajectory_file}")
                    try:
                        with open(compact_trajectory_file, 'r', encoding='utf-8') as f:
                            subtask_compact_trajectory = json.load(f)

                        # æå–å­ä»»åŠ¡çš„è½¨è¿¹æ•°æ®
                        subtask_executions = subtask_compact_trajectory.get('task_executions', [])
                        logger.debug(f"ğŸ“Š å­ä»»åŠ¡ {i+1} åŒ…å« {len(subtask_executions)} ä¸ªä»»åŠ¡æ‰§è¡Œè®°å½•")

                        for task in subtask_executions:
                            # ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ å­ä»»åŠ¡ç´¢å¼•ä¿¡æ¯
                            task['subtask_index'] = subtask_result.get('subtask_index', i+1)
                            task['subtask_output_dir'] = os.path.basename(subtask_output_dir)
                            aggregated_tasks.append(task)

                    except Exception as e:
                        logger.warning(f"âš ï¸ è¯»å–å­ä»»åŠ¡compact_trajectoryå¤±è´¥: {compact_trajectory_file}, é”™è¯¯: {e}")
                else:
                    logger.debug(f"âŒ è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {compact_trajectory_file}")

            # æ›´æ–°ä¸»è½¨è¿¹è®°å½•å™¨çš„compact_trajectory
            if aggregated_tasks:
                main_trajectory_recorder.compact_trajectory['task_executions'] = aggregated_tasks
                logger.info(f"âœ… æˆåŠŸèšåˆäº† {len(aggregated_tasks)} ä¸ªå­ä»»åŠ¡çš„è½¨è¿¹æ•°æ®")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯èšåˆçš„å­ä»»åŠ¡è½¨è¿¹æ•°æ®")

        except Exception as e:
            logger.exception(f"âŒ èšåˆcompact_trajectoryå¤±è´¥: {e}")
