import json
import logging
from typing import Dict, List, Optional, Any, Tuple

from OmniSimulator.core.enums import ActionStatus
from OmniSimulator.core.engine import SimulationEngine

from core.base_agent import BaseAgent
from config.config_manager import ConfigManager
from llm.base_llm import BaseLLM
from llm.llm_factory import create_llm_from_config
from utils.prompt_manager import PromptManager

# ç¡®ä¿loggerä½¿ç”¨æ­£ç¡®çš„åç§°ï¼Œä¸æ–‡ä»¶è·¯å¾„ä¸€è‡´
logger = logging.getLogger(__name__)

class LLMAgent(BaseAgent):
    """
    åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼Œä½¿ç”¨LLMå†³ç­–ä¸‹ä¸€æ­¥åŠ¨ä½œ
    """
    
    def __init__(self, simulator: SimulationEngine, agent_id: str, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–LLMæ™ºèƒ½ä½“"""
        super().__init__(simulator, agent_id, config)

        # åŠ è½½LLMé…ç½®
        config_manager = ConfigManager()
        self.llm_config = config_manager.get_config('llm_config')

        # åˆ›å»ºLLMå®ä¾‹
        self.llm = create_llm_from_config(self.llm_config)

        # åˆ›å»ºæç¤ºè¯ç®¡ç†å™¨
        self.prompt_manager = PromptManager("prompts_config")

        # æ¨¡å¼åç§°
        self.mode = "single_agent"

        # åŸºç¡€ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
        self.base_system_prompt = self.prompt_manager.get_prompt_template(
            self.mode,
            "system_prompt",
            "ä½ æ˜¯ä¸€ä¸ªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡çš„æ™ºèƒ½ä½“ã€‚"
        )

        # è½¨è¿¹è®°å½•å™¨å¼•ç”¨ï¼ˆç”¨äºè®°å½•LLM QAï¼‰
        self.trajectory_recorder = None

        # å¯¹è¯å†å²
        self.chat_history = []

        # è·å–å†å²é•¿åº¦é…ç½®
        history_config = self.config.get('history', {})
        max_history_length = history_config.get('max_history_length', 10)
        # -1 è¡¨ç¤ºä¸é™åˆ¶å†å²é•¿åº¦
        self.max_chat_history = None if max_history_length == -1 else max_history_length

        # åŒæ—¶æ›´æ–°åŠ¨ä½œå†å²çš„é•¿åº¦é™åˆ¶
        if max_history_length == -1:
            self.max_history = float('inf')  # ä¸é™åˆ¶åŠ¨ä½œå†å²é•¿åº¦
        else:
            self.max_history = max_history_length

        # ä»»åŠ¡æè¿°
        self.task_description = ""

        # ä¿å­˜æœ€åä¸€æ¬¡LLMå›å¤ï¼Œç”¨äºå†å²è®°å½•
        self.last_llm_response = ""

        # ç¯å¢ƒæè¿°ç¼“å­˜å’Œæ›´æ–°è®¡æ•°
        self.env_description_cache = ""
        self.step_count = 0

    def set_trajectory_recorder(self, trajectory_recorder):
        """è®¾ç½®è½¨è¿¹è®°å½•å™¨å¼•ç”¨"""
        self.trajectory_recorder = trajectory_recorder
        logger.debug(f"ğŸ”— æ™ºèƒ½ä½“ {self.agent_id} å·²è¿æ¥è½¨è¿¹è®°å½•å™¨")

    def set_task(self, task_description: str) -> None:
        """è®¾ç½®ä»»åŠ¡æè¿°"""
        self.task_description = task_description

    def _get_system_prompt(self) -> str:
        """è·å–åŒ…å«åŠ¨æ€åŠ¨ä½œæè¿°çš„ç³»ç»Ÿæç¤ºè¯"""
        # è·å–åŠ¨æ€åŠ¨ä½œæè¿°ï¼ˆä¼ å…¥å•ä¸ªæ™ºèƒ½ä½“IDï¼Œbridgeä¼šè‡ªåŠ¨è½¬æ¢ä¸ºåˆ—è¡¨ï¼‰
        actions_description = ""
        if self.bridge:
            actions_description = self.bridge.get_agent_supported_actions_description(self.agent_id)

        # å¦‚æœè·å–åˆ°åŠ¨ä½œæè¿°ï¼Œåˆ™æ’å…¥åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­
        if actions_description:
            return f"{self.base_system_prompt}\n\n{actions_description}"
        else:
            return self.base_system_prompt
    

    
    def _get_environment_description(self) -> str:
        """è·å–ç¯å¢ƒæè¿°ï¼Œæ ¹æ®é…ç½®å†³å®šè¯¦ç»†ç¨‹åº¦å’Œæ›´æ–°é¢‘ç‡"""
        if not self.bridge:
            return ""

        # è·å–ç¯å¢ƒæè¿°é…ç½®ï¼ˆä»agent_configä¸‹è¯»å–ï¼‰
        agent_config = self.config.get('agent_config', {})
        env_config = agent_config.get('environment_description', {})
        detail_level = env_config.get('detail_level', 'full')
        show_properties = env_config.get('show_object_properties', True)
        only_discovered = env_config.get('only_show_discovered', False)
        include_other_agents = env_config.get('include_other_agents', True)
        update_frequency = env_config.get('update_frequency', 0)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¯å¢ƒæè¿°ç¼“å­˜
        should_update = (
            update_frequency == 0 or  # æ¯æ­¥éƒ½æ›´æ–°
            self.step_count % update_frequency == 0 or  # æŒ‰é¢‘ç‡æ›´æ–°
            not self.env_description_cache  # é¦–æ¬¡è·å–
        )

        if not should_update:
            return self.env_description_cache

        # æ„å»ºæ¨¡æ‹Ÿå™¨é…ç½®
        sim_config = {
            'nlp_show_object_properties': show_properties,
            'nlp_only_show_discovered': only_discovered,
            'nlp_detail_level': detail_level
        }

        # è·å–æ™ºèƒ½ä½“ä¿¡æ¯
        agents = None
        if include_other_agents:
            agents = self.bridge.get_all_agents()
        else:
            # åªåŒ…å«å½“å‰æ™ºèƒ½ä½“
            agent_info = self.bridge.get_agent_info(self.agent_id)
            if agent_info:
                agents = {self.agent_id: agent_info}

        # æ ¹æ®è¯¦ç»†ç¨‹åº¦è·å–ç¯å¢ƒæè¿°
        if detail_level == 'room':
            # åªæè¿°å½“å‰æˆ¿é—´
            agent_info = self.bridge.get_agent_info(self.agent_id)
            if agent_info and 'location_id' in agent_info:
                room_id = agent_info.get('location_id')
                env_description = self.bridge.describe_room_natural_language(room_id, agents, sim_config)
            else:
                env_description = "æ— æ³•ç¡®å®šå½“å‰ä½ç½®"
        elif detail_level == 'brief':
            # åªæè¿°æ™ºèƒ½ä½“çŠ¶æ€
            env_description = self.bridge.describe_environment_natural_language(agents, sim_config)
        else:  # detail_level == 'full'
            # æè¿°å®Œæ•´ç¯å¢ƒ
            env_description = self.bridge.describe_environment_natural_language(agents, sim_config)

        # æ›´æ–°ç¼“å­˜
        self.env_description_cache = env_description
        return env_description

    def _parse_prompt(self) -> str:
        """æ„å»ºæç¤ºè¯"""
        # å†å²è®°å½•æ‘˜è¦
        history_summary = ""
        if self.history:
            # ä½¿ç”¨é…ç½®çš„å†å²é•¿åº¦ï¼Œå¦‚æœæ˜¯æ— é™åˆ¶(-1)åˆ™ä½¿ç”¨æ‰€æœ‰å†å²
            max_display_entries = len(self.history) if self.max_chat_history is None else self.max_chat_history
            history_summary = self.prompt_manager.format_history(self.mode, self.history, max_entries=max_display_entries)

        # è·å–ç¯å¢ƒæè¿°ï¼ˆæ ¹æ®é…ç½®ï¼‰
        env_description = self._get_environment_description()

        # æ ¼å¼åŒ–æç¤ºè¯
        prompt = self.prompt_manager.get_formatted_prompt(
            self.mode,
            "user_prompt",
            task_description=self.task_description,
            history_summary=history_summary,
            environment_description=env_description
        )

        return prompt
    
    def decide_action(self) -> str:
        """å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ"""
        # æ„å»ºæç¤ºè¯
        prompt = self._parse_prompt()

        # è®°å½•åˆ°å¯¹è¯å†å²
        self.chat_history.append({"role": "user", "content": prompt})

        # æ§åˆ¶å¯¹è¯å†å²é•¿åº¦
        if self.max_chat_history is not None and len(self.chat_history) > self.max_chat_history:
            self.chat_history = self.chat_history[-self.max_chat_history:]

        # è°ƒç”¨LLMç”Ÿæˆå“åº”ï¼Œä½¿ç”¨åŠ¨æ€ç³»ç»Ÿæç¤ºè¯
        system_prompt = self._get_system_prompt()
        response = self.llm.generate_chat(self.chat_history, system_message=system_prompt)

        # è§£æå“åº”ä¸­çš„åŠ¨ä½œå‘½ä»¤
        action = self._extract_action(response)

        # è®°å½•LLMäº¤äº’åˆ°è½¨è¿¹è®°å½•å™¨ï¼ˆä½¿ç”¨æ–°æ¥å£ï¼‰
        if self.trajectory_recorder:
            # è·å–tokenä½¿ç”¨æƒ…å†µ
            tokens_used = getattr(self.llm, 'last_token_usage', {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0})
            response_time_ms = getattr(self.llm, 'last_response_time_ms', 0.0)

            # è·å–å½“å‰ä»»åŠ¡ç´¢å¼•ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼1
            current_task_index = getattr(self, 'current_task_index', 1)

            # å•æ™ºèƒ½ä½“ä½¿ç”¨æ­¥æ•°ä½œä¸ºäº¤äº’ç´¢å¼•
            self.trajectory_recorder.record_llm_interaction(
                task_index=current_task_index,  # ä½¿ç”¨å½“å‰ä»»åŠ¡ç´¢å¼•
                interaction_index=self.step_count,  # ä½¿ç”¨æ­¥æ•°ä½œä¸ºäº¤äº’ç´¢å¼•
                prompt=prompt,
                response=response,
                tokens_used=tokens_used,
                response_time_ms=response_time_ms,
                extracted_action=action
            )

        # è®°å½•LLMå“åº”åˆ°å¯¹è¯å†å²
        self.chat_history.append({"role": "assistant", "content": response})

        # ä¿å­˜å®Œæ•´çš„LLMå›å¤ï¼Œç”¨äºå†å²è®°å½•
        self.last_llm_response = response

        # ä¿å­˜æœ€åä¸€æ¬¡LLMäº¤äº’ä¿¡æ¯ï¼ˆç”¨äºæ–°è¯„æµ‹å™¨ï¼‰
        self.last_llm_interaction = {
            'prompt': prompt,
            'response': response,
            'tokens_used': getattr(self.llm, 'last_token_usage', {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}),
            'response_time_ms': getattr(self.llm, 'last_response_time_ms', 0.0),
            'extracted_action': action
        }

        return action

    def get_llm_interaction_info(self) -> Dict[str, Any]:
        """è·å–æœ€åä¸€æ¬¡LLMäº¤äº’çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ–°è¯„æµ‹å™¨ï¼‰"""
        return getattr(self, 'last_llm_interaction', None)

    def _extract_action(self, response: str) -> str:
        """ä»LLMå“åº”ä¸­æå–åŠ¨ä½œå‘½ä»¤"""
        lines = response.split('\n')

        # ç›´æ¥åŒ¹é…"Action:"æ ¼å¼ï¼Œæå–åé¢çš„å‘½ä»¤
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # åŒ¹é…"Action:"æ ¼å¼ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            if line.startswith('Action:') or line.startswith('åŠ¨ä½œï¼š') or line.startswith('åŠ¨ä½œ:'):
                # æå–å†’å·åçš„å†…å®¹ä½œä¸ºåŠ¨ä½œå‘½ä»¤
                if line.startswith('Action:'):
                    action = line[7:].strip()  # å»æ‰"Action:"å‰ç¼€
                elif line.startswith('åŠ¨ä½œï¼š'):
                    action = line[3:].strip()  # å»æ‰"åŠ¨ä½œï¼š"å‰ç¼€
                else:
                    action = line[3:].strip()  # å»æ‰"åŠ¨ä½œ:"å‰ç¼€

                # å»é™¤å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
                action = action.rstrip('ã€‚ï¼Œï¼ï¼Ÿ.!?')

                if action:
                    return action

        # å¦‚æœæ²¡æ‰¾åˆ°"Action:"æˆ–"åŠ¨ä½œï¼š"æ ¼å¼ï¼Œè¿”å›æœ€åä¸€è¡Œéç©ºæ–‡æœ¬ä½œä¸ºå›é€€
        for line in reversed(lines):
            if line.strip():
                return line.strip()

        return response.strip()

    def record_action(self, action: str, result: Dict[str, Any]) -> None:
        """
        è®°å½•åŠ¨ä½œåˆ°å†å²ï¼ŒåŒ…å«å®Œæ•´çš„LLMå›å¤ï¼ˆæ€è€ƒ+åŠ¨ä½œï¼‰

        Args:
            action: åŠ¨ä½œå‘½ä»¤
            result: æ‰§è¡Œç»“æœ
        """
        # åˆ›å»ºåŒ…å«å®Œæ•´LLMå›å¤çš„å†å²è®°å½•
        history_entry = {
            'action': action,
            'result': result,
            'llm_response': getattr(self, 'last_llm_response', ''),  # åŒ…å«æ€è€ƒå†…å®¹çš„å®Œæ•´å›å¤
        }

        self.history.append(history_entry)

        # ä¿æŒå†å²é•¿åº¦åœ¨é™åˆ¶èŒƒå›´å†…
        if self.max_history != float('inf') and len(self.history) > self.max_history:
            self.history = self.history[-int(self.max_history):]

    def step(self) -> Tuple[ActionStatus, str, Optional[Dict[str, Any]]]:
        """æ‰§è¡Œä¸€æ­¥æ™ºèƒ½ä½“è¡Œä¸º"""
        # å¢åŠ æ­¥æ•°è®¡æ•°å™¨
        self.step_count += 1

        # å†³å®šè¦æ‰§è¡Œçš„åŠ¨ä½œ
        action = self.decide_action()
        action = action.strip()

        # è®°å½•æ‰§è¡Œå‘½ä»¤
        logger.info("Executing command: %s", action)

        # æ‰§è¡ŒåŠ¨ä½œ
        status, message, result = self.bridge.process_command(self.agent_id, action)

        # è®°å½•å†å²
        self.record_action(action, {"status": status, "message": message, "result": result})

        # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°
        if status == ActionStatus.FAILURE or status == ActionStatus.INVALID:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0

        return status, message, result