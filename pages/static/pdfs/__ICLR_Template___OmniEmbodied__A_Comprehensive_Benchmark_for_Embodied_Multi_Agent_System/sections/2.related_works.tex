\section{Related Works}

% Our work builds upon and extends recent advances in embodied AI evaluation. Existing benchmarks such as ALFRED \citep{shridhar2020alfredbenchmarkinterpretinggrounded} and BEHAVIOR-1K \citep{li2024behavior} primarily model physical properties through discrete states (e.g., open/closed, picked/placed), limiting assessment of continuous attribute reasoning. For tool usage, benchmarks like PARTNR \citep{chang2024partnrbenchmarkplanningreasoning} employ predefined tool configurations with fixed action spaces, whereas \framework introduces dynamic capability acquisition where agents must reason about tool needs based on task requirements. In multi-agent settings, current approaches either specify explicit collaboration instructions or optimize for efficiency \citep{zhang2024buildingcooperativeembodiedagents, yang2025embodiedbench}, while our implicit collaboration design requires agents to autonomously recognize when physical constraints necessitate coordination. These design choices enable \framework to evaluate genuine reasoning about embodied constraints rather than execution of predetermined strategies. A comprehensive comparison with related work is provided in Appendix A.

Prior embodied benchmarks have made significant contributions to task evaluation but differ fundamentally in their approach to physical reasoning and collaboration. While ALFRED \citep{shridhar2020alfredbenchmarkinterpretinggrounded} and BEHAVIOR-1K \citep{li2024behavior} provide extensive task coverage, they model physical states through discrete representations (e.g., binary door states, picked/placed objects) rather than continuous attributes necessary for reasoning about weight, temperature, or material properties. Tool usage evaluation spans from low-level manipulation in RoCo \citep{mandi2024roco} to high-level planning in PARTNR \citep{chang2024partnrbenchmarkplanningreasoning}, yet both maintain static action spaces determined at initialization, preventing assessment of dynamic capability acquisition. Recent multi-agent benchmarks including TDW-MAT \citep{zhang2024buildingcooperativeembodiedagents} and EmbodiedBench \citep{yang2025embodiedbench} advance collaboration evaluation through load constraints and task allocation optimization, but rely on explicit task division instructions or efficiency-driven participation rather than collaboration that emerges from physical constraints. In contrast, \framework introduces continuous property reasoning with 6,381 distinct attributes, dynamic tool-capability binding that expands action spaces during execution, and implicit collaboration where agents must autonomously recognize when tasks exceed individual capacities based on physical constraints, fundamentally shifting evaluation from instruction compliance to constraint-based reasoning. A comprehensive comparison with related work is provided in Appendix \ref{sec:related_works}.