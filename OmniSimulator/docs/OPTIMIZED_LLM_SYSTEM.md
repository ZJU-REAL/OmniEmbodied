# Optimized LLM Interaction System

## Overview

This document introduces the optimized LLM interaction system in embodied_framework, which uses the latest simulator APIs to implement dynamic prompt generation, intelligent error handling, and learning mechanisms.

## System Architecture

### Core Components

1. **LLMClient** - Unified LLM client
2. **DynamicPromptManager** - Dynamic prompt manager
3. **OptimizedLLMAgent** - Optimized LLM agent
4. **FinalOptimizedLLMAgent** - Final optimized version (with learning mechanism)

### Technical Features

#### âœ… Dynamic Prompt Generation
- **Real-time Action Retrieval**: Uses `get_agent_supported_actions_description()` API
- **Complete 3564-character Action Description**: Includes basic actions, attribute actions, and collaborative actions
- **Intelligent Content Optimization**: Automatically adjusts prompt length and structure
- **Context Awareness**: Dynamically adjusts content based on agent state

#### âœ… Multi-API Provider Support
- **OpenAI API**: Supports GPT-3.5/GPT-4 and other models
- **Custom API**: Supports third-party APIs like DeepSeek
- **Unified Interface**: Seamless switching between different LLM providers
- **Error Handling**: Automatic retry and fallback mechanisms

#### âœ… Intelligent Learning Mechanism
- **Failed Action Recording**: Avoids repeating failed actions
- **Environment Learning**: Records explored rooms and discovered objects
- **Adaptive Retry**: Allows intelligent retry for certain action types
- **Progress Tracking**: Real-time monitoring of task completion status

## API Updates

### New Simulator APIs

```python
# è·å–æ™ºèƒ½ä½“æ”¯æŒçš„åŠ¨ä½œæè¿°
description = bridge.get_agent_supported_actions_description(agent_id)

# è¿”å›å®Œæ•´çš„åŠ¨ä½œè¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š
# - åŸºç¡€åŠ¨ä½œ (GOTO, GRAB, PLACE, LOOK, EXPLORE)
# - å±æ€§åŠ¨ä½œ (OPEN, CLOSE, TURN_ON, TURN_OFF, CONNECT, etc.)
# - åä½œåŠ¨ä½œ (CORP_* ç³»åˆ—)
```

### ä¼˜åŒ–çš„æç¤ºè¯ç»“æ„

```
ğŸ¤– ç³»ç»Ÿè§’è‰²å®šä¹‰
ğŸ“‹ æ ¸å¿ƒèƒ½åŠ›æè¿°  
âš¡ æ™ºèƒ½è¡Œä¸ºå‡†åˆ™
ğŸ¯ å…³é”®æ“ä½œæç¤º
ğŸ“ å“åº”æ ¼å¼è¦æ±‚
âš ï¸ é‡è¦çº¦æŸ

ğŸ”§ å¯æ‰§è¡ŒåŠ¨ä½œ (3564å­—ç¬¦)
  - åŸºç¡€åŠ¨ä½œ
  - å±æ€§åŠ¨ä½œ (æ— éœ€å·¥å…·)
  - åä½œåŠ¨ä½œ

ğŸ  ç¯å¢ƒä¿¡æ¯
ğŸ¯ å½“å‰ä»»åŠ¡
ğŸ“ å½“å‰çŠ¶æ€
ğŸ“š å­¦ä¹ ä¿¡æ¯
ğŸ’¡ æ™ºèƒ½å»ºè®®
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from embodied_framework.utils.llm_client import create_llm_client
from examples.optimized_llm_agent_demo import OptimizedLLMAgent

# åˆ›å»ºLLMå®¢æˆ·ç«¯
llm_client = create_llm_client()

# åˆ›å»ºä¼˜åŒ–çš„æ™ºèƒ½ä½“
agent = OptimizedLLMAgent(bridge, agent_id, llm_client)

# æ‰§è¡Œä¸€æ­¥
status, message, result = agent.step()
```

### é«˜çº§ä½¿ç”¨ï¼ˆåŒ…å«å­¦ä¹ æœºåˆ¶ï¼‰

```python
from examples.final_optimized_llm_demo import FinalOptimizedLLMAgent

# åˆ›å»ºæœ€ç»ˆä¼˜åŒ–çš„æ™ºèƒ½ä½“
agent = FinalOptimizedLLMAgent(bridge, agent_id, llm_client)

# è¿è¡Œæ™ºèƒ½ä»»åŠ¡æ‰§è¡Œ
final_stats = agent.run_intelligent_task_execution(max_steps=20)
```

## é…ç½®è¯´æ˜

### LLMé…ç½®æ–‡ä»¶ (`config/defaults/llm_config.yaml`)

```yaml
# LLMæ¨ç†æ–¹å¼è®¾ç½®
mode: "api"  # å¯é€‰å€¼: "api" æˆ– "vllm"

# APIè°ƒç”¨æ–¹å¼é…ç½®
api:
  provider: "custom"  # å¯é€‰å€¼: "openai" æˆ– "custom"
  
  # è‡ªå®šä¹‰ç«¯ç‚¹APIé…ç½®
  custom:
    model: "deepseek-chat"
    temperature: 0.1
    max_tokens: 4096
    api_key: "your-api-key"
    endpoint: "https://api.deepseek.com"
```

### Supported Models

#### OpenAI
- `gpt-3.5-turbo`
- `gpt-4`
- `gpt-4-turbo`

#### Custom API (DeepSeek Example)
- `deepseek-chat`
- `deepseek-reasoner`

## Performance Metrics

### Test Results

**Optimized Version Demo**:
- âœ… **Success Rate**: 73.3% (11/15 actions successful)
- âœ… **Runtime**: 151.3 seconds
- âœ… **Action Frequency**: 5.9 actions/minute
- âœ… **LLM Calls**: 15 times
- âœ… **Environment Exploration**: Discovered 42 new objects

**Final Optimized Version**:
- âœ… **Learning Mechanism**: Records failed actions, avoids repetition
- âœ… **Intelligent Suggestions**: Generates action suggestions based on history
- âœ… **Adaptive Retry**: Intelligently determines whether to retry failed actions
- âœ… **Progress Monitoring**: Real-time tracking of task completion status

## Key Optimizations

### 1. Prompt Optimization
- **Length Control**: Complete context of 4763 characters
- **Structured Design**: Clear segmentation and formatting
- **Dynamic Content**: Real-time updates based on state
- **Intelligent Filtering**: Avoids redundant information

### 2. Error Handling
- **Retry Mechanism**: Automatic retry on LLM call failures
- **Format Validation**: Strict JSON response validation
- **Action Validation**: Intelligent action validity checking
- **Fallback Strategy**: Multi-level error recovery

### 3. Learning Mechanism
- **Experience Accumulation**: Records successful and failed experiences
- **Environment Mapping**: Builds cognitive map of environment
- **Strategy Optimization**: Adjusts behavioral strategies based on history
- **Progress Awareness**: Real-time understanding of task progress

## Extension Guide

### Adding New LLM Providers

```python
class CustomLLMClient(LLMClient):
    def _init_custom_provider(self):
        # å®ç°æ–°æä¾›å•†çš„åˆå§‹åŒ–é€»è¾‘
        pass
    
    def _custom_chat_completion(self, messages, **kwargs):
        # å®ç°æ–°æä¾›å•†çš„APIè°ƒç”¨
        pass
```

### è‡ªå®šä¹‰å­¦ä¹ ç­–ç•¥

```python
class CustomLearningAgent(FinalOptimizedLLMAgent):
    def _learn_from_action(self, action, status, message, result):
        # å®ç°è‡ªå®šä¹‰çš„å­¦ä¹ é€»è¾‘
        super()._learn_from_action(action, status, message, result)
        # æ·»åŠ é¢å¤–çš„å­¦ä¹ æœºåˆ¶
```

## æœ€ä½³å®è·µ

### 1. æç¤ºè¯è®¾è®¡
- ä½¿ç”¨æ¸…æ™°çš„ç»“æ„åŒ–æ ¼å¼
- åŒ…å«å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- é¿å…è¿‡é•¿çš„æç¤ºè¯
- ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

### 2. é”™è¯¯å¤„ç†
- å®ç°å¤šå±‚æ¬¡çš„é”™è¯¯æ¢å¤
- è®°å½•å’Œåˆ†æå¤±è´¥æ¨¡å¼
- æä¾›æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
- é¿å…æ— é™é‡è¯•å¾ªç¯

### 3. æ€§èƒ½ä¼˜åŒ–
- åˆç†æ§åˆ¶LLMè°ƒç”¨é¢‘ç‡
- ç¼“å­˜é‡å¤çš„è®¡ç®—ç»“æœ
- ä¼˜åŒ–æç¤ºè¯é•¿åº¦
- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **LLMå“åº”æ ¼å¼é”™è¯¯**
   - æ£€æŸ¥JSONæ ¼å¼è¦æ±‚
   - éªŒè¯æç¤ºè¯æ¸…æ™°åº¦
   - è°ƒæ•´temperatureå‚æ•°

2. **åŠ¨ä½œéªŒè¯å¤±è´¥**
   - æ£€æŸ¥åŠ¨ä½œå‘½ä»¤æ ¼å¼
   - éªŒè¯ç‰©ä½“IDæœ‰æ•ˆæ€§
   - ç¡®è®¤ç¯å¢ƒçŠ¶æ€

3. **APIè°ƒç”¨å¤±è´¥**
   - æ£€æŸ¥APIå¯†é’¥é…ç½®
   - éªŒè¯ç½‘ç»œè¿æ¥
   - ç¡®è®¤APIé¢åº¦

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger().setLevel(logging.DEBUG)

# æ£€æŸ¥æç¤ºè¯å†…å®¹
prompt = agent.prompt_manager.generate_full_context_prompt(agent_id)
print(f"æç¤ºè¯é•¿åº¦: {len(prompt)}")

# ç›‘æ§LLMè°ƒç”¨
stats = agent.get_statistics()
print(f"LLMè°ƒç”¨æ¬¡æ•°: {stats['llm_calls']}")
```

## æ€»ç»“

ä¼˜åŒ–çš„LLMäº¤äº’ç³»ç»Ÿä¸ºembodied_frameworkæä¾›äº†å¼ºå¤§çš„æ™ºèƒ½å†³ç­–èƒ½åŠ›ï¼š

- ğŸš€ **æœ€æ–°APIé›†æˆ**: ä½¿ç”¨æ¨¡æ‹Ÿå™¨æä¾›çš„å®æ—¶åŠ¨ä½œæè¿°
- ğŸ§  **æ™ºèƒ½æç¤ºè¯**: 4763å­—ç¬¦çš„åŠ¨æ€ä¸Šä¸‹æ–‡ç”Ÿæˆ
- ğŸ”„ **å­¦ä¹ æœºåˆ¶**: ä»ç»éªŒä¸­å­¦ä¹ å’Œé€‚åº”
- âš¡ **é«˜æ€§èƒ½**: 73.3%çš„åŠ¨ä½œæˆåŠŸç‡
- ğŸ›¡ï¸ **å¥å£®æ€§**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

è¿™ä¸ªç³»ç»Ÿç¡®ä¿äº†æ™ºèƒ½ä½“èƒ½å¤Ÿä¸çœŸå®çš„LLMè¿›è¡Œæœ‰æ•ˆäº¤äº’ï¼Œå®ç°å¤æ‚ä»»åŠ¡çš„è‡ªä¸»å®Œæˆã€‚
