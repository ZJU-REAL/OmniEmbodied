#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»å…¥å£è„šæœ¬ - é‡æ„åçš„è¯„æµ‹å™¨å…¥å£
"""

import argparse
import logging
import sys
import signal
from typing import Dict, Any

from .evaluation_interface import EvaluationInterface


def setup_logging(log_level: str = 'INFO'):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('evaluation.log', encoding='utf-8')
        ]
    )


def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ç¡®ä¿ä¸­æ–­æ—¶ä¿å­˜æ•°æ®"""
    _ = signum, frame  # é¿å…æœªä½¿ç”¨å˜é‡è­¦å‘Š
    logger = logging.getLogger(__name__)
    logger.info("ğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")

    # TODO: å®ç°ä¸­æ–­æ—¶çš„æ•°æ®ä¿å­˜é€»è¾‘
    # è¿™é‡Œéœ€è¦è®¿é—®å½“å‰è¿è¡Œçš„è¯„æµ‹å™¨å®ä¾‹æ¥ä¿å­˜æ•°æ®

    logger.info("âœ… æ•°æ®ä¿å­˜å®Œæˆï¼Œç¨‹åºé€€å‡º")
    sys.exit(0)


def main():
    """ä¸»å‡½æ•°"""
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = create_argument_parser()
    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)

    try:
        # è§£æåœºæ™¯é€‰æ‹©
        scenario_selection = EvaluationInterface.parse_scenario_string(args.scenarios)

        # æ·»åŠ ä»»åŠ¡ç­›é€‰
        if args.task_categories:
            task_filter = {}
            if args.task_categories:
                task_filter['categories'] = args.task_categories

            scenario_selection['task_filter'] = task_filter

        # éªŒè¯é…ç½®æ–‡ä»¶
        if not EvaluationInterface.validate_config_file(args.config):
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            available_configs = EvaluationInterface.list_available_configs()
            logger.info(f"å¯ç”¨é…ç½®: {available_configs}")
            return 1

        # æ˜¾ç¤ºè¯„æµ‹ä¿¡æ¯
        scenario_count = EvaluationInterface.get_scenario_count(scenario_selection)
        logger.info(f"ğŸ¯ è¯„æµ‹é…ç½®:")
        logger.info(f"   é…ç½®æ–‡ä»¶: {args.config}")
        logger.info(f"   æ™ºèƒ½ä½“ç±»å‹: {args.agent_type}")
        logger.info(f"   ä»»åŠ¡ç±»å‹: {args.task_type}")
        logger.info(f"   åœºæ™¯é€‰æ‹©: {args.scenarios} ({scenario_count} ä¸ªåœºæ™¯)")
        if args.task_categories:
            logger.info(f"   ä»»åŠ¡ç±»åˆ«ç­›é€‰: {args.task_categories}")
        logger.info(f"   è‡ªå®šä¹‰åç¼€: {args.suffix}")

        # è¿è¡Œè¯„æµ‹
        results = EvaluationInterface.run_evaluation(
            config_file=args.config,
            agent_type=args.agent_type,
            task_type=args.task_type,
            scenario_selection=scenario_selection,
            custom_suffix=args.suffix
        )

        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        display_results_summary(results)

        logger.info("ğŸ‰ è¯„æµ‹å®Œæˆ!")
        return 0

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­è¯„æµ‹")
        return 1
    except Exception as e:
        logger.error(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
        return 1


def create_argument_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='OmniEmbodiedè¯„æµ‹å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å•æ™ºèƒ½ä½“Sequentialæ¨¡å¼è¯„æµ‹
  python -m evaluation.evaluator --config single_agent_config --agent-type single --task-type sequential --scenarios 00001-00010 --suffix test1

  # å¤šæ™ºèƒ½ä½“Independentæ¨¡å¼è¯„æµ‹
  python -m evaluation.evaluator --config decentralized_config --agent-type multi --task-type independent --scenarios all --suffix experiment1

  # ç‰¹å®šåœºæ™¯åˆ—è¡¨è¯„æµ‹
  python -m evaluation.evaluator --config centralized_config --agent-type multi --task-type combined --scenarios 00001,00005,00010 --suffix selected_scenes

  # å•ä¸ªåœºæ™¯å¿«é€Ÿæµ‹è¯•
  python -m evaluation.evaluator --config single_agent_config --agent-type single --task-type sequential --scenarios 00001 --suffix quick_test

  # ç­›é€‰ç‰¹å®šä»»åŠ¡ç±»åˆ«
  python -m evaluation.evaluator --config single_agent_config --agent-type single --task-type sequential --scenarios all --task-categories direct_command attribute_reasoning --suffix filtered_tasks

  # åªè¯„æµ‹å·¥å…·ä½¿ç”¨ä»»åŠ¡
  python -m evaluation.evaluator --config single_agent_config --agent-type single --task-type sequential --scenarios all --task-categories tool_use --suffix tool_use_tasks

  # åªè¯„æµ‹åä½œä»»åŠ¡
  python -m evaluation.evaluator --config centralized_config --agent-type multi --task-type combined --scenarios all --task-categories explicit_collaboration implicit_collaboration --suffix collaboration_tasks
        """
    )

    parser.add_argument(
        '--config',
        required=True,
        help='é…ç½®æ–‡ä»¶å (single_agent_config, centralized_config, decentralized_config)'
    )

    parser.add_argument(
        '--agent-type',
        required=True,
        choices=['single', 'multi'],
        help='æ™ºèƒ½ä½“ç±»å‹'
    )

    parser.add_argument(
        '--task-type',
        required=True,
        choices=['sequential', 'combined', 'independent'],
        help='ä»»åŠ¡ç±»å‹'
    )

    parser.add_argument(
        '--scenarios',
        default='all',
        help='åœºæ™¯é€‰æ‹©: all, 00001-00010, 00001,00003,00005'
    )

    parser.add_argument(
        '--task-categories',
        nargs='*',
        help='ä»»åŠ¡ç±»åˆ«ç­›é€‰: direct_command attribute_reasoning tool_use spatial_reasoning'
    )



    parser.add_argument(
        '--suffix',
        default='evaluation',
        help='è‡ªå®šä¹‰åç¼€'
    )

    parser.add_argument(
        '--log-level',
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='æ—¥å¿—çº§åˆ«'
    )

    parser.add_argument(
        '--list-configs',
        action='store_true',
        help='åˆ—å‡ºå¯ç”¨çš„é…ç½®æ–‡ä»¶'
    )

    return parser


def display_results_summary(results: Dict[str, Any]):
    """æ˜¾ç¤ºç»“æœæ‘˜è¦"""
    logger = logging.getLogger(__name__)

    run_info = results.get('run_info', {})
    overall_summary = results.get('overall_summary', {})
    task_category_stats = results.get('task_category_statistics', {})

    logger.info("ğŸ“Š è¯„æµ‹ç»“æœæ‘˜è¦:")
    logger.info(f"   è¿è¡Œåç§°: {run_info.get('run_name', 'Unknown')}")
    logger.info(f"   æ€»è€—æ—¶: {run_info.get('total_duration', 0):.2f} ç§’")
    logger.info(f"   åœºæ™¯æ•°é‡: {overall_summary.get('total_scenarios', 0)}")
    logger.info(f"   ä»»åŠ¡æ€»æ•°: {overall_summary.get('total_tasks', 0)}")
    logger.info(f"   å®Œæˆä»»åŠ¡: {overall_summary.get('total_completed_tasks', 0)}")
    logger.info(f"   æ€»ä½“å®Œæˆç‡: {overall_summary.get('overall_completion_rate', 0):.2%}")
    logger.info(f"   æ¨¡å‹å‡†ç¡®ç‡: {overall_summary.get('overall_completion_accuracy', 0):.2%}")

    if task_category_stats:
        logger.info("ï¿½ ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
        for category, stats in task_category_stats.items():
            completion_rate = stats.get('completion_rate', 0)
            logger.info(f"   {category}: {completion_rate:.2%}")

    logger.info(f"ï¿½ ç»“æœä¿å­˜åœ¨: output/{run_info.get('run_name', 'unknown')}/")


def list_configs_command():
    """åˆ—å‡ºå¯ç”¨é…ç½®çš„å‘½ä»¤"""
    logger = logging.getLogger(__name__)

    configs = EvaluationInterface.list_available_configs()
    if configs:
        logger.info("ğŸ“‹ å¯ç”¨çš„é…ç½®æ–‡ä»¶:")
        for config in configs:
            logger.info(f"   - {config}")
    else:
        logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•é…ç½®æ–‡ä»¶")


if __name__ == '__main__':
    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if len(sys.argv) > 1 and sys.argv[1] == '--list-configs':
        setup_logging()
        list_configs_command()
        sys.exit(0)

    # è¿è¡Œä¸»ç¨‹åº
    exit_code = main()
    sys.exit(exit_code)
