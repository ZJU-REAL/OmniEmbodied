"""
è¯„æµ‹ç®¡ç†å™¨ - ç»Ÿä¸€è¯„æµ‹ç®¡ç†å’Œåœºæ™¯çº§å¹¶è¡Œæ‰§è¡Œ
"""

import os
import json
import yaml
import csv
import logging
import signal
import sys
from typing import Dict, List, Any, Optional
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

from config.config_manager import ConfigManager
from .scenario_selector import ScenarioSelector
from .scenario_executor import ScenarioExecutor

logger = logging.getLogger(__name__)


class EvaluationManager:
    """è¯„æµ‹ç®¡ç†å™¨ - ç»Ÿä¸€è¯„æµ‹ç®¡ç†å’Œåœºæ™¯çº§å¹¶è¡Œæ‰§è¡Œ"""
    
    def __init__(self, config_file: str, agent_type: str, task_type: str,
                 scenario_selection: Dict[str, Any], custom_suffix: str = None):
        """
        åˆå§‹åŒ–è¯„æµ‹ç®¡ç†å™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶å
            agent_type: æ™ºèƒ½ä½“ç±»å‹ ('single', 'multi')
            task_type: ä»»åŠ¡ç±»å‹ ('sequential', 'combined', 'independent')
            scenario_selection: åœºæ™¯é€‰æ‹©é…ç½®
            custom_suffix: è‡ªå®šä¹‰åç¼€
        """
        self.config_file = config_file
        self.agent_type = agent_type
        self.task_type = task_type
        self.custom_suffix = custom_suffix or 'demo'

        # æ˜ å°„agent_typeåˆ°å®é™…çš„æ™ºèƒ½ä½“æ¨¡å¼
        self.actual_agent_type = self._map_agent_type(agent_type, config_file)
        
        # åŠ è½½é…ç½®
        self.config_manager = ConfigManager()
        self.config = self.config_manager.get_config(config_file)
        
        # é€‰æ‹©åœºæ™¯å’Œä»»åŠ¡
        scenario_result = ScenarioSelector.get_scenario_list(self.config, scenario_selection)
        self.scenario_list = scenario_result['scenarios']
        self.task_indices = scenario_result['task_indices']
        
        # ç”Ÿæˆè¿è¡Œåç§°å’Œè¾“å‡ºç›®å½•
        self.run_name = self._generate_run_name()
        self.output_dir = self._create_output_directory()
        
        # å¹¶è¡Œé…ç½®
        parallel_config = self.config.get('parallel_evaluation', {})
        max_parallel = parallel_config.get('scenario_parallelism', {}).get('max_parallel_scenarios', 2)
        self.parallel_count = min(len(self.scenario_list), max_parallel)

        # è¿è¡Œå¼€å§‹æ—¶é—´
        self.start_time = datetime.now().isoformat()

        # è¿è¡ŒID
        self.run_id = self.run_name

        # æå–å¹¶ä¿å­˜æ¨¡å‹åç§°
        agent_config = self.config.get('agent_config', {})
        model_info = self._extract_model_info(agent_config)

        # ä¿å­˜æ¨¡å‹åç§°ä¸ºå®ä¾‹å˜é‡
        provider = model_info.get('provider', 'unknown')
        model_name = model_info.get('model_name', 'unknown')
        self.model_name = f"{provider}:{model_name}" if provider != 'unknown' and model_name != 'unknown' else 'unknown'

        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        self._register_signal_handlers()

        # ä¿å­˜å®éªŒé…ç½®
        self._save_experiment_config()

        logger.info(f"ğŸš€ è¯„æµ‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.run_name}")
        logger.info(f"ğŸ“Š åœºæ™¯æ•°é‡: {len(self.scenario_list)}, å¹¶è¡Œæ•°: {self.parallel_count}")

    def _map_agent_type(self, agent_type: str, config_file: str) -> str:
        """
        å°†è¯„æµ‹æ¥å£çš„agent_typeæ˜ å°„åˆ°å®é™…çš„æ™ºèƒ½ä½“æ¨¡å¼

        Args:
            agent_type: è¯„æµ‹æ¥å£ä¼ å…¥çš„ç±»å‹ ('single', 'multi')
            config_file: é…ç½®æ–‡ä»¶å

        Returns:
            str: å®é™…çš„æ™ºèƒ½ä½“æ¨¡å¼ ('single', 'centralized', 'decentralized')
        """
        if agent_type == 'single':
            return 'single'
        elif agent_type == 'multi':
            # æ ¹æ®é…ç½®æ–‡ä»¶ååˆ¤æ–­å¤šæ™ºèƒ½ä½“æ¨¡å¼
            if 'centralized' in config_file:
                return 'centralized'
            elif 'decentralized' in config_file:
                return 'decentralized'
            else:
                # é»˜è®¤ä¸ºä¸­å¿ƒåŒ–æ¨¡å¼
                return 'centralized'
        else:
            # ç›´æ¥è¿”å›ï¼Œæ”¯æŒç›´æ¥ä¼ å…¥å…·ä½“æ¨¡å¼
            return agent_type
    
    def _generate_run_name(self) -> str:
        """ç”Ÿæˆè¿è¡Œåç§°"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        scenario_range = self._format_scenario_range()
        return f"{timestamp}_{self.agent_type}_{self.task_type}_{scenario_range}_{self.custom_suffix}"
    
    def _format_scenario_range(self) -> str:
        """æ ¼å¼åŒ–åœºæ™¯èŒƒå›´å­—ç¬¦ä¸²"""
        if len(self.scenario_list) == 1:
            return self.scenario_list[0]
        elif len(self.scenario_list) <= 3:
            return "_".join(self.scenario_list)
        else:
            return f"{self.scenario_list[0]}_to_{self.scenario_list[-1]}"
    
    def _create_output_directory(self) -> str:
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        base_output_dir = self.config.get('evaluation', {}).get('output', {}).get('output_directory', 'output')
        output_dir = os.path.join(base_output_dir, self.run_name)
        
        # åˆ›å»ºå¿…è¦çš„å­ç›®å½•
        subdirs = ['trajectories', 'llm_qa']
        for subdir in subdirs:
            os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)
        
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir}")
        return output_dir

    def _save_experiment_config(self):
        """ä¿å­˜æœ¬æ¬¡å®éªŒçš„é…ç½®ä¿¡æ¯"""
        try:
            # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
            agent_config = self.config.get('agent_config', {})
            model_info = self._extract_model_info(agent_config)

            # æ„å»ºå®éªŒé…ç½®
            experiment_config = {
                'experiment_info': {
                    'run_name': self.run_name,
                    'timestamp': datetime.now().isoformat(),
                    'config_file': self.config_file,
                    'agent_type': self.agent_type,
                    'task_type': self.task_type,
                    'custom_suffix': self.custom_suffix
                },
                'model_config': model_info,
                'scenarios': {
                    'scenario_list': self.scenario_list,
                    'scenario_count': len(self.scenario_list),
                    'selection_mode': self.config.get('evaluation', {}).get('scenario_selection', {}).get('mode', 'unknown')
                },
                'execution_config': {
                    'parallel_count': self.parallel_count,
                    'max_total_steps': self.config.get('execution', {}).get('max_total_steps', 200),
                    'max_steps_per_task': self.config.get('execution', {}).get('max_steps_per_task', 50)
                },
                'evaluation_settings': self.config.get('evaluation', {}),
                'parallel_settings': self.config.get('parallel_evaluation', {})
            }

            # ä¿å­˜ä¸ºYAMLæ–‡ä»¶
            config_file = os.path.join(self.output_dir, 'experiment_config.yaml')
            with open(config_file, 'w', encoding='utf-8') as f:
                yaml.dump(experiment_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            logger.info(f"ğŸ“‹ å®éªŒé…ç½®å·²ä¿å­˜: experiment_config.yaml")

        except Exception as e:
            logger.error(f"ä¿å­˜å®éªŒé…ç½®å¤±è´¥: {e}")

    def _extract_model_info(self, agent_config: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æ™ºèƒ½ä½“é…ç½®ä¸­æå–æ¨¡å‹ä¿¡æ¯"""
        model_info = {
            'agent_class': agent_config.get('agent_class', 'unknown'),
            'max_failures': agent_config.get('max_failures', 3),
            'max_history': agent_config.get('max_history', 50)
        }

        # å°è¯•ä»LLMé…ç½®ä¸­æå–æ¨¡å‹ä¿¡æ¯
        try:
            llm_config_manager = ConfigManager()
            llm_config = llm_config_manager.get_config('llm_config')

            if llm_config:
                model_info['llm_mode'] = llm_config.get('mode', 'unknown')

                # è·å–APIé…ç½®
                api_config = llm_config.get('api', {})
                provider = api_config.get('provider', 'unknown')
                model_info['provider'] = provider

                # æ ¹æ®providerè·å–å…·ä½“æ¨¡å‹ä¿¡æ¯
                if provider in api_config:
                    provider_config = api_config[provider]
                    model_info['model_name'] = provider_config.get('model', 'unknown')
                    model_info['temperature'] = provider_config.get('temperature', 0.7)
                    model_info['max_tokens'] = provider_config.get('max_tokens', 1024)

                    # ä¸ä¿å­˜APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯
                    if 'endpoint' in provider_config:
                        model_info['api_endpoint'] = provider_config['endpoint']

        except Exception as e:
            logger.warning(f"æ— æ³•æå–LLMé…ç½®ä¿¡æ¯: {e}")

        return model_info

    def run_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œè¯„æµ‹"""
        logger.info(f"ğŸ¯ å¼€å§‹è¯„æµ‹: {self.run_name}")

        start_time = datetime.now()

        try:
            # æ‰§è¡Œåœºæ™¯
            self._execute_scenarios()

            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds()

            # ã€ä¿®æ”¹ã€‘ä½¿ç”¨æ··åˆæ•°æ®æºç”Ÿæˆæ‘˜è¦
            run_summary = self._generate_run_summary(
                start_time, end_time, total_duration,
                status="completed"
            )

            # ä¿å­˜è¿è¡Œæ‘˜è¦
            self._save_run_summary(run_summary)

            logger.info(f"âœ… è¯„æµ‹å®Œæˆ: {self.run_name}")
            return run_summary

        except Exception as e:
            logger.error(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
            raise
    

    
    def _execute_scenarios(self):
        """æ‰§è¡Œåœºæ™¯ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸è¿”å›ç»“æœï¼‰"""
        scenario_count = len(self.scenario_list)

        if scenario_count == 1:
            logger.info(f"ğŸ”„ æ‰§è¡Œåœºæ™¯: {self.scenario_list[0]}")
        else:
            logger.info(f"ğŸ”„ æ‰§è¡Œ {scenario_count} ä¸ªåœºæ™¯")

        self._executor = ProcessPoolExecutor(max_workers=self.parallel_count)
        try:
            # æäº¤æ‰€æœ‰åœºæ™¯ä»»åŠ¡
            future_to_scenario = {
                self._executor.submit(
                    execute_scenario_standalone,
                    scenario_id, self.config, self.output_dir,
                    self.actual_agent_type, self.task_type,
                    self.task_indices.get(scenario_id, [])
                ): scenario_id
                for scenario_id in self.scenario_list
            }

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆä¸æ”¶é›†ç»“æœï¼‰
            for future in as_completed(future_to_scenario):
                scenario_id = future_to_scenario[future]
                try:
                    future.result()  # åªæ˜¯ç­‰å¾…å®Œæˆï¼Œä¸ä¿å­˜ç»“æœ
                    logger.info(f"âœ… åœºæ™¯ {scenario_id} æ‰§è¡Œå®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ åœºæ™¯ {scenario_id} æ‰§è¡Œå¤±è´¥: {e}")

        finally:
            if hasattr(self, '_executor') and self._executor:
                self._executor.shutdown(wait=True)
                self._executor = None
    
    def _calculate_overall_summary_from_csv(self) -> Dict[str, Any]:
        """
        ä»CSVæ–‡ä»¶è®¡ç®—overall_summary

        Returns:
            Dict: overall_summaryæ•°æ®
        """
        csv_file = os.path.join(self.output_dir, "subtask_execution_log.csv")

        # é»˜è®¤å€¼
        summary = {
            "total_tasks": 0,
            "actually_completed": 0,
            "model_claimed_completed": 0,
            "total_llm_interactions": 0,
            "completion_rate": 0.0,
            "avg_llm_interactions": 0.0
        }

        if not os.path.exists(csv_file):
            logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
            return summary

        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)

                total_tasks = 0
                actually_completed = 0
                model_claimed_completed = 0
                total_llm_interactions = 0

                for row in reader:
                    total_tasks += 1

                    # ç»Ÿè®¡å®é™…å®Œæˆ
                    if row.get('subtask_completed', '').lower() == 'true':
                        actually_completed += 1

                    # ç»Ÿè®¡æ¨¡å‹å£°ç§°å®Œæˆ
                    if row.get('model_claimed_done', '').lower() == 'true':
                        model_claimed_completed += 1

                    # ç´¯è®¡LLMäº¤äº’æ¬¡æ•°
                    try:
                        llm_interactions = int(row.get('llm_interactions', 0) or 0)
                        total_llm_interactions += llm_interactions
                    except (ValueError, TypeError):
                        pass

                # è®¡ç®—æ¯”ç‡
                completion_rate = actually_completed / total_tasks if total_tasks > 0 else 0.0
                avg_llm_interactions = total_llm_interactions / total_tasks if total_tasks > 0 else 0.0

                summary.update({
                    "total_tasks": total_tasks,
                    "actually_completed": actually_completed,
                    "model_claimed_completed": model_claimed_completed,
                    "total_llm_interactions": total_llm_interactions,
                    "completion_rate": round(completion_rate, 4),
                    "avg_llm_interactions": round(avg_llm_interactions, 2)
                })

                logger.info(f"ğŸ“Š ä»CSVè®¡ç®—ç»Ÿè®¡: {total_tasks}ä¸ªä»»åŠ¡, {actually_completed}ä¸ªå®Œæˆ")

        except Exception as e:
            logger.error(f"è§£æCSVæ–‡ä»¶å¤±è´¥: {e}")

        return summary

    def _calculate_task_category_statistics_from_csv(self) -> Dict[str, Any]:
        """
        ä»CSVæ–‡ä»¶è®¡ç®—ä»»åŠ¡åˆ†ç±»ç»Ÿè®¡

        Returns:
            Dict: æŒ‰ä»»åŠ¡ç±»åˆ«åˆ†ç»„çš„ç»Ÿè®¡æ•°æ®
        """
        csv_file = os.path.join(self.output_dir, "subtask_execution_log.csv")

        if not os.path.exists(csv_file):
            return {}

        category_stats = {}

        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)

                for row in reader:
                    category = row.get('task_category', 'unknown')

                    if category not in category_stats:
                        category_stats[category] = {
                            "total": 0,
                            "completed": 0,
                            "model_claimed": 0,
                            "completion_rate": 0.0
                        }

                    category_stats[category]["total"] += 1

                    if row.get('subtask_completed', '').lower() == 'true':
                        category_stats[category]["completed"] += 1

                    if row.get('model_claimed_done', '').lower() == 'true':
                        category_stats[category]["model_claimed"] += 1

                # è®¡ç®—å®Œæˆç‡
                for category, stats in category_stats.items():
                    if stats["total"] > 0:
                        stats["completion_rate"] = round(stats["completed"] / stats["total"], 4)

        except Exception as e:
            logger.error(f"è®¡ç®—ä»»åŠ¡åˆ†ç±»ç»Ÿè®¡å¤±è´¥: {e}")

        return category_stats
    
    def _generate_run_summary(self, start_time: datetime, end_time: datetime,
                             total_duration: float, status: str = "completed",
                             note: str = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¿è¡Œæ‘˜è¦ï¼ˆæ··åˆæ•°æ®æºç‰ˆæœ¬ï¼‰

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            total_duration: æ€»æŒç»­æ—¶é—´
            status: è¿è¡ŒçŠ¶æ€
            note: å¤‡æ³¨ä¿¡æ¯

        Returns:
            Dict: è¿è¡Œæ‘˜è¦
        """

        # 1. è¿è¡Œæ—¶ä¿¡æ¯ï¼ˆä¸ä¾èµ–CSVï¼‰
        runinfo = {
            "run_id": self.run_id,
            "model_name": self.model_name,
            "agent_type": self.agent_type,
            "task_mode": self.task_type,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "total_scenarios": len(self.scenario_list),
            "config_file": self.config_file,
            "status": status,
            "duration_seconds": round(total_duration, 2)
        }

        if note:
            runinfo["note"] = note

        # 2. ä»CSVè®¡ç®—ç»Ÿè®¡æ•°æ®
        overall_summary = self._calculate_overall_summary_from_csv()
        task_category_statistics = self._calculate_task_category_statistics_from_csv()

        return {
            "runinfo": runinfo,
            "overall_summary": overall_summary,
            "task_category_statistics": task_category_statistics
        }
    
    def _save_run_summary(self, run_summary: Dict[str, Any]):
        """ä¿å­˜è¿è¡Œæ‘˜è¦"""
        summary_file = os.path.join(self.output_dir, 'run_summary.json')
        
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(run_summary, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“Š è¿è¡Œæ‘˜è¦å·²ä¿å­˜: {summary_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¿è¡Œæ‘˜è¦å¤±è´¥: {e}")
    
    def _register_signal_handlers(self):
        """æ³¨å†Œä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            logger.info("ğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")

            # å…³é—­è¿›ç¨‹æ± 
            if hasattr(self, '_executor') and self._executor:
                logger.info("ğŸ”„ æ­£åœ¨å…³é—­è¿›ç¨‹æ± ...")
                self._executor.shutdown(wait=False)
                self._executor = None
                logger.info("âœ… è¿›ç¨‹æ± å·²å…³é—­")

            # ä¿å­˜å½“å‰æ•°æ®
            self._save_emergency_summary()
            logger.info("âœ… ç´§æ€¥æ•°æ®ä¿å­˜å®Œæˆ")

            logger.info("ğŸšª ç¨‹åºé€€å‡º")
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

    def _save_emergency_summary(self):
        """å¼‚å¸¸æƒ…å†µä¸‹çš„ç´§æ€¥æ‘˜è¦ä¿å­˜"""
        try:
            end_time = datetime.now()
            start_time_dt = datetime.fromisoformat(self.start_time)
            total_duration = (end_time - start_time_dt).total_seconds()

            # æ£€æŸ¥æ˜¯å¦æœ‰CSVæ•°æ®
            csv_file = os.path.join(self.output_dir, "subtask_execution_log.csv")
            has_csv_data = os.path.exists(csv_file) and os.path.getsize(csv_file) > 100  # å¤§äºå¤´éƒ¨

            if has_csv_data:
                # æœ‰CSVæ•°æ®ï¼Œç”Ÿæˆå®Œæ•´æ‘˜è¦
                emergency_summary = self._generate_run_summary(
                    start_time_dt, end_time, total_duration,
                    status="emergency_exit",
                    note="Program terminated unexpectedly"
                )
            else:
                # æ²¡æœ‰CSVæ•°æ®ï¼Œç”ŸæˆåŸºæœ¬æ‘˜è¦
                emergency_summary = self._generate_run_summary(
                    start_time_dt, end_time, total_duration,
                    status="emergency_exit_no_data",
                    note="Program terminated before any task completion"
                )

            # ä¿å­˜æ‘˜è¦
            summary_path = os.path.join(self.output_dir, 'run_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(emergency_summary, f, indent=2, ensure_ascii=False)

            logger.info(f"ğŸ“Š ç´§æ€¥è¿è¡Œæ‘˜è¦å·²ä¿å­˜: {summary_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜ç´§æ€¥æ‘˜è¦å¤±è´¥: {e}")


def execute_scenario_standalone(scenario_id: str, config: Dict[str, Any],
                               output_dir: str, agent_type: str, task_type: str,
                               task_indices: List[int] = None) -> Dict[str, Any]:
    """
    ç‹¬ç«‹çš„åœºæ™¯æ‰§è¡Œå‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†
    é¿å…pickleåºåˆ—åŒ–é—®é¢˜

    Args:
        scenario_id: åœºæ™¯ID
        config: é…ç½®ä¿¡æ¯
        output_dir: è¾“å‡ºç›®å½•
        agent_type: æ™ºèƒ½ä½“ç±»å‹
        task_type: ä»»åŠ¡ç±»å‹
        task_indices: è¦æ‰§è¡Œçš„ä»»åŠ¡ç´¢å¼•åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    """
    try:
        scenario_executor = ScenarioExecutor(scenario_id, config, output_dir, task_indices)
        return scenario_executor.execute_scenario(agent_type, task_type)
    except Exception as e:
        return {
            'scenario_id': scenario_id,
            'status': 'failed',
            'error': str(e),
            'task_results': []
        }
