"""
è¯„æµ‹ç®¡ç†å™¨ - ç»Ÿä¸€è¯„æµ‹ç®¡ç†å’Œåœºæ™¯çº§å¹¶è¡Œæ‰§è¡Œ
"""

import os
import json
import yaml
import logging
import signal
import sys
from typing import Dict, List, Any, Optional
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

from config.config_manager import ConfigManager
from .scenario_selector import ScenarioSelector
from .scenario_executor import ScenarioExecutor

logger = logging.getLogger(__name__)


class EvaluationManager:
    """è¯„æµ‹ç®¡ç†å™¨ - ç»Ÿä¸€è¯„æµ‹ç®¡ç†å’Œåœºæ™¯çº§å¹¶è¡Œæ‰§è¡Œ"""
    
    def __init__(self, config_file: str, agent_type: str, task_type: str, 
                 scenario_selection: Dict[str, Any], custom_suffix: str = None):
        """
        åˆå§‹åŒ–è¯„æµ‹ç®¡ç†å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶å
            agent_type: æ™ºèƒ½ä½“ç±»å‹ ('single', 'centralized', 'decentralized')
            task_type: ä»»åŠ¡ç±»å‹ ('sequential', 'combined', 'independent')
            scenario_selection: åœºæ™¯é€‰æ‹©é…ç½®
            custom_suffix: è‡ªå®šä¹‰åç¼€
        """
        self.config_file = config_file
        self.agent_type = agent_type
        self.task_type = task_type
        self.custom_suffix = custom_suffix or 'demo'
        
        # åŠ è½½é…ç½®
        self.config_manager = ConfigManager()
        self.config = self.config_manager.get_config(config_file)
        
        # é€‰æ‹©åœºæ™¯
        self.scenario_list = ScenarioSelector.get_scenario_list(self.config, scenario_selection)
        
        # ç”Ÿæˆè¿è¡Œåç§°å’Œè¾“å‡ºç›®å½•
        self.run_name = self._generate_run_name()
        self.output_dir = self._create_output_directory()
        
        # å¹¶è¡Œé…ç½®
        parallel_config = self.config.get('parallel_evaluation', {})
        max_parallel = parallel_config.get('scenario_parallelism', {}).get('max_parallel_scenarios', 2)
        self.parallel_count = min(len(self.scenario_list), max_parallel)
        
        # ä»»åŠ¡ç»Ÿè®¡
        self.task_stats = {}

        # åœºæ™¯ç»“æœå­˜å‚¨ï¼ˆç”¨äºç´§æ€¥ä¿å­˜ï¼‰
        self._scenario_results = []

        # è¿è¡Œå¼€å§‹æ—¶é—´
        self.start_time = datetime.now().isoformat()

        # è¿è¡ŒID
        self.run_id = self.run_name

        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        self._register_signal_handlers()

        # ä¿å­˜å®éªŒé…ç½®
        self._save_experiment_config()

        logger.info(f"ğŸš€ è¯„æµ‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.run_name}")
        logger.info(f"ğŸ“Š åœºæ™¯æ•°é‡: {len(self.scenario_list)}, å¹¶è¡Œæ•°: {self.parallel_count}")
    
    def _generate_run_name(self) -> str:
        """ç”Ÿæˆè¿è¡Œåç§°"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        scenario_range = self._format_scenario_range()
        return f"{timestamp}_{self.agent_type}_{self.task_type}_{scenario_range}_{self.custom_suffix}"
    
    def _format_scenario_range(self) -> str:
        """æ ¼å¼åŒ–åœºæ™¯èŒƒå›´å­—ç¬¦ä¸²"""
        if len(self.scenario_list) == 1:
            return self.scenario_list[0]
        elif len(self.scenario_list) <= 3:
            return "_".join(self.scenario_list)
        else:
            return f"{self.scenario_list[0]}_to_{self.scenario_list[-1]}"
    
    def _create_output_directory(self) -> str:
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        base_output_dir = self.config.get('evaluation', {}).get('output', {}).get('output_directory', 'output')
        output_dir = os.path.join(base_output_dir, self.run_name)
        
        # åˆ›å»ºå¿…è¦çš„å­ç›®å½•
        subdirs = ['trajectories', 'llm_qa', 'logs']
        for subdir in subdirs:
            os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)
        
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir}")
        return output_dir

    def _save_experiment_config(self):
        """ä¿å­˜æœ¬æ¬¡å®éªŒçš„é…ç½®ä¿¡æ¯"""
        try:
            # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
            agent_config = self.config.get('agent_config', {})
            model_info = self._extract_model_info(agent_config)

            # æ„å»ºå®éªŒé…ç½®
            experiment_config = {
                'experiment_info': {
                    'run_name': self.run_name,
                    'timestamp': datetime.now().isoformat(),
                    'config_file': self.config_file,
                    'agent_type': self.agent_type,
                    'task_type': self.task_type,
                    'custom_suffix': self.custom_suffix
                },
                'model_config': model_info,
                'scenarios': {
                    'scenario_list': self.scenario_list,
                    'scenario_count': len(self.scenario_list),
                    'selection_mode': self.config.get('evaluation', {}).get('scenario_selection', {}).get('mode', 'unknown')
                },
                'execution_config': {
                    'parallel_count': self.parallel_count,
                    'max_total_steps': self.config.get('execution', {}).get('max_total_steps', 200),
                    'max_steps_per_task': self.config.get('execution', {}).get('max_steps_per_task', 50)
                },
                'evaluation_settings': self.config.get('evaluation', {}),
                'parallel_settings': self.config.get('parallel_evaluation', {})
            }

            # ä¿å­˜ä¸ºYAMLæ–‡ä»¶
            config_file = os.path.join(self.output_dir, 'experiment_config.yaml')
            with open(config_file, 'w', encoding='utf-8') as f:
                yaml.dump(experiment_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            logger.info(f"ğŸ“‹ å®éªŒé…ç½®å·²ä¿å­˜: experiment_config.yaml")

        except Exception as e:
            logger.error(f"ä¿å­˜å®éªŒé…ç½®å¤±è´¥: {e}")

    def _extract_model_info(self, agent_config: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æ™ºèƒ½ä½“é…ç½®ä¸­æå–æ¨¡å‹ä¿¡æ¯"""
        model_info = {
            'agent_class': agent_config.get('agent_class', 'unknown'),
            'max_failures': agent_config.get('max_failures', 3),
            'max_history': agent_config.get('max_history', 50)
        }

        # å°è¯•ä»LLMé…ç½®ä¸­æå–æ¨¡å‹ä¿¡æ¯
        try:
            llm_config_manager = ConfigManager()
            llm_config = llm_config_manager.get_config('llm_config')

            if llm_config:
                model_info['llm_mode'] = llm_config.get('mode', 'unknown')

                # è·å–APIé…ç½®
                api_config = llm_config.get('api', {})
                provider = api_config.get('provider', 'unknown')
                model_info['provider'] = provider

                # æ ¹æ®providerè·å–å…·ä½“æ¨¡å‹ä¿¡æ¯
                if provider in api_config:
                    provider_config = api_config[provider]
                    model_info['model_name'] = provider_config.get('model', 'unknown')
                    model_info['temperature'] = provider_config.get('temperature', 0.7)
                    model_info['max_tokens'] = provider_config.get('max_tokens', 1024)

                    # ä¸ä¿å­˜APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯
                    if 'endpoint' in provider_config:
                        model_info['api_endpoint'] = provider_config['endpoint']

        except Exception as e:
            logger.warning(f"æ— æ³•æå–LLMé…ç½®ä¿¡æ¯: {e}")

        return model_info

    def run_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œè¯„æµ‹"""
        logger.info(f"ğŸ¯ å¼€å§‹è¯„æµ‹: {self.run_name}")
        
        start_time = datetime.now()
        
        try:
            if len(self.scenario_list) == 1:
                # å•åœºæ™¯ç›´æ¥æ‰§è¡Œ
                scenario_results = self._execute_single_scenario()
            else:
                # å¤šåœºæ™¯å¹¶è¡Œæ‰§è¡Œ
                scenario_results = self._execute_parallel_scenarios()
            
            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds()
            
            # ç”Ÿæˆè¿è¡Œæ‘˜è¦
            run_summary = self._generate_run_summary(
                scenario_results, start_time, end_time, total_duration
            )
            
            # ä¿å­˜è¿è¡Œæ‘˜è¦
            self._save_run_summary(run_summary)
            
            logger.info(f"âœ… è¯„æµ‹å®Œæˆ: {self.run_name}")
            return run_summary
            
        except Exception as e:
            logger.error(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
            raise
    
    def _execute_single_scenario(self) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªåœºæ™¯"""
        scenario_id = self.scenario_list[0]
        logger.info(f"ğŸ”„ æ‰§è¡Œåœºæ™¯: {scenario_id}")
        
        try:
            scenario_executor = ScenarioExecutor(scenario_id, self.config, self.output_dir)
            result = scenario_executor.execute_scenario(self.agent_type, self.task_type)
            
            # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
            self._update_task_statistics(result)
            
            return {scenario_id: result}
            
        except Exception as e:
            logger.error(f"âŒ åœºæ™¯ {scenario_id} æ‰§è¡Œå¤±è´¥: {e}")
            return {scenario_id: {'status': 'failed', 'error': str(e)}}
    
    def _execute_parallel_scenarios(self) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåœºæ™¯"""
        logger.info(f"ğŸ”„ å¹¶è¡Œæ‰§è¡Œ {len(self.scenario_list)} ä¸ªåœºæ™¯")
        
        scenario_results = {}
        
        self._executor = ProcessPoolExecutor(max_workers=self.parallel_count)
        try:
            # æäº¤æ‰€æœ‰åœºæ™¯ä»»åŠ¡
            future_to_scenario = {
                self._executor.submit(
                    execute_scenario_standalone,
                    scenario_id, self.config, self.output_dir,
                    self.agent_type, self.task_type
                ): scenario_id
                for scenario_id in self.scenario_list
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_scenario):
                scenario_id = future_to_scenario[future]
                try:
                    result = future.result()
                    scenario_results[scenario_id] = result

                    # æ›´æ–°åœºæ™¯ç»“æœå­˜å‚¨ï¼ˆç”¨äºç´§æ€¥ä¿å­˜ï¼‰
                    self._scenario_results.append({
                        'scenario_id': scenario_id,
                        'result': result,
                        'completed_at': datetime.now().isoformat()
                    })

                    # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
                    self._update_task_statistics(result)

                    logger.info(f"âœ… åœºæ™¯ {scenario_id} æ‰§è¡Œå®Œæˆ")

                except Exception as e:
                    logger.error(f"âŒ åœºæ™¯ {scenario_id} æ‰§è¡Œå¤±è´¥: {e}")
                    error_result = {'status': 'failed', 'error': str(e)}
                    scenario_results[scenario_id] = error_result

                    # ä¹Ÿè¦è®°å½•å¤±è´¥çš„åœºæ™¯
                    self._scenario_results.append({
                        'scenario_id': scenario_id,
                        'result': error_result,
                        'completed_at': datetime.now().isoformat()
                    })

        finally:
            # ç¡®ä¿executorè¢«æ­£ç¡®å…³é—­
            if hasattr(self, '_executor') and self._executor:
                self._executor.shutdown(wait=True)
                self._executor = None

        return scenario_results
    
    def _update_task_statistics(self, scenario_result: Dict[str, Any]):
        """æ›´æ–°ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        task_results = scenario_result.get('task_results', [])
        
        for task_result in task_results:
            category = task_result.get('task_category', 'unknown')
            
            if category not in self.task_stats:
                self.task_stats[category] = {
                    'total_tasks': 0,
                    'completed_tasks': 0,
                    'model_claimed_tasks': 0
                }
            
            self.task_stats[category]['total_tasks'] += 1
            
            if task_result.get('subtask_completed', False):
                self.task_stats[category]['completed_tasks'] += 1
            
            if task_result.get('model_claimed_done', False):
                self.task_stats[category]['model_claimed_tasks'] += 1
    
    def _generate_run_summary(self, scenario_results: Dict[str, Any],
                             start_time: datetime, end_time: datetime,
                             total_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆè¿è¡Œæ‘˜è¦"""
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_scenarios = len(scenario_results)
        successful_scenarios = sum(1 for result in scenario_results.values() 
                                 if result.get('status') != 'failed')
        
        total_tasks = sum(result.get('total_tasks', 0) for result in scenario_results.values())
        total_completed_tasks = sum(result.get('completed_tasks', 0) for result in scenario_results.values())
        total_model_claimed_tasks = sum(
            len([task for task in result.get('task_results', []) 
                if task.get('model_claimed_done', False)])
            for result in scenario_results.values()
        )
        
        # è®¡ç®—å®Œæˆç‡å’Œå‡†ç¡®ç‡
        overall_completion_rate = total_completed_tasks / total_tasks if total_tasks > 0 else 0.0
        overall_completion_accuracy = (
            total_completed_tasks / total_model_claimed_tasks 
            if total_model_claimed_tasks > 0 else 0.0
        )
        
        # è®¡ç®—ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        task_category_statistics = {}
        for category, stats in self.task_stats.items():
            completion_rate = stats['completed_tasks'] / stats['total_tasks'] if stats['total_tasks'] > 0 else 0.0
            task_category_statistics[category] = {
                'total_tasks': stats['total_tasks'],
                'completed_tasks': stats['completed_tasks'],
                'model_claimed_tasks': stats['model_claimed_tasks'],
                'completion_rate': completion_rate
            }
        
        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        parallel_efficiency = self.parallel_count if total_scenarios > 1 else 1.0
        
        return {
            'run_info': {
                'run_name': self.run_name,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_duration': total_duration,
                'evaluation_mode': self.task_type,
                'agent_type': self.agent_type,
                'parallel_count': self.parallel_count,
                'total_scenarios': total_scenarios,
                'scenario_range': self._format_scenario_range()
            },
            'task_category_statistics': task_category_statistics,
            'overall_summary': {
                'total_scenarios': total_scenarios,
                'successful_scenarios': successful_scenarios,
                'total_tasks': total_tasks,
                'total_completed_tasks': total_completed_tasks,
                'overall_completion_rate': overall_completion_rate,
                'total_model_claimed_tasks': total_model_claimed_tasks,
                'overall_completion_accuracy': overall_completion_accuracy,
                'average_duration_per_scenario': total_duration / total_scenarios if total_scenarios > 0 else 0.0,
                'parallel_efficiency': parallel_efficiency
            },
            'scenario_results': scenario_results
        }
    
    def _save_run_summary(self, run_summary: Dict[str, Any]):
        """ä¿å­˜è¿è¡Œæ‘˜è¦"""
        summary_file = os.path.join(self.output_dir, 'run_summary.json')
        
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(run_summary, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“Š è¿è¡Œæ‘˜è¦å·²ä¿å­˜: {summary_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¿è¡Œæ‘˜è¦å¤±è´¥: {e}")
    
    def _register_signal_handlers(self):
        """æ³¨å†Œä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            logger.info("ğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")

            # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹æ± ï¼Œå°è¯•å…³é—­
            if hasattr(self, '_executor') and self._executor:
                logger.info("ğŸ”„ æ­£åœ¨å…³é—­è¿›ç¨‹æ± ...")
                try:
                    self._executor.shutdown(wait=False)
                    logger.info("âœ… è¿›ç¨‹æ± å·²å…³é—­")
                except Exception as e:
                    logger.warning(f"å…³é—­è¿›ç¨‹æ± æ—¶å‡ºé”™: {e}")

            # ä¿å­˜å½“å‰æ•°æ®
            try:
                # ç”Ÿæˆç´§æ€¥è¿è¡Œæ‘˜è¦
                emergency_summary = {
                    'run_info': {
                        'run_id': getattr(self, 'run_id', 'unknown'),
                        'start_time': getattr(self, 'start_time', datetime.now().isoformat()),
                        'end_time': datetime.now().isoformat(),
                        'status': 'interrupted',
                        'agent_type': getattr(self, 'agent_type', 'unknown'),
                        'task_type': getattr(self, 'task_type', 'unknown'),
                        'total_scenarios': len(getattr(self, 'scenario_list', [])),
                        'parallel_count': getattr(self, 'parallel_count', 1)
                    },
                    'scenario_results': getattr(self, '_scenario_results', []),
                    'interruption_info': {
                        'signal': signum,
                        'message': 'Evaluation interrupted by user'
                    }
                }
                self._save_run_summary(emergency_summary)
                logger.info("âœ… ç´§æ€¥æ•°æ®ä¿å­˜å®Œæˆ")
            except Exception as e:
                logger.warning(f"ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")

            logger.info("ğŸšª ç¨‹åºé€€å‡º")
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)


def execute_scenario_standalone(scenario_id: str, config: Dict[str, Any], 
                               output_dir: str, agent_type: str, task_type: str) -> Dict[str, Any]:
    """
    ç‹¬ç«‹çš„åœºæ™¯æ‰§è¡Œå‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†
    é¿å…pickleåºåˆ—åŒ–é—®é¢˜
    """
    try:
        scenario_executor = ScenarioExecutor(scenario_id, config, output_dir)
        return scenario_executor.execute_scenario(agent_type, task_type)
    except Exception as e:
        return {
            'scenario_id': scenario_id,
            'status': 'failed',
            'error': str(e),
            'task_results': []
        }
