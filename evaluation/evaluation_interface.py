"""
è¯„æµ‹æ¥å£ - ä¸ºbaselineæä¾›çš„ç»Ÿä¸€è¯„æµ‹æ¥å£
"""

import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class EvaluationInterface:
    """ä¸ºbaselineæä¾›çš„ç»Ÿä¸€è¯„æµ‹æ¥å£"""
    
    @staticmethod
    def run_evaluation(config_file: str, agent_type: str, task_type: str, 
                      scenario_selection: Dict[str, Any] = None, 
                      custom_suffix: str = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è¯„æµ‹å…¥å£
        
        Args:
            config_file: é…ç½®æ–‡ä»¶å ('single_agent_config', 'centralized_config', 'decentralized_config')
            agent_type: æ™ºèƒ½ä½“ç±»å‹ ('single', 'multi')
            task_type: ä»»åŠ¡ç±»å‹ ('sequential', 'combined', 'independent')
            scenario_selection: åœºæ™¯é€‰æ‹©é…ç½®
                {
                    'mode': 'range',  # 'all', 'range', 'list'
                    'range': {'start': '00001', 'end': '00010'},
                    'list': ['00001', '00003', '00005']
                }
            custom_suffix: è‡ªå®šä¹‰åç¼€
            
        Returns:
            Dict: è¯„æµ‹ç»“æœæ‘˜è¦
        """
        try:
            # éªŒè¯å‚æ•°
            EvaluationInterface._validate_parameters(
                config_file, agent_type, task_type, scenario_selection
            )
            
            # åˆ›å»ºè¯„æµ‹ç®¡ç†å™¨
            from .evaluation_manager import EvaluationManager
            
            manager = EvaluationManager(
                config_file=config_file,
                agent_type=agent_type,
                task_type=task_type,
                scenario_selection=scenario_selection,
                custom_suffix=custom_suffix
            )
            
            # è¿è¡Œè¯„æµ‹
            logger.info(f"ğŸš€ å¼€å§‹è¯„æµ‹: {config_file} - {agent_type} - {task_type}")
            result = manager.run_evaluation()
            
            logger.info(f"âœ… è¯„æµ‹å®Œæˆ: {result['run_info']['run_name']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def _validate_parameters(config_file: str, agent_type: str, task_type: str, 
                           scenario_selection: Optional[Dict[str, Any]]):
        """éªŒè¯å‚æ•°æœ‰æ•ˆæ€§"""
        # éªŒè¯é…ç½®æ–‡ä»¶
        valid_configs = ['single_agent_config', 'centralized_config', 'decentralized_config']
        if config_file not in valid_configs:
            raise ValueError(f"æ— æ•ˆçš„é…ç½®æ–‡ä»¶: {config_file}, æ”¯æŒçš„é…ç½®: {valid_configs}")
        
        # éªŒè¯æ™ºèƒ½ä½“ç±»å‹
        valid_agent_types = ['single', 'multi']
        if agent_type not in valid_agent_types:
            raise ValueError(f"æ— æ•ˆçš„æ™ºèƒ½ä½“ç±»å‹: {agent_type}, æ”¯æŒçš„ç±»å‹: {valid_agent_types}")
        
        # éªŒè¯ä»»åŠ¡ç±»å‹
        valid_task_types = ['sequential', 'combined', 'independent']
        if task_type not in valid_task_types:
            raise ValueError(f"æ— æ•ˆçš„ä»»åŠ¡ç±»å‹: {task_type}, æ”¯æŒçš„ç±»å‹: {valid_task_types}")
        
        # éªŒè¯åœºæ™¯é€‰æ‹©
        if scenario_selection is not None:
            from .scenario_selector import ScenarioSelector
            if not ScenarioSelector.validate_scenario_selection(scenario_selection):
                raise ValueError(f"æ— æ•ˆçš„åœºæ™¯é€‰æ‹©é…ç½®: {scenario_selection}")
    
    @staticmethod
    def parse_scenario_string(scenarios_str: str) -> Dict[str, Any]:
        """
        è§£æåœºæ™¯é€‰æ‹©å­—ç¬¦ä¸²
        
        Args:
            scenarios_str: åœºæ™¯é€‰æ‹©å­—ç¬¦ä¸²
                - 'all': æ‰€æœ‰åœºæ™¯
                - '00001-00010': èŒƒå›´åœºæ™¯
                - '00001,00003,00005': åˆ—è¡¨åœºæ™¯
                - '00001': å•ä¸ªåœºæ™¯
        
        Returns:
            Dict: åœºæ™¯é€‰æ‹©é…ç½®
        """
        from .scenario_selector import ScenarioSelector
        return ScenarioSelector.parse_scenario_selection_string(scenarios_str)
    
    @staticmethod
    def get_scenario_count(scenario_selection: Dict[str, Any] = None) -> int:
        """è·å–åœºæ™¯æ•°é‡"""
        from .scenario_selector import ScenarioSelector
        return ScenarioSelector.get_scenario_count(scenario_selection)
    
    @staticmethod
    def validate_config_file(config_file: str) -> bool:
        """éªŒè¯é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        from config.config_manager import ConfigManager
        config_manager = ConfigManager()
        try:
            config = config_manager.get_config(config_file)
            return config is not None
        except:
            return False
    
    @staticmethod
    def list_available_configs() -> list:
        """åˆ—å‡ºå¯ç”¨çš„é…ç½®æ–‡ä»¶"""
        import os
        import glob

        config_dir = "config/baseline"
        if not os.path.exists(config_dir):
            return []

        config_files = glob.glob(os.path.join(config_dir, "*.yaml"))
        config_names = []

        for config_file in config_files:
            config_name = os.path.basename(config_file)[:-5]  # ç§»é™¤.yamlåç¼€
            config_names.append(config_name)

        return sorted(config_names)
    
    @staticmethod
    def get_evaluation_status(run_name: str) -> Dict[str, Any]:
        """è·å–è¯„æµ‹çŠ¶æ€"""
        import os
        import json
        
        output_dir = os.path.join('output', run_name)
        if not os.path.exists(output_dir):
            return {'status': 'not_found', 'message': f'è¿è¡Œç›®å½•ä¸å­˜åœ¨: {run_name}'}
        
        # æ£€æŸ¥è¿è¡Œæ‘˜è¦æ–‡ä»¶
        summary_file = os.path.join(output_dir, 'run_summary.json')
        if os.path.exists(summary_file):
            try:
                with open(summary_file, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                return {
                    'status': 'completed',
                    'run_info': summary.get('run_info', {}),
                    'overall_summary': summary.get('overall_summary', {})
                }
            except Exception as e:
                return {'status': 'error', 'message': f'è¯»å–æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}'}
        else:
            return {'status': 'running', 'message': 'è¯„æµ‹æ­£åœ¨è¿›è¡Œä¸­'}


# ä¾¿åˆ©å‡½æ•°
def run_single_agent_evaluation(task_type: str = 'sequential', 
                               scenarios: str = 'all',
                               suffix: str = 'baseline') -> Dict[str, Any]:
    """è¿è¡Œå•æ™ºèƒ½ä½“è¯„æµ‹çš„ä¾¿åˆ©å‡½æ•°"""
    scenario_selection = EvaluationInterface.parse_scenario_string(scenarios)
    
    return EvaluationInterface.run_evaluation(
        config_file='single_agent_config',
        agent_type='single',
        task_type=task_type,
        scenario_selection=scenario_selection,
        custom_suffix=suffix
    )


def run_multi_agent_evaluation(config_type: str = 'centralized',
                              task_type: str = 'sequential',
                              scenarios: str = 'all',
                              suffix: str = 'baseline') -> Dict[str, Any]:
    """è¿è¡Œå¤šæ™ºèƒ½ä½“è¯„æµ‹çš„ä¾¿åˆ©å‡½æ•°"""
    config_file = f"{config_type}_config"
    scenario_selection = EvaluationInterface.parse_scenario_string(scenarios)
    
    return EvaluationInterface.run_evaluation(
        config_file=config_file,
        agent_type='multi',
        task_type=task_type,
        scenario_selection=scenario_selection,
        custom_suffix=suffix
    )


def run_comparison_evaluation(scenarios: str = '00001-00010') -> Dict[str, Any]:
    """è¿è¡Œå¯¹æ¯”è¯„æµ‹çš„ä¾¿åˆ©å‡½æ•°"""
    scenario_selection = EvaluationInterface.parse_scenario_string(scenarios)
    
    results = {}
    
    # å®šä¹‰è¯„æµ‹é…ç½®
    evaluation_configs = [
        {
            'name': 'single_sequential',
            'config_file': 'single_agent_config',
            'agent_type': 'single',
            'task_type': 'sequential'
        },
        {
            'name': 'single_combined',
            'config_file': 'single_agent_config',
            'agent_type': 'single',
            'task_type': 'combined'
        },
        {
            'name': 'single_independent',
            'config_file': 'single_agent_config',
            'agent_type': 'single',
            'task_type': 'independent'
        }
    ]
    
    # è¿è¡Œæ‰€æœ‰è¯„æµ‹
    for config in evaluation_configs:
        logger.info(f"ğŸš€ è¿è¡Œ {config['name']} è¯„æµ‹...")
        
        try:
            result = EvaluationInterface.run_evaluation(
                config_file=config['config_file'],
                agent_type=config['agent_type'],
                task_type=config['task_type'],
                scenario_selection=scenario_selection,
                custom_suffix=f"comparison_{config['name']}"
            )
            
            results[config['name']] = result
            logger.info(f"âœ… {config['name']} è¯„æµ‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ {config['name']} è¯„æµ‹å¤±è´¥: {e}")
            results[config['name']] = {'status': 'failed', 'error': str(e)}
    
    return results
