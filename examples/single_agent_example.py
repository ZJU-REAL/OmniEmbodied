#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•æ™ºèƒ½ä½“ç¤ºä¾‹ - ä½¿ç”¨ä»»åŠ¡éªŒè¯å™¨æ‰§è¡Œä»»åŠ¡

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä»»åŠ¡éªŒè¯å™¨æ¥æ‰§è¡Œå•æ™ºèƒ½ä½“ä»»åŠ¡ã€‚
ä»»åŠ¡éªŒè¯å™¨æä¾›äº†å®Œæ•´çš„æ—¥å¿—è®°å½•ã€è½¨è¿¹è®°å½•å’Œè¯„æµ‹åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. ä»é…ç½®æ–‡ä»¶åŠ è½½å®Œæ•´é…ç½®
2. æ”¯æŒä¸åŒçš„è¯„æµ‹æ¨¡å¼é…ç½®
3. è‡ªåŠ¨è®°å½•æ‰§è¡Œè½¨è¿¹å’Œæ—¥å¿—
4. ç”Ÿæˆè¯¦ç»†çš„è¯„æµ‹æŠ¥å‘Š
5. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®

ä½¿ç”¨æ–¹æ³•ï¼š
python examples/single_agent_example.py --mode sequential --scenario 00001 --suffix demo
python examples/single_agent_example.py --mode combined --scenario 00001 --suffix test
python examples/single_agent_example.py --config my_config.yaml
"""

import os
import sys
import time
import json
import logging
import argparse
import multiprocessing
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.logger import setup_logger
from utils.task_evaluator import TaskEvaluator
from utils.run_naming import RunNamingManager
from config import ConfigManager


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å•æ™ºèƒ½ä½“ä»»åŠ¡æ‰§è¡Œç¤ºä¾‹')
    parser.add_argument('--mode', type=str,
                        choices=['sequential', 'combined', 'independent'],
                        help='è¯„æµ‹æ¨¡å¼: sequential (é€ä¸ªè¯„æµ‹), combined (æ··åˆè¯„æµ‹), independent (ç‹¬ç«‹è¯„æµ‹)')
    parser.add_argument('--scenario', type=str,
                        help='åœºæ™¯ID')
    parser.add_argument('--suffix', type=str,
                        help='è¿è¡Œåç¼€')
    parser.add_argument('--config', type=str, default='single_agent_config',
                        help='é…ç½®æ–‡ä»¶å (é»˜è®¤: single_agent_config)')
    parser.add_argument('--log-level', type=str,
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--parallel', action='store_true',
                        help='å¯ç”¨å¹¶è¡Œè¯„æµ‹æ¨¡å¼')
    parser.add_argument('--max-steps', type=int,
                        help='æœ€å¤§æ‰§è¡Œæ­¥æ•°')
    parser.add_argument('--max-steps-per-task', type=int,
                        help='æ¯ä¸ªå­ä»»åŠ¡çš„æœ€å¤§æ­¥æ•°')
    return parser.parse_args()


def execute_single_scenario(args_tuple):
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­æ‰§è¡Œå•ä¸ªåœºæ™¯çš„è¯„æµ‹

    Args:
        args_tuple: (config_file, mode, scenario_id, suffix, output_dir, run_name)

    Returns:
        Dict[str, Any]: åœºæ™¯è¯„æµ‹ç»“æœ
    """
    config_file, mode, scenario_id, suffix, output_dir, main_run_name = args_tuple

    import time
    import os
    import sys
    import logging

    scenario_start_time = time.time()

    # ç›´æ¥ä½¿ç”¨ä¸»è¾“å‡ºç›®å½•ï¼Œä¸åˆ›å»ºåœºæ™¯å­ç›®å½•
    scenario_output_dir = output_dir

    try:
        # ç¡®ä¿è·¯å¾„æ­£ç¡®
        project_root = os.path.dirname(os.path.dirname(__file__))
        if project_root not in sys.path:
            sys.path.insert(0, project_root)

        # å¯¼å…¥TaskEvaluatorå’Œç›¸å…³æ¨¡å—
        from utils.task_evaluator import TaskEvaluator
        from utils.logger import setup_logger

        # è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨ï¼ˆé¿å…å†²çªï¼‰
        setup_logger(log_level=logging.INFO)
        scenario_logger = logging.getLogger(f"scenario_{scenario_id}")

        scenario_logger.info(f"ğŸš€ å¯åŠ¨åœºæ™¯ {scenario_id} è¯„æµ‹ï¼ˆè¿›ç¨‹å†…æ‰§è¡Œï¼‰")

        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©TaskEvaluatorä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•
        os.environ['SCENARIO_OUTPUT_DIR'] = scenario_output_dir
        os.environ['DISABLE_AUTO_OUTPUT_DIR'] = 'true'  # ç¦ç”¨è‡ªåŠ¨è¾“å‡ºç›®å½•åˆ›å»º

        # åˆ›å»ºTaskEvaluatorå®ä¾‹ï¼Œä¸ä½¿ç”¨custom_suffixé¿å…è‡ªåŠ¨ç”Ÿæˆç›®å½•
        evaluator = TaskEvaluator(
            config_file=config_file,
            agent_type='single',
            task_type=mode,
            scenario_id=scenario_id,
            custom_suffix=None  # ä¸ä½¿ç”¨åç¼€ï¼Œé¿å…è‡ªåŠ¨ç”Ÿæˆç‹¬ç«‹ç›®å½•
        )

        # å¼ºåˆ¶è¦†ç›–è¾“å‡ºç›®å½•å’Œè¿è¡Œåç§°ï¼Œä½¿æ‰€æœ‰åœºæ™¯è¾“å‡ºåˆ°åŒä¸€ä¸ªä¸»ç›®å½•
        evaluator.output_dir = scenario_output_dir
        scenario_run_name = f"scenario_{scenario_id}"
        evaluator.run_name = scenario_run_name

        # é‡æ–°åˆå§‹åŒ–è½¨è¿¹è®°å½•å™¨ï¼Œä½¿ç”¨åœºæ™¯å‰ç¼€çš„æ–‡ä»¶åå’Œæ­£ç¡®çš„scenario_id
        from utils.trajectory_recorder import TrajectoryRecorder
        evaluator.trajectory_recorder = TrajectoryRecorder(scenario_output_dir, scenario_run_name, scenario_id)

        # æ¸…ç†ç¯å¢ƒå˜é‡
        if 'SCENARIO_OUTPUT_DIR' in os.environ:
            del os.environ['SCENARIO_OUTPUT_DIR']
        if 'DISABLE_AUTO_OUTPUT_DIR' in os.environ:
            del os.environ['DISABLE_AUTO_OUTPUT_DIR']

        # è¿è¡Œè¯„æµ‹
        scenario_logger.info(f"ğŸ“‹ å¼€å§‹æ‰§è¡Œåœºæ™¯ {scenario_id} çš„ä»»åŠ¡è¯„æµ‹")
        results = evaluator.run_evaluation(scenario_id)

        # å¤„ç†è¯„æµ‹ç»“æœ
        scenario_logger.info(f"âœ… åœºæ™¯ {scenario_id} è¯„æµ‹æ‰§è¡Œå®Œæˆ")

        # æ„å»ºåœºæ™¯ç»“æœ
        summary = results.get('summary', {})
        output_files = results.get('output_files', {})
        # åˆ¤æ–­æˆåŠŸæ ‡å‡†ï¼šå¯¹äºindependentæ¨¡å¼ï¼Œæ£€æŸ¥å­ä»»åŠ¡å®Œæˆæƒ…å†µ
        success = False
        if mode == 'independent':
            # å¯¹äºindependentæ¨¡å¼ï¼Œåªè¦æœ‰å®Œæˆçš„ä»»åŠ¡å°±ç®—æˆåŠŸ
            success = summary.get('completed_tasks', 0) > 0
        else:
            success = summary.get('completed_tasks', 0) > 0

        scenario_result = {
            'scenario_id': scenario_id,
            'success': success,
            'total_duration': time.time() - scenario_start_time,
            'summary': summary,
            'output_dir': scenario_output_dir,
            'trajectory_file': output_files.get('trajectory_file'),
            'log_file': output_files.get('log_file')
        }

        # å¯¹äºindependentæ¨¡å¼ï¼Œæ·»åŠ å­ä»»åŠ¡ç»“æœ
        if mode == 'independent':
            subtask_results = []
            task_results = results.get('task_results', [])
            for task_result in task_results:
                if 'subtask_results' in task_result:
                    subtask_results.extend(task_result['subtask_results'])
            scenario_result['subtask_results'] = subtask_results

        scenario_logger.info(f"ğŸ¯ åœºæ™¯ {scenario_id} å®Œæˆç‡: {summary.get('completion_rate', 0):.1%}")
        return scenario_result

    except Exception as e:
        return {
            'scenario_id': scenario_id,
            'success': False,
            'error': str(e),
            'total_duration': time.time() - scenario_start_time,
            'summary': {},
            'output_dir': scenario_output_dir
        }


def load_config_with_overrides(config_file: str, args) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶å¹¶åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–

    Args:
        config_file: é…ç½®æ–‡ä»¶å
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        dict: åˆå¹¶åçš„é…ç½®
    """
    # åŠ è½½åŸºç¡€é…ç½®
    config_manager = ConfigManager()
    config = config_manager.get_config(config_file)

    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.mode:
        config.setdefault('evaluation', {})['task_type'] = args.mode

    if args.scenario:
        config.setdefault('evaluation', {})['default_scenario'] = args.scenario

    if args.suffix:
        config.setdefault('evaluation', {}).setdefault('run_settings', {})['default_suffix'] = args.suffix

    if args.log_level:
        config.setdefault('evaluation', {}).setdefault('run_settings', {})['log_level'] = args.log_level

    if args.max_steps:
        config.setdefault('execution', {})['max_total_steps'] = args.max_steps

    if args.max_steps_per_task:
        config.setdefault('task_evaluator', {})['max_steps_per_task'] = args.max_steps_per_task

    return config


def run_parallel_evaluation(config, config_file, mode, suffix):
    """
    è¿è¡Œå¹¶è¡Œè¯„æµ‹

    Args:
        config: é…ç½®å­—å…¸
        config_file: é…ç½®æ–‡ä»¶å
        mode: è¯„æµ‹æ¨¡å¼
        suffix: è¿è¡Œåç¼€

    Returns:
        int: é€€å‡ºç 
    """
    logger = logging.getLogger(__name__)

    # è·å–å¹¶è¡Œè¯„æµ‹é…ç½®
    parallel_config = config.get('parallel_evaluation', {})
    if not parallel_config.get('enabled', False):
        logger.error("âŒ å¹¶è¡Œè¯„æµ‹æœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® parallel_evaluation.enabled = true")
        return 1

    # åœºæ™¯çº§å¹¶è¡Œé…ç½®
    scenario_config = parallel_config.get('scenario_parallelism', {})
    max_parallel_scenarios = scenario_config.get('max_parallel_scenarios', 4)

    # åœºæ™¯é€‰æ‹©é…ç½®
    scenario_selection = parallel_config.get('scenario_selection', {})

    # ç”Ÿæˆè¿è¡Œåç§°å’Œè¾“å‡ºç›®å½•
    run_name = RunNamingManager.generate_run_name(
        agent_type='single',
        task_type=f"parallel_{mode}",
        scenario_id="multi",
        config_name=config_file,
        custom_suffix=suffix
    )

    # åˆ›å»ºä¸»è¾“å‡ºç›®å½•
    base_output_dir = config.get('output_dir', 'output')
    output_dir = RunNamingManager.generate_output_directory(base_output_dir, run_name)
    os.makedirs(output_dir, exist_ok=True)

    logger.info(f"ğŸš€ å¼€å§‹åœºæ™¯çº§å¹¶è¡Œè¯„æµ‹ - æ¨¡å¼: single_{mode}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    start_time = time.time()

    try:
        # è·å–è¦è¯„æµ‹çš„åœºæ™¯åˆ—è¡¨
        scenario_list = get_scenario_list(scenario_selection)
        if not scenario_list:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°è¦è¯„æµ‹çš„åœºæ™¯")
            return 1

        logger.info(f"ğŸ“‹ å‡†å¤‡å¹¶è¡Œè¯„æµ‹ {len(scenario_list)} ä¸ªåœºæ™¯: {scenario_list}")

        # æ‰§è¡Œå¹¶è¡Œè¯„æµ‹
        with ProcessPoolExecutor(max_workers=max_parallel_scenarios) as executor:
            # æäº¤æ‰€æœ‰åœºæ™¯ä»»åŠ¡
            future_to_scenario = {
                executor.submit(execute_single_scenario,
                               (config_file, mode, scenario_id, suffix, output_dir, run_name)): scenario_id
                for scenario_id in scenario_list
            }

            # æ”¶é›†ç»“æœ
            scenario_results = []
            successful_scenarios = 0
            failed_scenarios = 0

            for future in as_completed(future_to_scenario):
                scenario_id = future_to_scenario[future]

                try:
                    scenario_result = future.result()
                    scenario_results.append(scenario_result)

                    if scenario_result.get('success', False):
                        successful_scenarios += 1
                    else:
                        failed_scenarios += 1

                    logger.info(f"âœ… åœºæ™¯ {scenario_id} è¯„æµ‹å®Œæˆ")

                except Exception as e:
                    logger.exception(f"âŒ åœºæ™¯ {scenario_id} è¯„æµ‹å¤±è´¥: {e}")
                    failed_scenarios += 1

                    # è®°å½•å¤±è´¥åœºæ™¯
                    scenario_results.append({
                        'scenario_id': scenario_id,
                        'success': False,
                        'error': str(e),
                        'total_duration': 0,
                        'summary': {}
                    })

        # å¯¹äºindependentæ¨¡å¼ï¼Œè®¡ç®—å­ä»»åŠ¡ç»Ÿè®¡
        total_subtasks = 0
        successful_subtasks = 0
        if mode == 'independent':
            for result in scenario_results:
                summary = result.get('summary', {})
                total_subtasks += summary.get('total_tasks', 0)
                successful_subtasks += summary.get('completed_tasks', 0)

        # ç”Ÿæˆç»“æœ
        total_duration = time.time() - start_time
        results = {
            'run_info': {
                'run_name': run_name,
                'agent_type': 'single',
                'evaluation_type': mode,
                'config_file': config_file,
                'output_dir': output_dir,
                'start_time': datetime.now().isoformat(),
                'total_duration': total_duration,
                'parallel_config': parallel_config
            },
            'scenario_results': scenario_results,
            'overall_summary': {
                'total_scenarios': len(scenario_list),
                'successful_scenarios': successful_scenarios,
                'failed_scenarios': failed_scenarios,
                'scenario_success_rate': successful_scenarios / len(scenario_list) if scenario_list else 0.0
            }
        }

        # ä¸ºindependentæ¨¡å¼æ·»åŠ å­ä»»åŠ¡ç»Ÿè®¡
        if mode == 'independent':
            results['overall_summary'].update({
                'total_subtasks': total_subtasks,
                'successful_subtasks': successful_subtasks,
                'subtask_success_rate': successful_subtasks / total_subtasks if total_subtasks > 0 else 0.0
            })

        # ä¿å­˜ç»“æœ
        save_parallel_results(results, output_dir, mode)

        # æ˜¾ç¤ºç»“æœ
        logger.info(f"ğŸ¯ å¹¶è¡Œè¯„æµ‹å®Œæˆ")
        logger.info(f"ğŸ“Š æ€»åœºæ™¯æ•°: {len(scenario_list)}")
        logger.info(f"âœ… æˆåŠŸåœºæ™¯: {successful_scenarios}")
        logger.info(f"âŒ å¤±è´¥åœºæ™¯: {failed_scenarios}")
        logger.info(f"ğŸ“Š åœºæ™¯æˆåŠŸç‡: {successful_scenarios / len(scenario_list):.1%}")
        logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        return 0

    except Exception as e:
        logger.exception(f"âŒ å¹¶è¡Œè¯„æµ‹æ‰§è¡Œå¤±è´¥: {e}")
        return 1


def get_scenario_list(scenario_selection):
    """
    æ ¹æ®é…ç½®è·å–è¦è¯„æµ‹çš„åœºæ™¯åˆ—è¡¨

    Args:
        scenario_selection: åœºæ™¯é€‰æ‹©é…ç½®

    Returns:
        List[str]: åœºæ™¯IDåˆ—è¡¨
    """
    mode = scenario_selection.get('mode', 'range')

    if mode == 'all':
        # æ‰«ædata/sceneç›®å½•è·å–æ‰€æœ‰åœºæ™¯
        scene_dir = 'data/scene'
        if not os.path.exists(scene_dir):
            raise RuntimeError(f"åœºæ™¯ç›®å½•ä¸å­˜åœ¨: {scene_dir}")

        scenarios = []
        for filename in os.listdir(scene_dir):
            if filename.endswith('_scene.json'):
                scenario_id = filename.replace('_scene.json', '')
                scenarios.append(scenario_id)
        scenarios.sort()
        return scenarios

    elif mode == 'range':
        range_config = scenario_selection.get('range', {})
        start = range_config.get('start', '00001')
        end = range_config.get('end', '00010')

        # ç”ŸæˆèŒƒå›´å†…çš„åœºæ™¯ID
        start_num = int(start)
        end_num = int(end)
        scenarios = [f"{i:05d}" for i in range(start_num, end_num + 1)]
        return scenarios

    elif mode == 'list':
        return scenario_selection.get('list', ['00001'])

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åœºæ™¯é€‰æ‹©æ¨¡å¼: {mode}")


def save_parallel_results(results, output_dir, evaluation_type):
    """
    ä¿å­˜å¹¶è¡Œè¯„æµ‹ç»“æœï¼ˆåˆå¹¶ä¸ºmetaä¿¡æ¯æ–‡ä»¶ï¼‰

    Args:
        results: è¯„æµ‹ç»“æœæ•°æ®
        output_dir: è¾“å‡ºç›®å½•
        evaluation_type: è¯„æµ‹ç±»å‹
    """
    try:
        # åªä¿å­˜ä¸€ä¸ªåŒ…å«å…³é”®metaä¿¡æ¯çš„æ–‡ä»¶
        meta_file = os.path.join(output_dir, "parallel_evaluation_meta.json")

        # æå–å…³é”®metaä¿¡æ¯
        run_info = results.get('run_info', {})
        overall_summary = results.get('overall_summary', {})
        scenario_results = results.get('scenario_results', [])

        meta_data = {
            'evaluation_info': {
                'run_name': run_info.get('run_name', 'unknown'),
                'evaluation_type': evaluation_type,
                'agent_type': run_info.get('agent_type', 'unknown'),
                'start_time': run_info.get('start_time', ''),
                'total_duration': run_info.get('total_duration', 0),
                'output_dir': run_info.get('output_dir', '')
            },
            'overall_statistics': {
                'total_scenarios': overall_summary.get('total_scenarios', 0),
                'successful_scenarios': overall_summary.get('successful_scenarios', 0),
                'failed_scenarios': overall_summary.get('failed_scenarios', 0),
                'scenario_success_rate': overall_summary.get('scenario_success_rate', 0.0)
            },
            'scenario_summary': []
        }

        # æ·»åŠ independentæ¨¡å¼çš„å­ä»»åŠ¡ç»Ÿè®¡
        if evaluation_type == 'independent':
            meta_data['overall_statistics'].update({
                'total_subtasks': overall_summary.get('total_subtasks', 0),
                'successful_subtasks': overall_summary.get('successful_subtasks', 0),
                'subtask_success_rate': overall_summary.get('subtask_success_rate', 0.0)
            })

            # è®¡ç®—ä»»åŠ¡ç±»åˆ«ç»Ÿè®¡
            category_stats = {}
            for scenario_result in scenario_results:
                subtask_results = scenario_result.get('subtask_results', [])
                for subtask in subtask_results:
                    category = subtask.get('task_category', 'unknown')
                    if category not in category_stats:
                        category_stats[category] = {'total': 0, 'completed': 0}
                    category_stats[category]['total'] += 1
                    if subtask.get('completed', False):
                        category_stats[category]['completed'] += 1

            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å®Œæˆç‡
            for category, stats in category_stats.items():
                stats['completion_rate'] = stats['completed'] / stats['total'] if stats['total'] > 0 else 0.0

            meta_data['task_category_statistics'] = category_stats

        # æ·»åŠ åœºæ™¯çº§åˆ«çš„æ±‡æ€»ä¿¡æ¯
        for scenario_result in scenario_results:
            scenario_summary = {
                'scenario_id': scenario_result.get('scenario_id', 'unknown'),
                'success': scenario_result.get('success', False),
                'duration': scenario_result.get('total_duration', 0)
            }

            summary = scenario_result.get('summary', {})
            scenario_summary.update({
                'completion_rate': summary.get('completion_rate', 0.0),
                'total_steps': summary.get('total_steps', 0),
                'completed_tasks': summary.get('completed_tasks', 0),
                'total_tasks': summary.get('total_tasks', 0)
            })

            meta_data['scenario_summary'].append(scenario_summary)

        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)

        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ“Š å¹¶è¡Œè¯„æµ‹metaä¿¡æ¯å·²ä¿å­˜: {meta_file}")

    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.exception(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶å¹¶åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
    config = load_config_with_overrides(args.config, args)

    # ä»é…ç½®ä¸­è·å–è®¾ç½®
    eval_config = config.get('evaluation', {})
    run_settings = eval_config.get('run_settings', {})

    # ç¡®å®šæœ€ç»ˆå‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼Œç„¶åæ˜¯é…ç½®æ–‡ä»¶ï¼Œæœ€åæ˜¯é»˜è®¤å€¼ï¼‰
    mode = args.mode or eval_config.get('task_type', 'sequential')
    scenario = args.scenario or eval_config.get('default_scenario', '00001')
    suffix = args.suffix or run_settings.get('default_suffix', 'demo')
    log_level = args.log_level or run_settings.get('log_level', 'INFO')

    # è®¾ç½®æ—¥å¿—
    setup_logger(log_level=getattr(logging, log_level))
    logger = logging.getLogger(__name__)

    logger.info("ğŸš€ å¯åŠ¨å•æ™ºèƒ½ä½“ç¤ºä¾‹ï¼ˆä½¿ç”¨ä»»åŠ¡éªŒè¯å™¨ï¼‰")
    logger.info(f"ğŸ“‹ è¯„æµ‹æ¨¡å¼: {mode}")
    logger.info(f"ğŸ  åœºæ™¯ID: {scenario}")
    logger.info(f"ğŸ·ï¸ è¿è¡Œåç¼€: {suffix}")
    logger.info(f"âš™ï¸ é…ç½®æ–‡ä»¶: {args.config}")
    logger.info(f"ğŸ“Š æ—¥å¿—çº§åˆ«: {log_level}")

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œæ¨¡å¼ï¼ˆå‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶ï¼‰
    parallel_enabled = args.parallel
    if not parallel_enabled:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„å¹¶è¡Œè®¾ç½®
        parallel_config = config.get('parallel_evaluation', {})
        parallel_enabled = parallel_config.get('enabled', False)
        if parallel_enabled:
            logger.info("ğŸ”„ æ£€æµ‹åˆ°é…ç½®æ–‡ä»¶ä¸­å¯ç”¨äº†å¹¶è¡Œè¯„æµ‹æ¨¡å¼")

    if parallel_enabled:
        if args.parallel:
            logger.info("ğŸ”„ é€šè¿‡å‘½ä»¤è¡Œå‚æ•°å¯ç”¨å¹¶è¡Œè¯„æµ‹æ¨¡å¼")
        return run_parallel_evaluation(config, args.config, mode, suffix)

    # æ˜¾ç¤ºå…³é”®é…ç½®ä¿¡æ¯
    execution_config = config.get('execution', {})
    task_evaluator_config = config.get('task_evaluator', {})
    env_desc_config = config.get('environment_description', {})
    history_config = config.get('history', {})

    logger.info("âš™ï¸ é…ç½®ä¿¡æ¯:")
    logger.info(f"  - æœ€å¤§æ€»æ­¥æ•°: {execution_config.get('max_total_steps', 200)}")
    logger.info(f"  - æ¯ä»»åŠ¡æœ€å¤§æ­¥æ•°: {task_evaluator_config.get('max_steps_per_task', 20)}")
    logger.info(f"  - ç¯å¢ƒæè¿°çº§åˆ«: {env_desc_config.get('detail_level', 'full')}")
    logger.info(f"  - å†å²è®°å½•é•¿åº¦: {history_config.get('max_history_length', -1)}")
    logger.info(f"  - ä»»åŠ¡è¶…æ—¶æ—¶é—´: {execution_config.get('timeout_seconds', 900)}ç§’")

    try:
        # åˆ›å»ºä»»åŠ¡éªŒè¯å™¨
        evaluator = TaskEvaluator(
            config_file=args.config,
            agent_type='single',
            task_type=mode,
            scenario_id=scenario,
            custom_suffix=suffix
        )

        # å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•
        scenario_output_dir = os.environ.get('SCENARIO_OUTPUT_DIR')
        if scenario_output_dir:
            evaluator.output_dir = scenario_output_dir
            # é‡æ–°åˆå§‹åŒ–è½¨è¿¹è®°å½•å™¨åˆ°æŒ‡å®šç›®å½•ï¼Œä¼ é€’æ­£ç¡®çš„scenario_id
            from utils.trajectory_recorder import TrajectoryRecorder
            evaluator.trajectory_recorder = TrajectoryRecorder(scenario_output_dir, evaluator.run_name, scenario)
            logger.info(f"ğŸ“ ä½¿ç”¨æŒ‡å®šè¾“å‡ºç›®å½•: {scenario_output_dir}")

        logger.info("âœ… ä»»åŠ¡éªŒè¯å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {evaluator.output_dir}")
        logger.info(f"ğŸ“ è½¨è¿¹æ–‡ä»¶: {evaluator.trajectory_recorder.trajectory_file}")
        logger.info(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶: {evaluator.trajectory_recorder.log_file}")

        # æ‰§è¡Œè¯„æµ‹
        logger.info("ğŸ¬ å¼€å§‹æ‰§è¡Œä»»åŠ¡è¯„æµ‹...")
        results = evaluator.run_evaluation(scenario)

        # æ˜¾ç¤ºç»“æœ
        logger.info("\nğŸ‰ ä»»åŠ¡è¯„æµ‹å®Œæˆï¼")

        # è·å–æ±‡æ€»ä¿¡æ¯
        summary = results.get('summary', {})

        # æ˜¾ç¤ºåŸºæœ¬ç»“æœ
        logger.info("ğŸ“Š è¯„æµ‹ç»“æœ:")
        logger.info(f"  - æ€»ä»»åŠ¡æ•°: {summary.get('total_tasks', 0)}")
        logger.info(f"  - å®Œæˆä»»åŠ¡æ•°: {summary.get('completed_tasks', 0)}")
        logger.info(f"  - å¤±è´¥ä»»åŠ¡æ•°: {summary.get('failed_tasks', 0)}")
        logger.info(f"  - å®Œæˆç‡: {summary.get('completion_rate', 0):.1%}")

        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        logger.info(f"  - æ€»æ­¥æ•°: {summary.get('total_steps', 0)}")
        logger.info(f"  - å¹³å‡æ¯ä»»åŠ¡æ­¥æ•°: {summary.get('average_steps_per_task', 0):.1f}")
        logger.info(f"  - æ‰§è¡Œæ—¶é•¿: {summary.get('total_duration', 0):.2f}ç§’")

        # æ˜¾ç¤ºæ€§èƒ½è¯„ä»·
        completion_rate = summary.get('completion_rate', 0)
        if completion_rate >= 0.8:
            logger.info("ğŸŠ è¯„æµ‹ç»“æœä¼˜ç§€ï¼")
        elif completion_rate >= 0.6:
            logger.info("ğŸ‘ è¯„æµ‹ç»“æœè‰¯å¥½ï¼")
        else:
            logger.info("ğŸ“ˆ è¿˜æœ‰æ”¹è¿›ç©ºé—´")

        return 0

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
        return 1


if __name__ == "__main__":
    exit(main())
