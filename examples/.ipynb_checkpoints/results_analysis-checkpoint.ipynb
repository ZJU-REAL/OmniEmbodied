{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Subtask Execution Results Analysis Dashboard\n",
                "\n",
                "## üéØ Overview\n",
                "This comprehensive analysis notebook provides detailed insights into subtask execution performance with beautiful visualizations and multi-dimensional analysis.\n",
                "\n",
                "## üöÄ Features:\n",
                "- **üìà Overall Performance Metrics**: Completion rates, exeution statistics, and key performance indicators\n",
                "- \n",
                "- **üìã Task Category Analysis**: Performance breakdown by task types (direct_command, attribute_reasoning, tool_use, etc.)\n",
                "- **ü§ñ Agent Type Analysis**: Single-agent vs multi-agent performance comparison\n",
                "- **üîç Multi-Dimensional Analysis**: Cross-tabulation and correlation analysis\n",
                "- **‚ö†Ô∏è Bad Case Analysis**: Detailed failure pattern analysis\n",
                "- **üé® Beautiful Visualizations**: Light color scheme with professional styling\n",
                "- **üí° Intelligent Insights**: AI-generated recommendations and actionable insights\n",
                "\n",
                "## üì¶ Required Libraries\n",
                "Run the following command if any libraries are missing:\n",
                "```bash\n",
                "pip install pandas numpy matplotlib seaborn plotly jupyter\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "import os\n",
                "import glob\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style for beautiful visualizations with light color scheme\n",
                "plt.style.use('default')\n",
                "plt.rcParams['figure.facecolor'] = 'white'\n",
                "plt.rcParams['axes.facecolor'] = 'white'\n",
                "plt.rcParams['axes.edgecolor'] = '#CCCCCC'\n",
                "plt.rcParams['axes.linewidth'] = 0.8\n",
                "plt.rcParams['grid.color'] = '#E5E5E5'\n",
                "plt.rcParams['grid.linewidth'] = 0.5\n",
                "plt.rcParams['text.color'] = '#333333'\n",
                "plt.rcParams['axes.labelcolor'] = '#333333'\n",
                "plt.rcParams['xtick.color'] = '#666666'\n",
                "plt.rcParams['ytick.color'] = '#666666'\n",
                "plt.rcParams['font.size'] = 11\n",
                "plt.rcParams['axes.titlesize'] = 14\n",
                "plt.rcParams['axes.labelsize'] = 12\n",
                "\n",
                "# Define beautiful light color palettes\n",
                "LIGHT_COLORS = {\n",
                "    'primary': ['#E8F4FD', '#B3E0FF', '#7CC7FF', '#4DAAFF', '#1A8CFF', '#0066CC'],\n",
                "    'success': ['#E8F5E8', '#C8E6C9', '#A5D6A7', '#81C784', '#66BB6A', '#4CAF50'],\n",
                "    'warning': ['#FFF8E1', '#FFECB3', '#FFE082', '#FFD54F', '#FFCA28', '#FFC107'],\n",
                "    'error': ['#FFEBEE', '#FFCDD2', '#EF9A9A', '#E57373', '#EF5350', '#F44336'],\n",
                "    'info': ['#E3F2FD', '#BBDEFB', '#90CAF9', '#64B5F6', '#42A5F5', '#2196F3'],\n",
                "    'purple': ['#F3E5F5', '#E1BEE7', '#CE93D8', '#BA68C8', '#AB47BC', '#9C27B0'],\n",
                "    'teal': ['#E0F2F1', '#B2DFDB', '#80CBC4', '#4DB6AC', '#26A69A', '#009688'],\n",
                "    'orange': ['#FFF3E0', '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', '#FF9800']\n",
                "}\n",
                "\n",
                "# Set seaborn style with light theme\n",
                "sns.set_style(\"whitegrid\", {\n",
                "    \"axes.spines.left\": True,\n",
                "    \"axes.spines.bottom\": True,\n",
                "    \"axes.spines.top\": False,\n",
                "    \"axes.spines.right\": False,\n",
                "    \"axes.linewidth\": 0.8,\n",
                "    \"grid.linewidth\": 0.5,\n",
                "    \"grid.color\": \"#E5E5E5\"\n",
                "})\n",
                "\n",
                "# Configure display options\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.width', None)\n",
                "pd.set_option('display.max_colwidth', 50)\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")\n",
                "print(\"üé® Beautiful light color scheme configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÅ Data Loading and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_latest_log_file(output_dir='../output', custom_csv_path=None):\n",
                "    \"\"\"\n",
                "    Find the latest subtask_execution_log.csv file or use manual/custom path\n",
                "    \n",
                "    Args:\n",
                "        output_dir: Directory to search for log files\n",
                "        manual_path: Manual path to specific log file (deprecated, use custom_csv_path)\n",
                "        custom_csv_path: Custom path to any CSV file (supports any CSV file, not just subtask_execution_log.csv)\n",
                "    \n",
                "    Returns:\n",
                "        str: Path to the log file\n",
                "    \"\"\"\n",
                "    # Priority: custom_csv_path > manual_path > auto-detection\n",
                "    \n",
                "    # Check custom CSV path first (highest priority)\n",
                "    if custom_csv_path:\n",
                "        if os.path.exists(custom_csv_path):\n",
                "            if custom_csv_path.endswith('.csv'):\n",
                "                print(f\"üìÅ Using custom CSV path: {custom_csv_path}\")\n",
                "                return custom_csv_path\n",
                "            else:\n",
                "                print(f\"‚ö†Ô∏è  Warning: Custom path '{custom_csv_path}' is not a CSV file, but proceeding...\")\n",
                "                return custom_csv_path\n",
                "        else:\n",
                "            print(f\"‚ùå Custom CSV path does not exist: {custom_csv_path}\")\n",
                "            print(f\"üîç Falling back to auto-detection...\")\n",
                "    \n",
                "    # Auto-detection: Find all subtask_execution_log.csv files\n",
                "    pattern = os.path.join(output_dir, '**/subtask_execution_log.csv')\n",
                "    log_files = glob.glob(pattern, recursive=True)\n",
                "    \n",
                "    if not log_files:\n",
                "        raise FileNotFoundError(f\"‚ùå No subtask_execution_log.csv files found in {output_dir}\")\n",
                "    \n",
                "    # Sort by modification time and get the latest\n",
                "    latest_file = max(log_files, key=os.path.getmtime)\n",
                "    print(f\"üìÅ Using latest log file: {latest_file}\")\n",
                "    return latest_file\n",
                "\n",
                "def validate_data_for_plotting(df):\n",
                "    \"\"\"\n",
                "    Validate data and report potential issues that could cause empty plots\n",
                "    \"\"\"\n",
                "    print(\"\\nüîç DATA VALIDATION FOR PLOTTING\")\n",
                "    print(\"-\" * 40)\n",
                "    \n",
                "    # Check for required columns\n",
                "    required_cols = ['task_category', 'subtask_completed', 'total_steps', 'duration_seconds', 'command_success_rate_numeric']\n",
                "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
                "    if missing_cols:\n",
                "        print(f\"‚ö†Ô∏è Missing columns: {missing_cols}\")\n",
                "    \n",
                "    # Check data distribution\n",
                "    print(f\"üìä Data shape: {df.shape}\")\n",
                "    print(f\"üìã Task categories: {list(df['task_category'].unique())}\")\n",
                "    print(f\"‚úÖ Completed tasks: {df['subtask_completed'].sum()}/{len(df)} ({df['subtask_completed'].mean()*100:.1f}%)\")\n",
                "    \n",
                "    # Check for empty categories\n",
                "    category_counts = df['task_category'].value_counts()\n",
                "    print(f\"üìà Tasks per category:\")\n",
                "    for cat, count in category_counts.items():\n",
                "        success_rate = df[df['task_category']==cat]['subtask_completed'].mean() * 100\n",
                "        print(f\"  - {cat}: {count} tasks ({success_rate:.1f}% success)\")\n",
                "    \n",
                "    # Check for NaN values\n",
                "    nan_cols = df.isnull().sum()\n",
                "    nan_cols = nan_cols[nan_cols > 0]\n",
                "    if len(nan_cols) > 0:\n",
                "        print(f\"‚ö†Ô∏è Columns with NaN values: {dict(nan_cols)}\")\n",
                "    \n",
                "    return True\n",
                "\n",
                "def safe_numeric_conversion(series, default_value=0):\n",
                "    \"\"\"\n",
                "    Safely convert a series to numeric, handling NaN and missing values\n",
                "    \"\"\"\n",
                "    try:\n",
                "        return pd.to_numeric(series, errors='coerce').fillna(default_value)\n",
                "    except Exception:\n",
                "        return pd.Series([default_value] * len(series), index=series.index)\n",
                "\n",
                "def load_and_preprocess_data(file_path):\n",
                "    \"\"\"\n",
                "    Load and preprocess the subtask execution log data\n",
                "    \n",
                "    Args:\n",
                "        file_path: Path to the CSV file\n",
                "    \n",
                "    Returns:\n",
                "        pd.DataFrame: Preprocessed data\n",
                "    \"\"\"\n",
                "    # Load data\n",
                "    df = pd.read_csv(file_path)\n",
                "    \n",
                "    # Convert timestamps\n",
                "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
                "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
                "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
                "    \n",
                "    # Convert boolean columns\n",
                "    df['task_executed'] = df['task_executed'].astype(bool)\n",
                "    df['subtask_completed'] = df['subtask_completed'].astype(bool)\n",
                "    \n",
                "    # Convert command_success_rate from percentage string to float\n",
                "    # First ensure the column is string type and handle NaN values\n",
                "    df['command_success_rate'] = df['command_success_rate'].astype(str)\n",
                "    df['command_success_rate_numeric'] = df['command_success_rate'].str.rstrip('%').astype(float)\n",
                "    \n",
                "    # Extract scenario number from scenario_id (handle missing values)\n",
                "    # First ensure the column is string type\n",
                "    df['scenario_id'] = df['scenario_id'].astype(str)\n",
                "    scenario_numbers = df['scenario_id'].str.extract(r'(\\\\d+)')\n",
                "    df['scenario_number'] = pd.to_numeric(scenario_numbers[0], errors='coerce').fillna(0).astype(int)\n",
                "    \n",
                "    # Add derived columns\n",
                "    df['failure_rate'] = 100 - df['command_success_rate_numeric']\n",
                "    df['steps_per_second'] = df['total_steps'] / df['duration_seconds']\n",
                "    df['efficiency_score'] = (df['command_success_rate_numeric'] * df['subtask_completed'].astype(int)) / (df['total_steps'] + 1)\n",
                "    \n",
                "    print(f\"üìä Loaded {len(df)} records from {file_path}\")\n",
                "    print(f\"üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
                "    print(f\"üìã Task categories: {', '.join(df['task_category'].unique())}\")\n",
                "    print(f\"ü§ñ Agent types: {', '.join(df['agent_type'].unique())}\")\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Load data - You can specify a custom CSV file path\n",
                "# Ê≥®ÊÑèÔºöËØ∑ÂÖàËøêË°åËØÑÊµã‰ªªÂä°ÁîüÊàêÊï∞ÊçÆÊñá‰ª∂ÔºåÊàñËÄÖÂ∞ÜË∑ØÂæÑ‰øÆÊîπ‰∏∫ÂÆûÈôÖÂ≠òÂú®ÁöÑCSVÊñá‰ª∂\n",
                "custom_csv_path =   \"../output/20250706_164502_single_parallel_independent_scenario_multi_demo/subtask_execution_log.csv\"\n",
                "# custom_csv_path = \"/Users/wangzixuan/workspace/sim/embodied_framework/output/20250704_164535_single_parallel_independent_scenario_multi_demo/subtask_execution_log.csv\"  # Á§∫‰æãÁªùÂØπË∑ØÂæÑ\n",
                "# Examples of usage:\n",
                "# custom_csv_path = '../output/specific_folder/subtask_execution_log.csv'  # Specific subtask log\n",
                "# custom_csv_path = '../output/my_custom_results.csv'  # Any custom CSV file\n",
                "# custom_csv_path = '/absolute/path/to/your/data.csv'  # Absolute path\n",
                "\n",
                "log_file_path = find_latest_log_file(custom_csv_path=custom_csv_path)\n",
                "df = load_and_preprocess_data(log_file_path)\n",
                "\n",
                "# Display basic info about the dataset\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìã DATASET OVERVIEW\")\n",
                "print(\"=\"*60)\n",
                "print(df.info())\n",
                "print(\"\\nüìä First few rows:\")\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Overall Performance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_overall_performance(df):\n",
                "    \"\"\"\n",
                "    Analyze overall performance metrics with beautiful visualizations\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üìä OVERALL PERFORMANCE METRICS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    total_tasks = len(df)\n",
                "    completed_tasks = df['subtask_completed'].sum()\n",
                "    executed_tasks = df['task_executed'].sum()\n",
                "    completion_rate = (completed_tasks / total_tasks) * 100\n",
                "    execution_rate = (executed_tasks / total_tasks) * 100\n",
                "    \n",
                "    print(f\"üìà Total Tasks Analyzed: {total_tasks}\")\n",
                "    print(f\"‚úÖ Tasks Executed: {executed_tasks} ({execution_rate:.1f}%)\")\n",
                "    print(f\"‚úÖ Successfully Completed: {completed_tasks} ({completion_rate:.1f}%)\")\n",
                "    print(f\"‚ùå Failed Tasks: {total_tasks - completed_tasks} ({100 - completion_rate:.1f}%)\")\n",
                "    print(f\"‚è±Ô∏è  Average Duration: {df['duration_seconds'].mean():.1f} seconds\")\n",
                "    print(f\"üî¢ Average Steps per Task: {df['total_steps'].mean():.1f}\")\n",
                "    print(f\"üéØ Average Command Success Rate: {df['command_success_rate_numeric'].mean():.1f}%\")\n",
                "    print(f\"ü§ñ Average LLM Interactions: {df['llm_interactions'].mean():.1f}\")\n",
                "    \n",
                "    # Task Success Rate by Category (Most Important Analysis)\n",
                "    print(f\"\\nüìä TASK SUCCESS RATE BY CATEGORY (KEY METRICS)\")\n",
                "    print(\"-\" * 60)\n",
                "    category_success = df.groupby('task_category')['subtask_completed'].agg(['count', 'sum', 'mean']).round(3)\n",
                "    category_success.columns = ['Total_Tasks', 'Successful_Tasks', 'Success_Rate']\n",
                "    category_success['Success_Rate_Percent'] = (category_success['Success_Rate'] * 100).round(1)\n",
                "    category_success['Failed_Tasks'] = category_success['Total_Tasks'] - category_success['Successful_Tasks']\n",
                "    \n",
                "    # Reorder columns for better readability\n",
                "    category_success = category_success[['Total_Tasks', 'Successful_Tasks', 'Failed_Tasks', 'Success_Rate_Percent']]\n",
                "    print(category_success.to_string())\n",
                "    \n",
                "    # Average Steps by Category (Second Most Important)\n",
                "    print(f\"\\nüë£ AVERAGE STEPS BY TASK CATEGORY\")\n",
                "    print(\"-\" * 40)\n",
                "    steps_by_category = df.groupby('task_category')['total_steps'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
                "    steps_by_category.columns = ['Avg_Steps', 'Std_Steps', 'Min_Steps', 'Max_Steps']\n",
                "    print(steps_by_category.to_string())\n",
                "    \n",
                "    # Validate data before plotting\n",
                "    validate_data_for_plotting(df)\n",
                "    \n",
                "    # Create beautiful overview dashboard\n",
                "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "    fig.patch.set_facecolor('white')\n",
                "    fig.suptitle('üìä OVERALL PERFORMANCE METRICS - Task Success Analysis', fontsize=20, fontweight='bold', y=0.98)\n",
                "    \n",
                "    # 1. Completion Rate Pie Chart\n",
                "    ax1 = axes[0, 0]\n",
                "    completion_counts = df['subtask_completed'].value_counts()\n",
                "    colors = [LIGHT_COLORS['error'][2], LIGHT_COLORS['success'][2]]\n",
                "    wedges, texts, autotexts = ax1.pie(completion_counts.values, labels=['Failed', 'Completed'], \n",
                "                                      autopct='%1.1f%%', colors=colors, startangle=90,\n",
                "                                      wedgeprops=dict(edgecolor='white', linewidth=3))\n",
                "    ax1.set_title('üéØ Task Success Rate\\n(Most Important Metric)', fontweight='bold', pad=20, fontsize=14)\n",
                "    for autotext in autotexts:\n",
                "        autotext.set_color('white')\n",
                "        autotext.set_fontweight('bold')\n",
                "        autotext.set_fontsize(12)\n",
                "    \n",
                "    # 2. Execution Status Distribution\n",
                "    ax2 = axes[0, 1]\n",
                "    status_counts = df['status'].value_counts()\n",
                "    colors2 = LIGHT_COLORS['primary'][:len(status_counts)]\n",
                "    bars2 = ax2.bar(status_counts.index, status_counts.values, color=colors2, \n",
                "                    edgecolor='white', linewidth=2)\n",
                "    ax2.set_title('üìã Task Execution Status\\nDistribution', fontweight='bold', pad=20, fontsize=14)\n",
                "    ax2.set_ylabel('Count')\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    for bar in bars2:\n",
                "        height = bar.get_height()\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
                "                 f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 3. Command Success Rate Distribution\n",
                "    ax3 = axes[0, 2]\n",
                "    ax3.hist(df['command_success_rate_numeric'], bins=20, color=LIGHT_COLORS['info'][2], \n",
                "             alpha=0.8, edgecolor='white', linewidth=1.5)\n",
                "    ax3.axvline(df['command_success_rate_numeric'].mean(), color=LIGHT_COLORS['error'][3], \n",
                "                linestyle='--', linewidth=2, label=f'Mean: {df[\"command_success_rate_numeric\"].mean():.1f}%')\n",
                "    ax3.set_title('Command Success Rate Distribution', fontweight='bold', pad=20)\n",
                "    ax3.set_xlabel('Command Success Rate (%)')\n",
                "    ax3.set_ylabel('Frequency')\n",
                "    ax3.legend()\n",
                "    ax3.grid(True, alpha=0.3)\n",
                "    \n",
                "    return fig\n",
                "\n",
                "# Generate overall performance analysis\n",
                "overall_fig = analyze_overall_performance(df)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã Task Category Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_task_categories(df):\n",
                "    \"\"\"\n",
                "    Analyze performance by task category with beautiful visualizations\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üìã TASK CATEGORY ANALYSIS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    category_stats = df.groupby('task_category').agg({\n",
                "        'subtask_completed': ['count', 'sum', 'mean'],\n",
                "        'total_steps': ['mean', 'std'],\n",
                "        'duration_seconds': ['mean', 'std'],\n",
                "        'command_success_rate_numeric': ['mean', 'std'],\n",
                "        'llm_interactions': ['mean', 'std']\n",
                "    }).round(2)\n",
                "    \n",
                "    category_stats.columns = ['_'.join(col).strip() for col in category_stats.columns]\n",
                "    category_summary = category_stats[['subtask_completed_count', 'subtask_completed_sum', 'subtask_completed_mean',\n",
                "                                      'total_steps_mean', 'duration_seconds_mean', 'command_success_rate_numeric_mean']]\n",
                "    category_summary.columns = ['Total_Tasks', 'Completed_Tasks', 'Completion_Rate', \n",
                "                               'Avg_Steps', 'Avg_Duration', 'Avg_Command_Success']\n",
                "    category_summary['Completion_Rate'] = (category_summary['Completion_Rate'] * 100).round(1)\n",
                "    \n",
                "    print(category_summary)\n",
                "    \n",
                "    best_category = category_summary['Completion_Rate'].idxmax()\n",
                "    worst_category = category_summary['Completion_Rate'].idxmin()\n",
                "    fastest_category = category_summary['Avg_Duration'].idxmin()\n",
                "    slowest_category = category_summary['Avg_Duration'].idxmax()\n",
                "    \n",
                "    print(f\"\\nüèÜ Best Performing Category: {best_category} ({category_summary.loc[best_category, 'Completion_Rate']:.1f}% completion)\")\n",
                "    print(f\"‚ö†Ô∏è  Worst Performing Category: {worst_category} ({category_summary.loc[worst_category, 'Completion_Rate']:.1f}% completion)\")\n",
                "    print(f\"‚ö° Fastest Category: {fastest_category} ({category_summary.loc[fastest_category, 'Avg_Duration']:.1f}s avg)\")\n",
                "    print(f\"üêå Slowest Category: {slowest_category} ({category_summary.loc[slowest_category, 'Avg_Duration']:.1f}s avg)\")\n",
                "    \n",
                "    # Create comprehensive category analysis dashboard\n",
                "    fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
                "    fig.patch.set_facecolor('white')\n",
                "    fig.suptitle('üìã Task Category Performance Analysis', fontsize=20, fontweight='bold', y=0.98)\n",
                "    \n",
                "    categories = category_summary.index\n",
                "    n_categories = len(categories)\n",
                "    colors = LIGHT_COLORS['primary'][:n_categories] if n_categories <= 6 else sns.color_palette(\"husl\", n_categories)\n",
                "    \n",
                "    # 1. Task Distribution by Category\n",
                "    ax1 = axes[0, 0]\n",
                "    category_counts = df['task_category'].value_counts()\n",
                "    wedges, texts, autotexts = ax1.pie(category_counts.values, labels=category_counts.index, \n",
                "                                      autopct='%1.1f%%', colors=colors, startangle=90,\n",
                "                                      wedgeprops=dict(edgecolor='white', linewidth=2))\n",
                "    ax1.set_title('Task Distribution by Category', fontweight='bold', pad=20)\n",
                "    for autotext in autotexts:\n",
                "        autotext.set_color('white')\n",
                "        autotext.set_fontweight('bold')\n",
                "    \n",
                "    # 2. Completion Rate by Category\n",
                "    ax2 = axes[0, 1]\n",
                "    bars2 = ax2.bar(categories, category_summary['Completion_Rate'], color=colors,\n",
                "                    edgecolor='white', linewidth=2)\n",
                "    ax2.set_title('üéØ Task Success Rate by Category\\n(Key Performance Indicator)', fontweight='bold', pad=20, fontsize=14)\n",
                "    ax2.set_ylabel('Completion Rate (%)')\n",
                "    ax2.set_ylim(0, 105)\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    ax2.tick_params(axis='x', rotation=45)\n",
                "    for bar in bars2:\n",
                "        height = bar.get_height()\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
                "                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 3. Average Steps by Category\n",
                "    ax3 = axes[0, 2]\n",
                "    bars3 = ax3.bar(categories, category_summary['Avg_Steps'], color=LIGHT_COLORS['info'][:n_categories],\n",
                "                    edgecolor='white', linewidth=2)\n",
                "    ax3.set_title('üë£ Average Steps by Category\\n(Efficiency Metric)', fontweight='bold', pad=20, fontsize=14)\n",
                "    ax3.set_ylabel('Average Steps')\n",
                "    ax3.grid(True, alpha=0.3)\n",
                "    ax3.tick_params(axis='x', rotation=45)\n",
                "    for bar in bars3:\n",
                "        height = bar.get_height()\n",
                "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
                "                 f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 4. Duration Distribution by Category (Box Plot)\n",
                "    ax4 = axes[1, 0]\n",
                "    sns.boxplot(data=df, x='task_category', y='duration_seconds', ax=ax4, palette=colors)\n",
                "    ax4.set_title('Duration Distribution by Category', fontweight='bold', pad=20)\n",
                "    ax4.set_xlabel('Task Category')\n",
                "    ax4.set_ylabel('Duration (seconds)')\n",
                "    ax4.tick_params(axis='x', rotation=45)\n",
                "    ax4.grid(True, alpha=0.3)\n",
                "    \n",
                "    # 5. Command Success Rate by Category (Violin Plot)\n",
                "    ax5 = axes[1, 1]\n",
                "    sns.violinplot(data=df, x='task_category', y='command_success_rate_numeric', ax=ax5, palette=colors)\n",
                "    ax5.set_title('Command Success Rate Distribution', fontweight='bold', pad=20)\n",
                "    ax5.set_xlabel('Task Category')\n",
                "    ax5.set_ylabel('Command Success Rate (%)')\n",
                "    ax5.tick_params(axis='x', rotation=45)\n",
                "    ax5.grid(True, alpha=0.3)\n",
                "    \n",
                "    # 6. Steps vs Success Rate Scatter\n",
                "    ax6 = axes[1, 2]\n",
                "    for i, category in enumerate(categories):\n",
                "        cat_data = df[df['task_category'] == category]\n",
                "        ax6.scatter(cat_data['total_steps'], cat_data['command_success_rate_numeric'], \n",
                "                   color=colors[i], label=category, alpha=0.7, s=60, edgecolors='white', linewidth=1)\n",
                "    ax6.set_title('Steps vs Command Success Rate', fontweight='bold', pad=20)\n",
                "    ax6.set_xlabel('Total Steps')\n",
                "    ax6.set_ylabel('Command Success Rate (%)')\n",
                "    ax6.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "    ax6.grid(True, alpha=0.3)\n",
                "    \n",
                "    return fig, category_summary\n",
                "\n",
                "# Generate task category analysis\n",
                "category_fig, category_summary = analyze_task_categories(df)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Detailed Category-wise Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_detailed_category_stats(df):\n",
                "    \"\"\"\n",
                "    Detailed statistical analysis for each task category and agent type\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"üìä DETAILED CATEGORY-WISE STATISTICAL ANALYSIS\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # 1. Comprehensive statistics by category\n",
                "    print(\"\\nüìã 1. COMPREHENSIVE STATISTICS BY TASK CATEGORY\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    category_detailed_stats = df.groupby('task_category').agg({\n",
                "        'subtask_completed': ['count', 'sum', 'mean'],\n",
                "        'total_steps': ['mean', 'std', 'min', 'max'],\n",
                "        'duration_seconds': ['mean', 'std', 'min', 'max'],\n",
                "        'command_success_rate_numeric': ['mean', 'std', 'min', 'max'],\n",
                "        'llm_interactions': ['mean', 'std', 'min', 'max'],\n",
                "        'efficiency_score': ['mean', 'std']\n",
                "    }).round(2)\n",
                "    \n",
                "    # Flatten column names\n",
                "    category_detailed_stats.columns = ['_'.join(col).strip() for col in category_detailed_stats.columns]\n",
                "    \n",
                "    # Create summary table\n",
                "    category_summary_table = pd.DataFrame({\n",
                "        'Total_Tasks': category_detailed_stats['subtask_completed_count'],\n",
                "        'Completed': category_detailed_stats['subtask_completed_sum'],\n",
                "        'Completion_Rate_%': (category_detailed_stats['subtask_completed_mean'] * 100).round(1),\n",
                "        'Avg_Steps': category_detailed_stats['total_steps_mean'],\n",
                "        'Avg_Duration_s': category_detailed_stats['duration_seconds_mean'],\n",
                "        'Avg_Cmd_Success_%': category_detailed_stats['command_success_rate_numeric_mean'],\n",
                "        'Avg_LLM_Interactions': category_detailed_stats['llm_interactions_mean'],\n",
                "        'Efficiency_Score': category_detailed_stats['efficiency_score_mean']\n",
                "    })\n",
                "    \n",
                "    print(category_summary_table.to_string())\n",
                "    \n",
                "    # 2. Agent type analysis (if multiple agent types exist)\n",
                "    if len(df['agent_type'].unique()) > 1:\n",
                "        print(\"\\nü§ñ 2. COMPREHENSIVE STATISTICS BY AGENT TYPE\")\n",
                "        print(\"-\" * 60)\n",
                "        \n",
                "        agent_detailed_stats = df.groupby('agent_type').agg({\n",
                "            'subtask_completed': ['count', 'sum', 'mean'],\n",
                "            'total_steps': ['mean', 'std'],\n",
                "            'duration_seconds': ['mean', 'std'],\n",
                "            'command_success_rate_numeric': ['mean', 'std'],\n",
                "            'llm_interactions': ['mean', 'std'],\n",
                "            'efficiency_score': ['mean', 'std']\n",
                "        }).round(2)\n",
                "        \n",
                "        agent_detailed_stats.columns = ['_'.join(col).strip() for col in agent_detailed_stats.columns]\n",
                "        \n",
                "        agent_summary_table = pd.DataFrame({\n",
                "            'Total_Tasks': agent_detailed_stats['subtask_completed_count'],\n",
                "            'Completed': agent_detailed_stats['subtask_completed_sum'],\n",
                "            'Completion_Rate_%': (agent_detailed_stats['subtask_completed_mean'] * 100).round(1),\n",
                "            'Avg_Steps': agent_detailed_stats['total_steps_mean'],\n",
                "            'Avg_Duration_s': agent_detailed_stats['duration_seconds_mean'],\n",
                "            'Avg_Cmd_Success_%': agent_detailed_stats['command_success_rate_numeric_mean'],\n",
                "            'Avg_LLM_Interactions': agent_detailed_stats['llm_interactions_mean'],\n",
                "            'Efficiency_Score': agent_detailed_stats['efficiency_score_mean']\n",
                "        })\n",
                "        \n",
                "        print(agent_summary_table.to_string())\n",
                "    else:\n",
                "        print(f\"\\nü§ñ 2. SINGLE AGENT TYPE DETECTED: {df['agent_type'].unique()[0]}\")\n",
                "        print(\"   All statistics above apply to this agent type.\")\n",
                "    \n",
                "    # 3. Create detailed visualization dashboard\n",
                "    fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
                "    fig.patch.set_facecolor('white')\n",
                "    fig.suptitle('üìä Detailed Category-wise Performance Analysis', fontsize=20, fontweight='bold', y=0.98)\n",
                "    \n",
                "    categories = df['task_category'].unique()\n",
                "    colors = LIGHT_COLORS['primary'][:len(categories)] if len(categories) <= 6 else sns.color_palette(\"husl\", len(categories))\n",
                "    \n",
                "    # 1. Completion Rate by Category (Bar Chart)\n",
                "    ax1 = axes[0, 0]\n",
                "    completion_rates = category_summary_table['Completion_Rate_%']\n",
                "    bars1 = ax1.bar(completion_rates.index, completion_rates.values, \n",
                "                    color=colors, edgecolor='white', linewidth=2)\n",
                "    ax1.set_title('Completion Rate by Category', fontweight='bold', pad=20)\n",
                "    ax1.set_ylabel('Completion Rate (%)')\n",
                "    ax1.tick_params(axis='x', rotation=45)\n",
                "    ax1.set_ylim(0, 105)\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    for bar in bars1:\n",
                "        height = bar.get_height()\n",
                "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
                "                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 2. Average Steps by Category\n",
                "    ax2 = axes[0, 1]\n",
                "    avg_steps = category_summary_table['Avg_Steps']\n",
                "    bars2 = ax2.bar(avg_steps.index, avg_steps.values, \n",
                "                    color=colors, edgecolor='white', linewidth=2)\n",
                "    ax2.set_title('Average Steps by Category', fontweight='bold', pad=20)\n",
                "    ax2.set_ylabel('Average Steps')\n",
                "    ax2.tick_params(axis='x', rotation=45)\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    for bar in bars2:\n",
                "        height = bar.get_height()\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
                "                 f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 3. Average Duration by Category\n",
                "    ax3 = axes[0, 2]\n",
                "    avg_duration = category_summary_table['Avg_Duration_s']\n",
                "    bars3 = ax3.bar(avg_duration.index, avg_duration.values, \n",
                "                    color=colors, edgecolor='white', linewidth=2)\n",
                "    ax3.set_title('Average Duration by Category', fontweight='bold', pad=20)\n",
                "    ax3.set_ylabel('Average Duration (seconds)')\n",
                "    ax3.tick_params(axis='x', rotation=45)\n",
                "    ax3.grid(True, alpha=0.3)\n",
                "    for bar in bars3:\n",
                "        height = bar.get_height()\n",
                "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
                "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 4. Command Success Rate by Category\n",
                "    ax4 = axes[1, 0]\n",
                "    cmd_success = category_summary_table['Avg_Cmd_Success_%']\n",
                "    bars4 = ax4.bar(cmd_success.index, cmd_success.values, \n",
                "                    color=colors, edgecolor='white', linewidth=2)\n",
                "    ax4.set_title('Command Success Rate by Category', fontweight='bold', pad=20)\n",
                "    ax4.set_ylabel('Command Success Rate (%)')\n",
                "    ax4.tick_params(axis='x', rotation=45)\n",
                "    ax4.set_ylim(0, 105)\n",
                "    ax4.grid(True, alpha=0.3)\n",
                "    for bar in bars4:\n",
                "        height = bar.get_height()\n",
                "        ax4.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
                "                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 5. LLM Interactions by Category\n",
                "    ax5 = axes[1, 1]\n",
                "    llm_interactions = category_summary_table['Avg_LLM_Interactions']\n",
                "    bars5 = ax5.bar(llm_interactions.index, llm_interactions.values, \n",
                "                    color=colors, edgecolor='white', linewidth=2)\n",
                "    ax5.set_title('Average LLM Interactions by Category', fontweight='bold', pad=20)\n",
                "    ax5.set_ylabel('Average LLM Interactions')\n",
                "    ax5.tick_params(axis='x', rotation=45)\n",
                "    ax5.grid(True, alpha=0.3)\n",
                "    for bar in bars5:\n",
                "        height = bar.get_height()\n",
                "        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
                "                 f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 6. Efficiency Score by Category\n",
                "    ax6 = axes[1, 2]\n",
                "    efficiency = category_summary_table['Efficiency_Score']\n",
                "    bars6 = ax6.bar(efficiency.index, efficiency.values, \n",
                "                    color=colors, edgecolor='white', linewidth=2)\n",
                "    ax6.set_title('Efficiency Score by Category', fontweight='bold', pad=20)\n",
                "    ax6.set_ylabel('Efficiency Score')\n",
                "    ax6.tick_params(axis='x', rotation=45)\n",
                "    ax6.grid(True, alpha=0.3)\n",
                "    for bar in bars6:\n",
                "        height = bar.get_height()\n",
                "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
                "                 f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 7. Steps Distribution by Category (Box Plot)\n",
                "    ax7 = axes[2, 0]\n",
                "    sns.boxplot(data=df, x='task_category', y='total_steps', ax=ax7, \n",
                "                palette=colors)\n",
                "    ax7.set_title('Steps Distribution by Category', fontweight='bold', pad=20)\n",
                "    ax7.set_xlabel('Task Category')\n",
                "    ax7.set_ylabel('Total Steps')\n",
                "    ax7.tick_params(axis='x', rotation=45)\n",
                "    ax7.grid(True, alpha=0.3)\n",
                "    \n",
                "    # 8. Duration Distribution by Category (Box Plot)\n",
                "    ax8 = axes[2, 1]\n",
                "    sns.boxplot(data=df, x='task_category', y='duration_seconds', ax=ax8,\n",
                "                palette=colors)\n",
                "    ax8.set_title('Duration Distribution by Category', fontweight='bold', pad=20)\n",
                "    ax8.set_xlabel('Task Category')\n",
                "    ax8.set_ylabel('Duration (seconds)')\n",
                "    ax8.tick_params(axis='x', rotation=45)\n",
                "    ax8.grid(True, alpha=0.3)\n",
                "    \n",
                "    # 9. Scatter plot: Steps vs Duration by Category\n",
                "    ax9 = axes[2, 2]\n",
                "    for i, category in enumerate(categories):\n",
                "        cat_data = df[df['task_category'] == category]\n",
                "        ax9.scatter(cat_data['total_steps'], cat_data['duration_seconds'],\n",
                "                   alpha=0.7, color=colors[i], label=category, s=60,\n",
                "                   edgecolors='white', linewidth=1)\n",
                "    ax9.set_title('Steps vs Duration by Category', fontweight='bold', pad=20)\n",
                "    ax9.set_xlabel('Total Steps')\n",
                "    ax9.set_ylabel('Duration (seconds)')\n",
                "    ax9.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "    ax9.grid(True, alpha=0.3)\n",
                "    \n",
                "    return fig, {\n",
                "        'category_detailed_stats': category_summary_table,\n",
                "        'agent_stats': agent_summary_table if len(df['agent_type'].unique()) > 1 else None\n",
                "    }\n",
                "\n",
                "# Generate detailed category statistics\n",
                "detailed_stats_fig, detailed_stats_data = analyze_detailed_category_stats(df)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü•ß Completion Rate Analysis with Pie Charts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_completion_pie_charts(df):\n",
                "    \"\"\"\n",
                "    Create comprehensive pie chart analysis for completion rates\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"ü•ß COMPLETION RATE PIE CHART ANALYSIS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Create pie chart dashboard\n",
                "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "    fig.patch.set_facecolor('white')\n",
                "    fig.suptitle('ü•ß Comprehensive Completion Rate Analysis', fontsize=20, fontweight='bold', y=0.98)\n",
                "    \n",
                "    # 1. Overall Completion Rate\n",
                "    ax1 = axes[0, 0]\n",
                "    overall_completion = df['subtask_completed'].value_counts()\n",
                "    colors_overall = [LIGHT_COLORS['error'][2], LIGHT_COLORS['success'][2]]\n",
                "    labels_overall = ['Failed', 'Completed']\n",
                "    wedges1, texts1, autotexts1 = ax1.pie(overall_completion.values, labels=labels_overall,\n",
                "                                          autopct='%1.1f%%', colors=colors_overall, startangle=90,\n",
                "                                          wedgeprops=dict(edgecolor='white', linewidth=3))\n",
                "    ax1.set_title('Overall Task Completion Rate', fontweight='bold', pad=20)\n",
                "    for autotext in autotexts1:\n",
                "        autotext.set_color('white')\n",
                "        autotext.set_fontweight('bold')\n",
                "    \n",
                "    # 2. Completion Rate by Category\n",
                "    categories = df['task_category'].unique()\n",
                "    category_colors = LIGHT_COLORS['primary'][:len(categories)] if len(categories) <= 6 else sns.color_palette(\"husl\", len(categories))\n",
                "    \n",
                "    for i, category in enumerate(categories[:4]):  # Show up to 4 categories\n",
                "        row = (i + 1) // 3\n",
                "        col = (i + 1) % 3\n",
                "        ax = axes[row, col]\n",
                "        \n",
                "        cat_data = df[df['task_category'] == category]\n",
                "        cat_completion = cat_data['subtask_completed'].value_counts()\n",
                "        \n",
                "        if len(cat_completion) == 2:\n",
                "            colors_cat = [LIGHT_COLORS['error'][2], LIGHT_COLORS['success'][2]]\n",
                "            labels_cat = ['Failed', 'Completed']\n",
                "        else:\n",
                "            # Handle case where all tasks are completed or all failed\n",
                "            if cat_completion.index[0] == True:\n",
                "                colors_cat = [LIGHT_COLORS['success'][2]]\n",
                "                labels_cat = ['Completed']\n",
                "            else:\n",
                "                colors_cat = [LIGHT_COLORS['error'][2]]\n",
                "                labels_cat = ['Failed']\n",
                "        \n",
                "        wedges, texts, autotexts = ax.pie(cat_completion.values, labels=labels_cat,\n",
                "                                         autopct='%1.1f%%', colors=colors_cat, startangle=90,\n",
                "                                         wedgeprops=dict(edgecolor='white', linewidth=2))\n",
                "        ax.set_title(f'{category}\\n({len(cat_data)} tasks)', fontweight='bold', pad=20)\n",
                "        for autotext in autotexts:\n",
                "            autotext.set_color('white')\n",
                "            autotext.set_fontweight('bold')\n",
                "    \n",
                "    # If there are remaining categories, show them in the last subplot\n",
                "    if len(categories) > 4:\n",
                "        ax_last = axes[1, 2]\n",
                "        remaining_categories = categories[4:]\n",
                "        remaining_data = df[df['task_category'].isin(remaining_categories)]\n",
                "        remaining_completion = remaining_data['subtask_completed'].value_counts()\n",
                "        \n",
                "        colors_remaining = [LIGHT_COLORS['error'][2], LIGHT_COLORS['success'][2]]\n",
                "        labels_remaining = ['Failed', 'Completed']\n",
                "        wedges, texts, autotexts = ax_last.pie(remaining_completion.values, labels=labels_remaining,\n",
                "                                              autopct='%1.1f%%', colors=colors_remaining, startangle=90,\n",
                "                                              wedgeprops=dict(edgecolor='white', linewidth=2))\n",
                "        ax_last.set_title(f'Other Categories\\n({len(remaining_data)} tasks)', fontweight='bold', pad=20)\n",
                "        for autotext in autotexts:\n",
                "            autotext.set_color('white')\n",
                "            autotext.set_fontweight('bold')\n",
                "    \n",
                "    # Hide empty subplots\n",
                "    for i in range(len(categories) + 1, 6):\n",
                "        row = i // 3\n",
                "        col = i % 3\n",
                "        if row < 2 and col < 3:\n",
                "            axes[row, col].set_visible(False)\n",
                "    \n",
                "    return fig\n",
                "\n",
                "# Generate completion rate pie charts\n",
                "pie_charts_fig = create_completion_pie_charts(df)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîç Multi-Dimensional Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_multi_dimensional(df):\n",
                "    \"\"\"\n",
                "    Perform detailed multi-dimensional analysis with cross-tabulation and correlations\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üîç DETAILED MULTI-DIMENSIONAL ANALYSIS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # 1. By Agent Type\n",
                "    print(\"\\nü§ñ 1. PERFORMANCE BY AGENT TYPE\")\n",
                "    print(\"-\" * 40)\n",
                "    agent_stats = df.groupby('agent_type').agg({\n",
                "        'subtask_completed': ['count', 'sum', 'mean'],\n",
                "        'total_steps': ['mean', 'std'],\n",
                "        'duration_seconds': ['mean', 'std'],\n",
                "        'command_success_rate_numeric': ['mean', 'std'],\n",
                "        'llm_interactions': ['mean', 'std']\n",
                "    }).round(2)\n",
                "    \n",
                "    agent_stats.columns = ['_'.join(col).strip() for col in agent_stats.columns]\n",
                "    agent_summary = agent_stats[['subtask_completed_count', 'subtask_completed_sum', 'subtask_completed_mean',\n",
                "                                'total_steps_mean', 'duration_seconds_mean', 'command_success_rate_numeric_mean']]\n",
                "    agent_summary.columns = ['Total_Tasks', 'Completed', 'Completion_Rate', \n",
                "                            'Avg_Steps', 'Avg_Duration', 'Avg_Cmd_Success']\n",
                "    agent_summary['Completion_Rate'] = (agent_summary['Completion_Rate'] * 100).round(1)\n",
                "    print(agent_summary)\n",
                "    \n",
                "    # 2. Cross-tabulation: Category vs Agent Type\n",
                "    print(\"\\nüîÑ 2. CROSS-ANALYSIS: COMPLETION RATE BY CATEGORY & AGENT TYPE\")\n",
                "    print(\"-\" * 60)\n",
                "    cross_completion = pd.crosstab(df['task_category'], df['agent_type'], \n",
                "                                  values=df['subtask_completed'], aggfunc='mean') * 100\n",
                "    cross_completion = cross_completion.round(1)\n",
                "    print(cross_completion)\n",
                "    \n",
                "    # 3. Performance by Status\n",
                "    print(\"\\nüìà 3. PERFORMANCE BY EXECUTION STATUS\")\n",
                "    print(\"-\" * 40)\n",
                "    status_stats = df.groupby('status').agg({\n",
                "        'subtask_completed': ['count', 'sum', 'mean'],\n",
                "        'total_steps': 'mean',\n",
                "        'duration_seconds': 'mean',\n",
                "        'command_success_rate_numeric': 'mean'\n",
                "    }).round(2)\n",
                "    \n",
                "    status_stats.columns = ['_'.join(col).strip() for col in status_stats.columns]\n",
                "    status_summary = status_stats[['subtask_completed_count', 'subtask_completed_sum', 'subtask_completed_mean',\n",
                "                                  'total_steps_mean', 'duration_seconds_mean', 'command_success_rate_numeric_mean']]\n",
                "    status_summary.columns = ['Total_Tasks', 'Completed', 'Completion_Rate', \n",
                "                             'Avg_Steps', 'Avg_Duration', 'Avg_Cmd_Success']\n",
                "    status_summary['Completion_Rate'] = (status_summary['Completion_Rate'] * 100).round(1)\n",
                "    print(status_summary)\n",
                "    \n",
                "    # Create multi-dimensional visualization dashboard\n",
                "    fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
                "    fig.patch.set_facecolor('white')\n",
                "    fig.suptitle('üîç Multi-Dimensional Performance Analysis', fontsize=20, fontweight='bold', y=0.98)\n",
                "    \n",
                "    # 1. Cross-tabulation Heatmap\n",
                "    ax1 = axes[0, 0]\n",
                "    sns.heatmap(cross_completion, annot=True, fmt='.1f', cmap='RdYlGn', \n",
                "                cbar_kws={'label': 'Completion Rate (%)'}, ax=ax1,\n",
                "                linewidths=1, linecolor='white')\n",
                "    ax1.set_title('Completion Rate: Category vs Agent Type', fontweight='bold', pad=20)\n",
                "    ax1.set_xlabel('Agent Type')\n",
                "    ax1.set_ylabel('Task Category')\n",
                "    \n",
                "    # 2. Agent Type Performance Comparison\n",
                "    ax2 = axes[0, 1]\n",
                "    agent_types = agent_summary.index\n",
                "    x_pos = np.arange(len(agent_types))\n",
                "    bars2 = ax2.bar(x_pos, agent_summary['Completion_Rate'], \n",
                "                    color=LIGHT_COLORS['success'][:len(agent_types)],\n",
                "                    edgecolor='white', linewidth=2)\n",
                "    ax2.set_title('Completion Rate by Agent Type', fontweight='bold', pad=20)\n",
                "    ax2.set_ylabel('Completion Rate (%)')\n",
                "    ax2.set_xticks(x_pos)\n",
                "    ax2.set_xticklabels(agent_types, rotation=45)\n",
                "    ax2.set_ylim(0, 105)\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    for bar in bars2:\n",
                "        height = bar.get_height()\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
                "                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 3. Efficiency Bubble Chart\n",
                "    ax3 = axes[0, 2]\n",
                "    categories = df['task_category'].unique()\n",
                "    colors = LIGHT_COLORS['primary'][:len(categories)]\n",
                "    for i, category in enumerate(categories):\n",
                "        cat_data = df[df['task_category'] == category]\n",
                "        ax3.scatter(cat_data['total_steps'], cat_data['command_success_rate_numeric'],\n",
                "                   s=cat_data['duration_seconds']*3, alpha=0.6, color=colors[i],\n",
                "                   label=category, edgecolors='white', linewidth=1)\n",
                "    ax3.set_title('Efficiency Analysis\\n(Size = Duration)', fontweight='bold', pad=20)\n",
                "    ax3.set_xlabel('Total Steps')\n",
                "    ax3.set_ylabel('Command Success Rate (%)')\n",
                "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "    ax3.grid(True, alpha=0.3)\n",
                "    \n",
                "    return fig, {\n",
                "        'agent_stats': agent_summary,\n",
                "        'cross_completion': cross_completion,\n",
                "        'status_stats': status_summary\n",
                "    }\n",
                "\n",
                "# Generate multi-dimensional analysis\n",
                "multi_dim_fig, multi_dim_stats = analyze_multi_dimensional(df)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚ö†Ô∏è Bad Case Analysis & Failure Patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_bad_cases(df):\n",
                "    \"\"\"\n",
                "    Analyze failed tasks and identify failure patterns\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"‚ö†Ô∏è BAD CASE ANALYSIS & FAILURE PATTERNS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Identify failed tasks\n",
                "    failed_tasks = df[df['subtask_completed'] == False]\n",
                "    successful_tasks = df[df['subtask_completed'] == True]\n",
                "    \n",
                "    print(f\"‚ùå Total Failed Tasks: {len(failed_tasks)} ({len(failed_tasks)/len(df)*100:.1f}%)\")\n",
                "    print(f\"‚úÖ Total Successful Tasks: {len(successful_tasks)} ({len(successful_tasks)/len(df)*100:.1f}%)\")\n",
                "    \n",
                "    if len(failed_tasks) == 0:\n",
                "        print(\"üéâ No failed tasks found! All tasks completed successfully.\")\n",
                "        return None, None\n",
                "    \n",
                "    # Failure analysis by category\n",
                "    print(\"\\nüìä FAILURE ANALYSIS BY CATEGORY\")\n",
                "    print(\"-\" * 40)\n",
                "    failure_by_category = failed_tasks['task_category'].value_counts()\n",
                "    total_by_category = df['task_category'].value_counts()\n",
                "    failure_rate_by_category = (failure_by_category / total_by_category * 100).fillna(0).round(1)\n",
                "    \n",
                "    failure_analysis = pd.DataFrame({\n",
                "        'Total_Tasks': total_by_category,\n",
                "        'Failed_Tasks': failure_by_category.fillna(0).astype(int),\n",
                "        'Failure_Rate': failure_rate_by_category\n",
                "    }).sort_values('Failure_Rate', ascending=False)\n",
                "    \n",
                "    print(failure_analysis)\n",
                "    \n",
                "    # Compare failed vs successful tasks\n",
                "    print(\"\\nüîç FAILED vs SUCCESSFUL TASKS COMPARISON\")\n",
                "    print(\"-\" * 50)\n",
                "    comparison = pd.DataFrame({\n",
                "        'Failed_Tasks': [\n",
                "            failed_tasks['total_steps'].mean(),\n",
                "            failed_tasks['duration_seconds'].mean(),\n",
                "            failed_tasks['command_success_rate_numeric'].mean(),\n",
                "            failed_tasks['llm_interactions'].mean()\n",
                "        ],\n",
                "        'Successful_Tasks': [\n",
                "            successful_tasks['total_steps'].mean(),\n",
                "            successful_tasks['duration_seconds'].mean(),\n",
                "            successful_tasks['command_success_rate_numeric'].mean(),\n",
                "            successful_tasks['llm_interactions'].mean()\n",
                "        ]\n",
                "    }, index=['Avg_Steps', 'Avg_Duration', 'Avg_Cmd_Success', 'Avg_LLM_Interactions']).round(2)\n",
                "    \n",
                "    comparison['Difference'] = (comparison['Failed_Tasks'] - comparison['Successful_Tasks']).round(2)\n",
                "    print(comparison)\n",
                "    \n",
                "    # Create failure analysis visualizations\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "    fig.patch.set_facecolor('white')\n",
                "    fig.suptitle('‚ö†Ô∏è Failure Pattern Analysis', fontsize=18, fontweight='bold', y=0.98)\n",
                "    \n",
                "    # 1. Failure Rate by Category\n",
                "    ax1 = axes[0, 0]\n",
                "    bars1 = ax1.bar(failure_analysis.index, failure_analysis['Failure_Rate'],\n",
                "                    color=LIGHT_COLORS['error'][:len(failure_analysis)],\n",
                "                    edgecolor='white', linewidth=2)\n",
                "    ax1.set_title('Failure Rate by Category', fontweight='bold', pad=20)\n",
                "    ax1.set_ylabel('Failure Rate (%)')\n",
                "    ax1.tick_params(axis='x', rotation=45)\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    for bar in bars1:\n",
                "        height = bar.get_height()\n",
                "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
                "                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 2. Failed vs Successful Comparison\n",
                "    ax2 = axes[0, 1]\n",
                "    x_pos = np.arange(len(comparison.index))\n",
                "    width = 0.35\n",
                "    bars2a = ax2.bar(x_pos - width/2, comparison['Failed_Tasks'], width, \n",
                "                     label='Failed Tasks', color=LIGHT_COLORS['error'][2],\n",
                "                     edgecolor='white', linewidth=1.5)\n",
                "    bars2b = ax2.bar(x_pos + width/2, comparison['Successful_Tasks'], width,\n",
                "                     label='Successful Tasks', color=LIGHT_COLORS['success'][2],\n",
                "                     edgecolor='white', linewidth=1.5)\n",
                "    ax2.set_title('Failed vs Successful Tasks Metrics', fontweight='bold', pad=20)\n",
                "    ax2.set_ylabel('Average Value')\n",
                "    ax2.set_xticks(x_pos)\n",
                "    ax2.set_xticklabels(comparison.index, rotation=45)\n",
                "    ax2.legend()\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    \n",
                "    # 3. Command Success Rate Distribution\n",
                "    ax3 = axes[1, 0]\n",
                "    ax3.hist(failed_tasks['command_success_rate_numeric'], bins=15, alpha=0.7, \n",
                "             label='Failed Tasks', color=LIGHT_COLORS['error'][2], edgecolor='white')\n",
                "    ax3.hist(successful_tasks['command_success_rate_numeric'], bins=15, alpha=0.7,\n",
                "             label='Successful Tasks', color=LIGHT_COLORS['success'][2], edgecolor='white')\n",
                "    ax3.set_title('Command Success Rate Distribution', fontweight='bold', pad=20)\n",
                "    ax3.set_xlabel('Command Success Rate (%)')\n",
                "    ax3.set_ylabel('Frequency')\n",
                "    ax3.legend()\n",
                "    ax3.grid(True, alpha=0.3)\n",
                "    \n",
                "    # 4. Duration vs Steps Scatter\n",
                "    ax4 = axes[1, 1]\n",
                "    ax4.scatter(failed_tasks['total_steps'], failed_tasks['duration_seconds'],\n",
                "               alpha=0.7, color=LIGHT_COLORS['error'][2], label='Failed Tasks',\n",
                "               s=60, edgecolors='white', linewidth=1)\n",
                "    ax4.scatter(successful_tasks['total_steps'], successful_tasks['duration_seconds'],\n",
                "               alpha=0.7, color=LIGHT_COLORS['success'][2], label='Successful Tasks',\n",
                "               s=60, edgecolors='white', linewidth=1)\n",
                "    ax4.set_title('Duration vs Steps Analysis', fontweight='bold', pad=20)\n",
                "    ax4.set_xlabel('Total Steps')\n",
                "    ax4.set_ylabel('Duration (seconds)')\n",
                "    ax4.legend()\n",
                "    ax4.grid(True, alpha=0.3)\n",
                "    \n",
                "    return fig, {\n",
                "        'failure_analysis': failure_analysis,\n",
                "        'comparison': comparison,\n",
                "        'failed_tasks': failed_tasks,\n",
                "        'successful_tasks': successful_tasks\n",
                "    }\n",
                "\n",
                "# Generate bad case analysis\n",
                "bad_case_fig, bad_case_stats = analyze_bad_cases(df)\n",
                "if bad_case_fig is not None:\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üé® Interactive Visualizations with Plotly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_interactive_dashboard(df):\n",
                "    \"\"\"\n",
                "    Create interactive visualizations using Plotly\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üé® INTERACTIVE VISUALIZATIONS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # 1. Interactive Completion Rate by Category\n",
                "    category_completion = df.groupby('task_category')['subtask_completed'].agg(['count', 'sum', 'mean']).reset_index()\n",
                "    category_completion['completion_rate'] = category_completion['mean'] * 100\n",
                "    \n",
                "    fig1 = px.bar(category_completion, x='task_category', y='completion_rate',\n",
                "                  title='üìä Interactive Completion Rate by Task Category',\n",
                "                  labels={'completion_rate': 'Completion Rate (%)', 'task_category': 'Task Category'},\n",
                "                  color='completion_rate',\n",
                "                  color_continuous_scale='RdYlGn',\n",
                "                  hover_data={'count': True, 'sum': True})\n",
                "    fig1.update_layout(height=500, showlegend=False, \n",
                "                       font=dict(size=12),\n",
                "                       plot_bgcolor='white',\n",
                "                       paper_bgcolor='white')\n",
                "    fig1.show()\n",
                "    \n",
                "    # 2. Interactive Scatter Plot: Steps vs Success Rate\n",
                "    fig2 = px.scatter(df, x='total_steps', y='command_success_rate_numeric',\n",
                "                      color='task_category', size='duration_seconds',\n",
                "                      hover_data=['scenario_id', 'agent_type', 'subtask_completed'],\n",
                "                      title='üîç Interactive Analysis: Steps vs Command Success Rate',\n",
                "                      labels={'command_success_rate_numeric': 'Command Success Rate (%)',\n",
                "                             'total_steps': 'Total Steps'})\n",
                "    fig2.update_layout(height=600,\n",
                "                       font=dict(size=12),\n",
                "                       plot_bgcolor='white',\n",
                "                       paper_bgcolor='white')\n",
                "    fig2.show()\n",
                "    \n",
                "    # 3. Interactive Heatmap: Category vs Agent Type\n",
                "    if len(df['agent_type'].unique()) > 1:\n",
                "        cross_completion = pd.crosstab(df['task_category'], df['agent_type'], \n",
                "                                      values=df['subtask_completed'], aggfunc='mean') * 100\n",
                "        \n",
                "        fig3 = px.imshow(cross_completion.values,\n",
                "                         x=cross_completion.columns,\n",
                "                         y=cross_completion.index,\n",
                "                         color_continuous_scale='RdYlGn',\n",
                "                         title='üîÑ Interactive Cross-Analysis: Category vs Agent Type',\n",
                "                         labels=dict(x=\"Agent Type\", y=\"Task Category\", color=\"Completion Rate (%)\"))\n",
                "        fig3.update_layout(height=500,\n",
                "                           font=dict(size=12),\n",
                "                           plot_bgcolor='white',\n",
                "                           paper_bgcolor='white')\n",
                "        fig3.show()\n",
                "    \n",
                "    # 4. Interactive Timeline Analysis\n",
                "    df_timeline = df.copy()\n",
                "    df_timeline['date'] = df_timeline['timestamp'].dt.date\n",
                "    timeline_stats = df_timeline.groupby('date').agg({\n",
                "        'subtask_completed': ['count', 'sum', 'mean']\n",
                "    }).reset_index()\n",
                "    timeline_stats.columns = ['date', 'total_tasks', 'completed_tasks', 'completion_rate']\n",
                "    timeline_stats['completion_rate'] = timeline_stats['completion_rate'] * 100\n",
                "    \n",
                "    fig4 = px.line(timeline_stats, x='date', y='completion_rate',\n",
                "                   title='üìà Interactive Timeline: Daily Completion Rate Trend',\n",
                "                   labels={'completion_rate': 'Completion Rate (%)', 'date': 'Date'},\n",
                "                   markers=True)\n",
                "    fig4.add_scatter(x=timeline_stats['date'], y=timeline_stats['total_tasks'],\n",
                "                     mode='markers', name='Total Tasks', yaxis='y2',\n",
                "                     marker=dict(size=8, color='orange'))\n",
                "    fig4.update_layout(height=500,\n",
                "                       font=dict(size=12),\n",
                "                       plot_bgcolor='white',\n",
                "                       paper_bgcolor='white',\n",
                "                       yaxis2=dict(title='Total Tasks', overlaying='y', side='right'))\n",
                "    fig4.show()\n",
                "    \n",
                "    print(\"‚úÖ Interactive visualizations generated successfully!\")\n",
                "    print(\"üí° Hover over data points for detailed information\")\n",
                "    print(\"üîç Use zoom, pan, and selection tools for detailed exploration\")\n",
                "\n",
                "# Generate interactive visualizations\n",
                "create_interactive_dashboard(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üí° Intelligent Insights & Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_intelligent_insights(df, category_summary, multi_dim_stats, bad_case_stats):\n",
                "    \"\"\"\n",
                "    Generate AI-powered insights and recommendations based on the analysis\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üí° INTELLIGENT INSIGHTS & RECOMMENDATIONS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    insights = []\n",
                "    recommendations = []\n",
                "    \n",
                "    # Overall performance insights\n",
                "    total_completion_rate = df['subtask_completed'].mean() * 100\n",
                "    avg_cmd_success = df['command_success_rate_numeric'].mean()\n",
                "    \n",
                "    if total_completion_rate >= 90:\n",
                "        insights.append(f\"üéâ Excellent overall performance with {total_completion_rate:.1f}% completion rate!\")\n",
                "    elif total_completion_rate >= 70:\n",
                "        insights.append(f\"üëç Good overall performance with {total_completion_rate:.1f}% completion rate.\")\n",
                "        recommendations.append(\"üéØ Focus on optimizing the remaining failure cases to reach excellence.\")\n",
                "    else:\n",
                "        insights.append(f\"‚ö†Ô∏è Performance needs improvement with {total_completion_rate:.1f}% completion rate.\")\n",
                "        recommendations.append(\"üö® Urgent attention needed to identify and fix major issues.\")\n",
                "    \n",
                "    # Category-specific insights\n",
                "    best_category = category_summary['Completion_Rate'].idxmax()\n",
                "    worst_category = category_summary['Completion_Rate'].idxmin()\n",
                "    best_rate = category_summary.loc[best_category, 'Completion_Rate']\n",
                "    worst_rate = category_summary.loc[worst_category, 'Completion_Rate']\n",
                "    \n",
                "    insights.append(f\"üèÜ '{best_category}' tasks perform best ({best_rate:.1f}% completion).\")\n",
                "    if worst_rate < 80:\n",
                "        insights.append(f\"‚ö†Ô∏è '{worst_category}' tasks need attention ({worst_rate:.1f}% completion).\")\n",
                "        recommendations.append(f\"üîß Investigate and optimize '{worst_category}' task execution logic.\")\n",
                "    \n",
                "    # Command success vs task completion correlation\n",
                "    correlation = df['command_success_rate_numeric'].corr(df['subtask_completed'].astype(int))\n",
                "    if correlation > 0.7:\n",
                "        insights.append(f\"üìà Strong correlation ({correlation:.2f}) between command success and task completion.\")\n",
                "        recommendations.append(\"üí° Focus on improving command execution quality for better overall performance.\")\n",
                "    \n",
                "    # Duration and efficiency insights\n",
                "    avg_duration = df['duration_seconds'].mean()\n",
                "    if avg_duration > 30:\n",
                "        insights.append(f\"‚è±Ô∏è Tasks take relatively long on average ({avg_duration:.1f}s).\")\n",
                "        recommendations.append(\"‚ö° Consider optimizing task execution speed and reducing unnecessary steps.\")\n",
                "    \n",
                "    # Steps efficiency\n",
                "    avg_steps = df['total_steps'].mean()\n",
                "    successful_avg_steps = df[df['subtask_completed'] == True]['total_steps'].mean()\n",
                "    failed_avg_steps = df[df['subtask_completed'] == False]['total_steps'].mean()\n",
                "    \n",
                "    if not pd.isna(failed_avg_steps) and failed_avg_steps > successful_avg_steps * 1.2:\n",
                "        insights.append(f\"üîç Failed tasks use more steps on average ({failed_avg_steps:.1f} vs {successful_avg_steps:.1f}).\")\n",
                "        recommendations.append(\"üéØ Implement early failure detection to avoid unnecessary step execution.\")\n",
                "    \n",
                "    # Agent type insights (if multiple types exist)\n",
                "    if len(df['agent_type'].unique()) > 1:\n",
                "        agent_performance = multi_dim_stats['agent_stats']['Completion_Rate']\n",
                "        best_agent = agent_performance.idxmax()\n",
                "        worst_agent = agent_performance.idxmin()\n",
                "        insights.append(f\"ü§ñ '{best_agent}' agents outperform '{worst_agent}' agents.\")\n",
                "        recommendations.append(f\"üìä Analyze '{best_agent}' agent strategies for potential improvements to '{worst_agent}' agents.\")\n",
                "    \n",
                "    # Print insights and recommendations\n",
                "    print(\"\\nüîç KEY INSIGHTS:\")\n",
                "    for i, insight in enumerate(insights, 1):\n",
                "        print(f\"{i}. {insight}\")\n",
                "    \n",
                "    print(\"\\nüéØ RECOMMENDATIONS:\")\n",
                "    for i, rec in enumerate(recommendations, 1):\n",
                "        print(f\"{i}. {rec}\")\n",
                "    \n",
                "    # Performance score calculation\n",
                "    performance_score = (\n",
                "        total_completion_rate * 0.4 +  # 40% weight on completion\n",
                "        avg_cmd_success * 0.3 +        # 30% weight on command success\n",
                "        (100 - min(avg_duration/60*10, 50)) * 0.2 +  # 20% weight on speed (capped)\n",
                "        (100 - min(avg_steps/20*10, 50)) * 0.1       # 10% weight on efficiency (capped)\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nüìä OVERALL PERFORMANCE SCORE: {performance_score:.1f}/100\")\n",
                "    \n",
                "    if performance_score >= 85:\n",
                "        print(\"üåü EXCELLENT - System is performing at a high level!\")\n",
                "    elif performance_score >= 70:\n",
                "        print(\"üëç GOOD - System is performing well with room for optimization.\")\n",
                "    elif performance_score >= 50:\n",
                "        print(\"‚ö†Ô∏è FAIR - System needs improvement in several areas.\")\n",
                "    else:\n",
                "        print(\"üö® POOR - System requires immediate attention and optimization.\")\n",
                "    \n",
                "    return {\n",
                "        'insights': insights,\n",
                "        'recommendations': recommendations,\n",
                "        'performance_score': performance_score\n",
                "    }\n",
                "\n",
                "# Generate intelligent insights\n",
                "if bad_case_stats is not None:\n",
                "    insights_data = generate_intelligent_insights(df, category_summary, multi_dim_stats, bad_case_stats)\n",
                "else:\n",
                "    insights_data = generate_intelligent_insights(df, category_summary, multi_dim_stats, {})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã Executive Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_executive_summary(df, insights_data):\n",
                "    \"\"\"\n",
                "    Generate a comprehensive executive summary report\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"üìã EXECUTIVE SUMMARY REPORT\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Header information\n",
                "    print(f\"üìÖ Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "    print(f\"üìä Analysis Period: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}\")\n",
                "    print(f\"üìà Total Records Analyzed: {len(df):,}\")\n",
                "    print(f\"üèÜ Overall Performance Score: {insights_data['performance_score']:.1f}/100\")\n",
                "    \n",
                "    # Key metrics summary\n",
                "    print(\"\\n\" + \"-\"*50)\n",
                "    print(\"üìä KEY PERFORMANCE METRICS\")\n",
                "    print(\"-\"*50)\n",
                "    \n",
                "    total_completion_rate = df['subtask_completed'].mean() * 100\n",
                "    execution_rate = df['task_executed'].mean() * 100\n",
                "    avg_cmd_success = df['command_success_rate_numeric'].mean()\n",
                "    avg_duration = df['duration_seconds'].mean()\n",
                "    avg_steps = df['total_steps'].mean()\n",
                "    avg_llm_interactions = df['llm_interactions'].mean()\n",
                "    \n",
                "    metrics_table = pd.DataFrame({\n",
                "        'Metric': [\n",
                "            'Task Completion Rate',\n",
                "            'Task Execution Rate', \n",
                "            'Command Success Rate',\n",
                "            'Average Duration',\n",
                "            'Average Steps per Task',\n",
                "            'Average LLM Interactions'\n",
                "        ],\n",
                "        'Value': [\n",
                "            f\"{total_completion_rate:.1f}%\",\n",
                "            f\"{execution_rate:.1f}%\",\n",
                "            f\"{avg_cmd_success:.1f}%\",\n",
                "            f\"{avg_duration:.1f} seconds\",\n",
                "            f\"{avg_steps:.1f}\",\n",
                "            f\"{avg_llm_interactions:.1f}\"\n",
                "        ],\n",
                "        'Status': [\n",
                "            'üü¢ Excellent' if total_completion_rate >= 90 else 'üü° Good' if total_completion_rate >= 70 else 'üî¥ Needs Improvement',\n",
                "            'üü¢ Excellent' if execution_rate >= 95 else 'üü° Good' if execution_rate >= 85 else 'üî¥ Needs Improvement',\n",
                "            'üü¢ Excellent' if avg_cmd_success >= 90 else 'üü° Good' if avg_cmd_success >= 75 else 'üî¥ Needs Improvement',\n",
                "            'üü¢ Fast' if avg_duration <= 15 else 'üü° Moderate' if avg_duration <= 30 else 'üî¥ Slow',\n",
                "            'üü¢ Efficient' if avg_steps <= 10 else 'üü° Moderate' if avg_steps <= 20 else 'üî¥ Inefficient',\n",
                "            'üü¢ Efficient' if avg_llm_interactions <= 10 else 'üü° Moderate' if avg_llm_interactions <= 20 else 'üî¥ High Usage'\n",
                "        ]\n",
                "    })\n",
                "    \n",
                "    print(metrics_table.to_string(index=False))\n",
                "    \n",
                "    # Category performance summary\n",
                "    print(\"\\n\" + \"-\"*50)\n",
                "    print(\"üìã CATEGORY PERFORMANCE SUMMARY\")\n",
                "    print(\"-\"*50)\n",
                "    \n",
                "    category_performance = df.groupby('task_category').agg({\n",
                "        'subtask_completed': ['count', 'mean']\n",
                "    }).round(3)\n",
                "    category_performance.columns = ['Total_Tasks', 'Completion_Rate']\n",
                "    category_performance['Completion_Rate'] = (category_performance['Completion_Rate'] * 100).round(1)\n",
                "    category_performance = category_performance.sort_values('Completion_Rate', ascending=False)\n",
                "    \n",
                "    for category, row in category_performance.iterrows():\n",
                "        status_icon = 'üü¢' if row['Completion_Rate'] >= 90 else 'üü°' if row['Completion_Rate'] >= 70 else 'üî¥'\n",
                "        print(f\"{status_icon} {category}: {row['Completion_Rate']:.1f}% ({int(row['Total_Tasks'])} tasks)\")\n",
                "    \n",
                "    # Top insights and recommendations\n",
                "    print(\"\\n\" + \"-\"*50)\n",
                "    print(\"üí° TOP INSIGHTS & RECOMMENDATIONS\")\n",
                "    print(\"-\"*50)\n",
                "    \n",
                "    print(\"\\nüîç Key Insights:\")\n",
                "    for insight in insights_data['insights'][:3]:  # Top 3 insights\n",
                "        print(f\"  ‚Ä¢ {insight}\")\n",
                "    \n",
                "    print(\"\\nüéØ Priority Recommendations:\")\n",
                "    for rec in insights_data['recommendations'][:3]:  # Top 3 recommendations\n",
                "        print(f\"  ‚Ä¢ {rec}\")\n",
                "    \n",
                "    # Footer\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"üìä End of Executive Summary Report\")\n",
                "    print(\"üîÑ For detailed analysis, refer to the visualizations above\")\n",
                "    print(\"üìß Contact the development team for further optimization strategies\")\n",
                "    print(\"=\"*80)\n",
                "\n",
                "# Generate executive summary\n",
                "generate_executive_summary(df, insights_data)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "RL",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
