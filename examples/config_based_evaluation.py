#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºé…ç½®æ–‡ä»¶çš„è¯„æµ‹ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å‚æ•°è¿›è¡Œè¯„æµ‹ï¼Œå‡å°‘å‘½ä»¤è¡Œå‚æ•°çš„ä½¿ç”¨
"""

import os
import sys
import argparse
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from utils.task_evaluator import TaskEvaluator
from config import ConfigManager


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - åªä¿ç•™å¿…è¦çš„å‚æ•°ï¼Œå…¶ä»–ä»é…ç½®æ–‡ä»¶è¯»å–"""
    parser = argparse.ArgumentParser(
        description='åŸºäºé…ç½®æ–‡ä»¶çš„è¯„æµ‹ç¤ºä¾‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨è¯´æ˜:
  å¤§éƒ¨åˆ†å‚æ•°éƒ½å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼Œå‘½ä»¤è¡Œå‚æ•°åªç”¨äºè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å€¼ã€‚
  
é…ç½®æ–‡ä»¶ä¸­å¯è®¾ç½®çš„å‚æ•°:
  - evaluation.task_type: è¯„æµ‹æ¨¡å¼ (sequential/combined/independent)
  - evaluation.default_scenario: é»˜è®¤åœºæ™¯ID
  - evaluation.run_settings.default_suffix: é»˜è®¤è¿è¡Œåç¼€
  - evaluation.run_settings.log_level: æ—¥å¿—çº§åˆ«
  - task_evaluator.max_steps_per_task: æ¯ä¸ªå­ä»»åŠ¡æœ€å¤§æ­¥æ•°
  - execution.max_total_steps: æ€»æœ€å¤§æ­¥æ•°

ç¤ºä¾‹:
  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰é»˜è®¤å€¼
  python examples/config_based_evaluation.py --config single_agent_config
  
  # åªè¦†ç›–åœºæ™¯IDï¼Œå…¶ä»–ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼
  python examples/config_based_evaluation.py --config single_agent_config --scenario 00002
  
  # è¦†ç›–è¯„æµ‹æ¨¡å¼å’Œåç¼€
  python examples/config_based_evaluation.py --config single_agent_config --mode independent --suffix my_test
        """
    )
    
    parser.add_argument(
        '--config', '-c',
        type=str,
        required=True,
        help='é…ç½®æ–‡ä»¶å (å¿…éœ€ï¼Œå¦‚: single_agent_config, centralized_config)'
    )
    
    parser.add_argument(
        '--mode', '-m',
        choices=['sequential', 'combined', 'independent'],
        help='è¯„æµ‹æ¨¡å¼ (å¯é€‰ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)'
    )
    
    parser.add_argument(
        '--scenario', '-s',
        type=str,
        help='åœºæ™¯ID (å¯é€‰ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)'
    )
    
    parser.add_argument(
        '--suffix',
        type=str,
        help='è¿è¡Œåç¼€ (å¯é€‰ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å¹²è¿è¡Œæ¨¡å¼ï¼Œåªæ£€æŸ¥é…ç½®ä¸æ‰§è¡Œè¯„æµ‹'
    )
    
    return parser.parse_args()


def determine_agent_type(config: dict) -> str:
    """æ ¹æ®é…ç½®æ–‡ä»¶å†…å®¹ç¡®å®šæ™ºèƒ½ä½“ç±»å‹"""
    if 'coordinator' in config or 'worker_agents' in config:
        return 'multi'
    elif 'autonomous_agent' in config or 'communication' in config:
        return 'multi'
    else:
        return 'single'


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # åŠ è½½é…ç½®
    config_manager = ConfigManager()
    config = config_manager.get_config(args.config)
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–é»˜è®¤å€¼
    eval_config = config.get('evaluation', {})
    run_settings = eval_config.get('run_settings', {})
    
    # ç¡®å®šå‚æ•°å€¼ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
    task_type = args.mode or eval_config.get('task_type', 'sequential')
    scenario_id = args.scenario or eval_config.get('default_scenario', '00001')
    custom_suffix = args.suffix or run_settings.get('default_suffix', 'demo')
    log_level = run_settings.get('log_level', 'INFO')
    
    # ç¡®å®šæ™ºèƒ½ä½“ç±»å‹
    agent_type = determine_agent_type(config)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("ğŸ“‹ è¯„æµ‹é…ç½®ä¿¡æ¯:")
    print(f"   é…ç½®æ–‡ä»¶: {args.config}")
    print(f"   æ™ºèƒ½ä½“ç±»å‹: {agent_type}")
    print(f"   è¯„æµ‹æ¨¡å¼: {task_type}")
    print(f"   åœºæ™¯ID: {scenario_id}")
    print(f"   è¿è¡Œåç¼€: {custom_suffix}")
    print(f"   æ—¥å¿—çº§åˆ«: {log_level}")
    print()
    
    # æ˜¾ç¤ºä»é…ç½®æ–‡ä»¶è¯»å–çš„å…¶ä»–é‡è¦å‚æ•°
    max_steps_per_task = config.get('task_evaluator', {}).get('max_steps_per_task', 30)
    max_total_steps = config.get('execution', {}).get('max_total_steps', 300)
    print("ğŸ“Š æ‰§è¡Œå‚æ•°:")
    print(f"   æ¯ä¸ªå­ä»»åŠ¡æœ€å¤§æ­¥æ•°: {max_steps_per_task}")
    print(f"   æ€»æœ€å¤§æ­¥æ•°: {max_total_steps}")
    print()
    
    # å¹²è¿è¡Œæ¨¡å¼
    if args.dry_run:
        print("âœ… å¹²è¿è¡Œå®Œæˆ - é…ç½®éªŒè¯é€šè¿‡")
        return 0
    
    try:
        # åˆ›å»ºè¯„æµ‹å™¨ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ï¼‰
        logger.info(f"ğŸš€ å¯åŠ¨åŸºäºé…ç½®æ–‡ä»¶çš„è¯„æµ‹å™¨")
        evaluator = TaskEvaluator(
            config_file=args.config,
            agent_type=agent_type,
            task_type=task_type,
            scenario_id=scenario_id,
            custom_suffix=custom_suffix
        )
        
        # è¿è¡Œè¯„æµ‹
        results = evaluator.run()
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print("\nğŸ“Š è¯„æµ‹å®Œæˆ!")
        print(f"   è¿è¡Œåç§°: {evaluator.run_name}")
        print(f"   è¾“å‡ºç›®å½•: {evaluator.output_dir}")
        print(f"   å®Œæˆç‡: {results['summary']['completion_rate']:.1%}")
        print(f"   æ€»æ­¥æ•°: {results['summary']['total_steps']}")
        
        return 0
        
    except Exception as e:
        logger.exception(f"âŒ è¯„æµ‹æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
