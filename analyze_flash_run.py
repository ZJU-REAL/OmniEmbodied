#!/usr/bin/env python3
"""
åˆ†æ Gemini 2.5 Flash æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ä»»åŠ¡ä¸Šçš„è¡¨ç°
ç‰¹åˆ«å…³æ³¨ compound_collaboration ä»»åŠ¡çš„å¤±è´¥æƒ…å†µ
"""

import pandas as pd
import json
import os
from collections import defaultdict, Counter

def load_csv_data():
    """åŠ è½½CSVæ•°æ®"""
    csv_path = "output/20250724_183034_multi_independent_00001_to_00600_qianduoduo_2.5-flash_wo_multi/subtask_execution_log.csv"
    
    if not os.path.exists(csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    print(f"âœ… æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œå…± {len(df)} æ¡è®°å½•")
    return df

def analyze_overall_performance(df):
    """åˆ†ææ•´ä½“æ€§èƒ½"""
    print(f"\nğŸ“Š æ•´ä½“æ€§èƒ½åˆ†æ:")
    print(f"æ€»ä»»åŠ¡æ•°: {len(df)}")
    
    # æŒ‰ä»»åŠ¡ç±»åˆ«ç»Ÿè®¡
    category_stats = df.groupby('task_category').agg({
        'subtask_completed': ['count', 'sum'],
        'model_claimed_done': 'sum',
        'command_success_rate': 'mean',
        'llm_interactions': 'mean',
        'duration_seconds': 'mean'
    }).round(3)
    
    print(f"\nğŸ“ˆ å„ä»»åŠ¡ç±»åˆ«è¡¨ç°:")
    for category in df['task_category'].unique():
        category_data = df[df['task_category'] == category]
        total = len(category_data)
        completed = category_data['subtask_completed'].sum()
        model_claimed = category_data['model_claimed_done'].sum()
        completion_rate = completed / total
        avg_cmd_success = category_data['command_success_rate'].mean()
        avg_interactions = category_data['llm_interactions'].mean()
        avg_duration = category_data['duration_seconds'].mean()
        
        print(f"\n{category}:")
        print(f"  æ€»æ•°: {total}")
        print(f"  å®é™…å®Œæˆ: {completed} ({completion_rate:.1%})")
        print(f"  æ¨¡å‹å£°ç§°å®Œæˆ: {model_claimed}")
        print(f"  å¹³å‡å‘½ä»¤æˆåŠŸç‡: {avg_cmd_success:.3f}")
        print(f"  å¹³å‡LLMäº¤äº’æ¬¡æ•°: {avg_interactions:.1f}")
        print(f"  å¹³å‡æ‰§è¡Œæ—¶é•¿: {avg_duration:.1f}ç§’")

def analyze_compound_collaboration_failures(df):
    """æ·±å…¥åˆ†æcompound_collaborationä»»åŠ¡çš„å¤±è´¥æƒ…å†µ"""
    compound_tasks = df[df['task_category'] == 'compound_collaboration'].copy()
    failed_tasks = compound_tasks[compound_tasks['subtask_completed'] == False].copy()
    
    print(f"\nğŸ” compound_collaboration ä»»åŠ¡å¤±è´¥åˆ†æ:")
    print(f"æ€»æ•°: {len(compound_tasks)}")
    print(f"å¤±è´¥æ•°: {len(failed_tasks)} ({len(failed_tasks)/len(compound_tasks):.1%})")
    
    # åˆ†æå¤±è´¥åŸå› 
    print(f"\nå¤±è´¥ä»»åŠ¡çŠ¶æ€åˆ†å¸ƒ:")
    status_counts = failed_tasks['status'].value_counts()
    for status, count in status_counts.items():
        print(f"  {status}: {count}")
    
    # åˆ†ææ¨¡å‹è¯¯åˆ¤æƒ…å†µ
    false_positives = failed_tasks[failed_tasks['model_claimed_done'] == True]
    print(f"\næ¨¡å‹è¯¯åˆ¤å®Œæˆçš„å¤±è´¥ä»»åŠ¡: {len(false_positives)} / {len(failed_tasks)} ({len(false_positives)/len(failed_tasks):.1%})")
    
    # åˆ†æå‘½ä»¤æˆåŠŸç‡åˆ†å¸ƒ
    print(f"\nå¤±è´¥ä»»åŠ¡å‘½ä»¤æˆåŠŸç‡ç»Ÿè®¡:")
    print(f"  å¹³å‡: {failed_tasks['command_success_rate'].mean():.3f}")
    print(f"  ä¸­ä½æ•°: {failed_tasks['command_success_rate'].median():.3f}")
    print(f"  æœ€ä½: {failed_tasks['command_success_rate'].min():.3f}")
    print(f"  æœ€é«˜: {failed_tasks['command_success_rate'].max():.3f}")
    
    # æŒ‰å‘½ä»¤æˆåŠŸç‡åˆ†ç»„
    low_success = failed_tasks[failed_tasks['command_success_rate'] < 0.3]
    medium_success = failed_tasks[(failed_tasks['command_success_rate'] >= 0.3) & (failed_tasks['command_success_rate'] < 0.7)]
    high_success = failed_tasks[failed_tasks['command_success_rate'] >= 0.7]
    
    print(f"\næŒ‰å‘½ä»¤æˆåŠŸç‡åˆ†ç»„:")
    print(f"  ä½æˆåŠŸç‡ (<30%): {len(low_success)} ä¸ªä»»åŠ¡")
    print(f"  ä¸­ç­‰æˆåŠŸç‡ (30%-70%): {len(medium_success)} ä¸ªä»»åŠ¡")
    print(f"  é«˜æˆåŠŸç‡ (>=70%): {len(high_success)} ä¸ªä»»åŠ¡")
    
    return compound_tasks, failed_tasks

def analyze_task_patterns(failed_tasks):
    """åˆ†æå¤±è´¥ä»»åŠ¡çš„æ¨¡å¼"""
    print(f"\nğŸ” å¤±è´¥ä»»åŠ¡æ¨¡å¼åˆ†æ:")
    
    # åˆ†æä»»åŠ¡æè¿°ä¸­çš„å…³é”®è¯
    task_keywords = defaultdict(int)
    for desc in failed_tasks['task_description']:
        desc_lower = desc.lower()
        if 'printer' in desc_lower:
            task_keywords['printer'] += 1
        if 'heavy' in desc_lower:
            task_keywords['heavy'] += 1
        if 'repair' in desc_lower:
            task_keywords['repair'] += 1
        if 'use' in desc_lower or 'using' in desc_lower:
            task_keywords['use_tool'] += 1
        if 'move' in desc_lower:
            task_keywords['move'] += 1
        if 'open' in desc_lower:
            task_keywords['open'] += 1
        if 'turn on' in desc_lower:
            task_keywords['turn_on'] += 1
    
    print(f"\nå¤±è´¥ä»»åŠ¡å…³é”®è¯ç»Ÿè®¡:")
    for keyword, count in sorted(task_keywords.items(), key=lambda x: x[1], reverse=True):
        print(f"  {keyword}: {count}")
    
    # åˆ†æLLMäº¤äº’æ¬¡æ•°
    print(f"\nLLMäº¤äº’æ¬¡æ•°åˆ†æ:")
    print(f"  å¹³å‡: {failed_tasks['llm_interactions'].mean():.1f}")
    print(f"  ä¸­ä½æ•°: {failed_tasks['llm_interactions'].median():.1f}")
    print(f"  æœ€å¤š: {failed_tasks['llm_interactions'].max()}")
    print(f"  æœ€å°‘: {failed_tasks['llm_interactions'].min()}")
    
    # è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°çš„ä»»åŠ¡
    max_interactions = failed_tasks[failed_tasks['llm_interactions'] == 35]
    print(f"  è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°(35)çš„ä»»åŠ¡: {len(max_interactions)}")

def analyze_specific_failures(failed_tasks):
    """åˆ†æå…·ä½“çš„å¤±è´¥æ¡ˆä¾‹"""
    print(f"\nğŸ” å…·ä½“å¤±è´¥æ¡ˆä¾‹åˆ†æ:")
    print("=" * 80)
    
    # æŒ‰å‘½ä»¤æˆåŠŸç‡æ’åºï¼Œåˆ†ææœ€ä¸¥é‡çš„å¤±è´¥
    failed_sorted = failed_tasks.sort_values('command_success_rate').reset_index(drop=True)
    
    for idx in range(min(10, len(failed_sorted))):
        task = failed_sorted.iloc[idx]
        scenario_id = int(task['scenario_id'])
        
        print(f"\nğŸ“‹ å¤±è´¥æ¡ˆä¾‹ #{idx+1}")
        print(f"åœºæ™¯ID: {scenario_id:05d}")
        print(f"ä»»åŠ¡æè¿°: {task['task_description']}")
        print(f"çŠ¶æ€: {task['status']}")
        print(f"æ¨¡å‹å£°ç§°å®Œæˆ: {task['model_claimed_done']}")
        print(f"å‘½ä»¤æˆåŠŸç‡: {task['command_success_rate']:.3f}")
        print(f"LLMäº¤äº’æ¬¡æ•°: {task['llm_interactions']}")
        print(f"æ‰§è¡Œæ—¶é•¿: {task['duration_seconds']:.1f}ç§’")
        
        # å°è¯•åŠ è½½è½¨è¿¹æ•°æ®
        trajectory_path = f"output/20250724_183034_multi_independent_00001_to_00600_qianduoduo_2.5-flash_wo_multi/trajectories/{scenario_id:05d}_task_00001_trajectory.json"
        if os.path.exists(trajectory_path):
            try:
                with open(trajectory_path, 'r', encoding='utf-8') as f:
                    trajectory_data = json.load(f)
                    if 'error_message' in trajectory_data:
                        print(f"é”™è¯¯ä¿¡æ¯: {trajectory_data['error_message']}")
                    if 'final_status' in trajectory_data:
                        print(f"æœ€ç»ˆçŠ¶æ€: {trajectory_data['final_status']}")
            except:
                pass
        
        print("-" * 60)

def compare_with_gpt4_results():
    """ä¸GPT-4ç»“æœå¯¹æ¯”"""
    print(f"\nğŸ“Š ä¸GPT-4ç»“æœå¯¹æ¯”:")
    
    # åŠ è½½GPT-4çš„ç»“æœ
    gpt4_path = "raw_output/20250723_225455_multi_independent_00001_to_00600_qianduoduo_4o_wo_multi/subtask_execution_log.csv"
    if os.path.exists(gpt4_path):
        gpt4_df = pd.read_csv(gpt4_path)
        gpt4_compound = gpt4_df[gpt4_df['task_category'] == 'compound_collaboration']
        gpt4_completion_rate = gpt4_compound['subtask_completed'].mean()
        
        # åŠ è½½Flashç»“æœ
        flash_path = "output/20250724_183034_multi_independent_00001_to_00600_qianduoduo_2.5-flash_wo_multi/subtask_execution_log.csv"
        flash_df = pd.read_csv(flash_path)
        flash_compound = flash_df[flash_df['task_category'] == 'compound_collaboration']
        flash_completion_rate = flash_compound['subtask_completed'].mean()
        
        print(f"GPT-4 compound_collaboration å®Œæˆç‡: {gpt4_completion_rate:.1%}")
        print(f"Gemini 2.5 Flash compound_collaboration å®Œæˆç‡: {flash_completion_rate:.1%}")
        print(f"æ€§èƒ½å·®è·: {(gpt4_completion_rate - flash_completion_rate):.1%}")
        
        # è¯¦ç»†å¯¹æ¯”
        print(f"\nè¯¦ç»†å¯¹æ¯”:")
        print(f"GPT-4: æ€»æ•° {len(gpt4_compound)}, å®Œæˆ {gpt4_compound['subtask_completed'].sum()}")
        print(f"Flash: æ€»æ•° {len(flash_compound)}, å®Œæˆ {flash_compound['subtask_completed'].sum()}")
    else:
        print("æœªæ‰¾åˆ°GPT-4ç»“æœæ–‡ä»¶ï¼Œæ— æ³•å¯¹æ¯”")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹åˆ†æ Gemini 2.5 Flash æ¨¡å‹è¡¨ç°...")
    
    # åŠ è½½æ•°æ®
    df = load_csv_data()
    if df is None:
        return
    
    # æ•´ä½“æ€§èƒ½åˆ†æ
    analyze_overall_performance(df)
    
    # compound_collaborationå¤±è´¥åˆ†æ
    compound_tasks, failed_tasks = analyze_compound_collaboration_failures(df)
    
    # ä»»åŠ¡æ¨¡å¼åˆ†æ
    analyze_task_patterns(failed_tasks)
    
    # å…·ä½“å¤±è´¥æ¡ˆä¾‹åˆ†æ
    analyze_specific_failures(failed_tasks)
    
    # ä¸GPT-4å¯¹æ¯”
    compare_with_gpt4_results()
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
